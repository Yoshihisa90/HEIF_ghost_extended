{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def is_double_compressed(mean_difference, final_QP, threshold):    \n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "\n",
    "    energy_clamp = torch.clamp(mean_difference, min=0)\n",
    "    energy = torch.sum(torch.square(energy_clamp))\n",
    "    mean_difference_right_clamp = torch.clamp(mean_difference[final_QP+1:52], min=0)\n",
    "    right_energy = torch.sum(torch.square(mean_difference_right_clamp))\n",
    "    \n",
    "    if energy != 0:\n",
    "        energy_ratio = right_energy / energy\n",
    "        if energy_ratio >= threshold:\n",
    "            return True\n",
    "        elif energy_ratio < threshold:\n",
    "            return False\n",
    "    else:\n",
    "        # energyが0の場合、エラーを処理するか、適切な値を返す\n",
    "        return -1\n",
    "\n",
    "    # if (right_energy / energy) != 0 and (right_energy / energy) >= threshold:\n",
    "    #     return True\n",
    "    # elif (right_energy / energy) != 0 and (right_energy / energy) < threshold:\n",
    "    #     return False\n",
    "    # else:\n",
    "    #     return -1\n",
    "    \n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = loaded_data\n",
    "    shifted_mae = loaded_data_shifted\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = [shifted - original for original, shifted in zip(original_mae, shifted_mae)]\n",
    "    \n",
    "    # mae_differenceをtensorに変換\n",
    "    mae_difference_tensor = torch.tensor(mae_difference)\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    mae_difference_positive = [0 if val <= 0 else val for val in mae_difference]\n",
    "    \n",
    "    return mae_difference_positive, mae_difference_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_csv_path1_list:  3080\n",
      "single_train_csv_path2_list:  3080\n",
      "second_train_csv_path1_list:  17556\n",
      "second_train_csv_path2_list:  17556\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "\n",
    "single_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_single_csv')\n",
    "single_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_second_sameQP_csv')\n",
    "\n",
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_csv')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_csv')\n",
    "\n",
    "\n",
    "single_train_csv_path1_list = [os.path.join(single_train_csv_path1, file) for file in sorted(os.listdir(single_train_csv_path1))]\n",
    "single_train_csv_path2_list = [os.path.join(single_train_csv_path2, file) for file in sorted(os.listdir(single_train_csv_path2))]\n",
    "\n",
    "second_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_csv_path1_list: \", len(single_train_csv_path1_list))\n",
    "print(\"single_train_csv_path2_list: \", len(single_train_csv_path2_list))\n",
    "\n",
    "print(\"second_train_csv_path1_list: \", len(second_train_csv_path1_list))\n",
    "print(\"second_train_csv_path2_list: \", len(second_train_csv_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_sameQP_train_csv_path1_list:  3080\n",
      "second_sameQP_train_csv_path2_list:  3080\n"
     ]
    }
   ],
   "source": [
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_sameQP_csv')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_sameQP_csv')\n",
    "\n",
    "second_sameQP_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_sameQP_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "print(\"second_sameQP_train_csv_path1_list: \", len(second_sameQP_train_csv_path1_list))\n",
    "print(\"second_sameQP_train_csv_path2_list: \", len(second_sameQP_train_csv_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_largeQP_train_csv_path1_list:  12012\n",
      "second_largeQP_train_csv_path2_list:  12012\n"
     ]
    }
   ],
   "source": [
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_largeQP_csv')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_largeQP_csv')\n",
    "\n",
    "second_largeQP_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_largeQP_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "print(\"second_largeQP_train_csv_path1_list: \", len(second_largeQP_train_csv_path1_list))\n",
    "print(\"second_largeQP_train_csv_path2_list: \", len(second_largeQP_train_csv_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_pkl_path1_list:  3080\n",
      "single_train_pkl_path2_list:  3080\n",
      "second_train_pkl_path1_list:  17556\n",
      "second_train_pkl_path2_list:  17556\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "single_train_pkl_path1 = os.path.join(rootpath, 'pkl_single')\n",
    "single_train_pkl_path2 = os.path.join(rootpath, 'pkl_second_sameQP')\n",
    "\n",
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple')\n",
    "\n",
    "\n",
    "single_train_pkl_path1_list = [os.path.join(single_train_pkl_path1, file) for file in sorted(os.listdir(single_train_pkl_path1))]\n",
    "single_train_pkl_path2_list = [os.path.join(single_train_pkl_path2, file) for file in sorted(os.listdir(single_train_pkl_path2))]\n",
    "\n",
    "second_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_pkl_path1_list: \", len(single_train_pkl_path1_list))\n",
    "print(\"single_train_pkl_path2_list: \", len(single_train_pkl_path2_list))\n",
    "\n",
    "print(\"second_train_pkl_path1_list: \", len(second_train_pkl_path1_list))\n",
    "print(\"second_train_pkl_path2_list: \", len(second_train_pkl_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_sameQP_train_pkl_path1_list:  3080\n",
      "second_sameQP_train_pkl_path2_list:  3080\n"
     ]
    }
   ],
   "source": [
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second_sameQP')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple_sameQP')\n",
    "\n",
    "second_sameQP_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_sameQP_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "print(\"second_sameQP_train_pkl_path1_list: \", len(second_sameQP_train_pkl_path1_list))\n",
    "print(\"second_sameQP_train_pkl_path2_list: \", len(second_sameQP_train_pkl_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_largeQP_train_pkl_path1_list:  12012\n",
      "second_largeQP_train_pkl_path2_list:  12012\n"
     ]
    }
   ],
   "source": [
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second_largeQP')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple_largeQP')\n",
    "\n",
    "second_largeQP_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_largeQP_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "print(\"second_largeQP_train_pkl_path1_list: \", len(second_largeQP_train_pkl_path1_list))\n",
    "print(\"second_largeQP_train_pkl_path2_list: \", len(second_largeQP_train_pkl_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  6000\n"
     ]
    }
   ],
   "source": [
    "single_train_csv = list(zip(single_train_csv_path1_list, single_train_pkl_path1_list, single_train_csv_path2_list, single_train_pkl_path2_list))\n",
    "\n",
    "second_train_csv = list(zip(second_train_csv_path1_list, second_train_pkl_path1_list, second_train_csv_path2_list, second_train_pkl_path2_list))\n",
    "second_sameQP_train_csv = list(zip(second_sameQP_train_csv_path1_list, second_sameQP_train_pkl_path1_list, second_sameQP_train_csv_path2_list, second_sameQP_train_pkl_path2_list))\n",
    "second_largeQP_train_csv = list(zip(second_largeQP_train_csv_path1_list, second_largeQP_train_pkl_path1_list, second_largeQP_train_csv_path2_list, second_largeQP_train_pkl_path2_list))\n",
    "\n",
    "single_train_csv = random.sample(single_train_csv, 3000)\n",
    "second_train_csv = random.sample(second_train_csv, 1000)\n",
    "second_sameQP_train_csv = random.sample(second_sameQP_train_csv, 1000)\n",
    "second_largeQP_train_csv = random.sample(second_largeQP_train_csv, 1000)\n",
    "\n",
    "train_csv_list = single_train_csv + second_train_csv + second_sameQP_train_csv + second_largeQP_train_csv\n",
    "# train_csv_list = single_train_csv + second_train_csv\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_csv_path1_list:  308\n",
      "single_train_csv_path2_list:  308\n",
      "second_train_csv_path1_list:  2772\n",
      "second_train_csv_path2_list:  2772\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/DATA_QP2_GROUP/\"\n",
    "\n",
    "single_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_single_csv/QP10')\n",
    "single_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_second_sameQP_csv/2ndQP10')\n",
    "\n",
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_csv/2ndQP10')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_csv/2ndQP10')\n",
    "\n",
    "\n",
    "single_train_csv_path1_list = [os.path.join(single_train_csv_path1, file) for file in sorted(os.listdir(single_train_csv_path1))]\n",
    "single_train_csv_path2_list = [os.path.join(single_train_csv_path2, file) for file in sorted(os.listdir(single_train_csv_path2))]\n",
    "\n",
    "second_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_csv_path1_list: \", len(single_train_csv_path1_list))\n",
    "print(\"single_train_csv_path2_list: \", len(single_train_csv_path2_list))\n",
    "\n",
    "print(\"second_train_csv_path1_list: \", len(second_train_csv_path1_list))\n",
    "print(\"second_train_csv_path2_list: \", len(second_train_csv_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_sameQP_train_csv_path1_list:  308\n",
      "second_sameQP_train_csv_path2_list:  308\n"
     ]
    }
   ],
   "source": [
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_sameQP_csv/2ndQP10')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_sameQP_csv/2ndQP10')\n",
    "\n",
    "second_sameQP_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_sameQP_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "print(\"second_sameQP_train_csv_path1_list: \", len(second_sameQP_train_csv_path1_list))\n",
    "print(\"second_sameQP_train_csv_path2_list: \", len(second_sameQP_train_csv_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_pkl_path1_list:  308\n",
      "single_train_pkl_path2_list:  308\n",
      "second_train_pkl_path1_list:  2772\n",
      "second_train_pkl_path2_list:  2772\n"
     ]
    }
   ],
   "source": [
    "# rootpath = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "single_train_pkl_path1 = os.path.join(rootpath, 'pkl_single/QP10')\n",
    "single_train_pkl_path2 = os.path.join(rootpath, 'pkl_second_sameQP/2ndQP10')\n",
    "\n",
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second/2ndQP10')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple/2ndQP10')\n",
    "\n",
    "\n",
    "single_train_pkl_path1_list = [os.path.join(single_train_pkl_path1, file) for file in sorted(os.listdir(single_train_pkl_path1))]\n",
    "single_train_pkl_path2_list = [os.path.join(single_train_pkl_path2, file) for file in sorted(os.listdir(single_train_pkl_path2))]\n",
    "\n",
    "second_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_pkl_path1_list: \", len(single_train_pkl_path1_list))\n",
    "print(\"single_train_pkl_path2_list: \", len(single_train_pkl_path2_list))\n",
    "\n",
    "print(\"second_train_pkl_path1_list: \", len(second_train_pkl_path1_list))\n",
    "print(\"second_train_pkl_path2_list: \", len(second_train_pkl_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_sameQP_train_pkl_path1_list:  308\n",
      "second_sameQP_train_pkl_path2_list:  308\n"
     ]
    }
   ],
   "source": [
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second_sameQP/2ndQP10')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple_sameQP/2ndQP10')\n",
    "\n",
    "second_sameQP_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_sameQP_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "print(\"second_sameQP_train_pkl_path1_list: \", len(second_sameQP_train_pkl_path1_list))\n",
    "print(\"second_sameQP_train_pkl_path2_list: \", len(second_sameQP_train_pkl_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "single_train_csv = list(zip(single_train_csv_path1_list, single_train_pkl_path1_list, single_train_csv_path2_list, single_train_pkl_path2_list))\n",
    "\n",
    "second_train_csv = list(zip(second_train_csv_path1_list, second_train_pkl_path1_list, second_train_csv_path2_list, second_train_pkl_path2_list))\n",
    "second_sameQP_train_csv = list(zip(second_sameQP_train_csv_path1_list, second_sameQP_train_pkl_path1_list, second_sameQP_train_csv_path2_list, second_sameQP_train_pkl_path2_list))\n",
    "# second_largeQP_train_csv = list(zip(second_largeQP_train_csv_path1_list, second_largeQP_train_pkl_path1_list, second_largeQP_train_csv_path2_list, second_largeQP_train_pkl_path2_list))\n",
    "\n",
    "single_train_csv = random.sample(single_train_csv, 300)\n",
    "second_train_csv = random.sample(second_train_csv, 150)\n",
    "second_sameQP_train_csv = random.sample(second_sameQP_train_csv, 150)\n",
    "# second_largeQP_train_csv = random.sample(second_largeQP_train_csv, 1000)\n",
    "\n",
    "test_csv_list = single_train_csv + second_train_csv + second_sameQP_train_csv\n",
    "# train_csv_list = single_train_csv + second_train_csv\n",
    "\n",
    "print(\"test_csv_list: \", len(test_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 6000\n",
      "Length of train_df_onlyGhost: 6000\n",
      "Length of train_df5: 6000\n",
      "Length of train_df6: 6000\n"
     ]
    }
   ],
   "source": [
    "# 列名をリストにまとめる\n",
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    train_pkl_list = [path2, path4]\n",
    "    \n",
    "    # pu_columnsの値を取得\n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    train_df1 = pd.concat([train_df1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    \n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "    \n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "    \n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(train_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    \n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "    \n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "    \n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)\n",
    "\n",
    "# 各データフレームの長さを表示\n",
    "print(f'Length of train_df: {len(train_df)}')\n",
    "print(f'Length of train_df_onlyGhost: {len(train_df_onlyGhost)}')\n",
    "print(f'Length of train_df5: {len(train_df5)}')\n",
    "print(f'Length of train_df6: {len(train_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_df: 600\n",
      "Length of test_df_onlyGhost: 600\n",
      "Length of test_df5: 600\n",
      "Length of test_df6: 600\n"
     ]
    }
   ],
   "source": [
    "# 列名をリストにまとめる\n",
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "test_df1 = pd.DataFrame(columns=pu_columns)\n",
    "test_df2 = pd.DataFrame(columns=label_columns)\n",
    "test_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "test_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "test_df5 = pd.DataFrame(columns=mae_columns)\n",
    "test_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in test_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    test_pkl_list = [path2, path4]\n",
    "    \n",
    "    # pu_columnsの値を取得\n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    test_df1 = pd.concat([test_df1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    \n",
    "    # label_columnsの値を取得\n",
    "    test_df2 = pd.concat([test_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "    \n",
    "    final_QP = extract_finalQP(test_pkl_list[0])\n",
    "    \n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(test_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(test_pkl_list[1])\n",
    "    \n",
    "    \n",
    "    # mae_columnsの値を取得\n",
    "    test_df5 = pd.concat([test_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "    \n",
    "    # final_qp_columnsの値を取得\n",
    "    test_df6 = pd.concat([test_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    test_df3 = pd.concat([test_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "    \n",
    "    # mae2_columnsの値を取得\n",
    "    test_df4 = pd.concat([test_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "test_df1.reset_index(drop=True, inplace=True)\n",
    "test_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "test_df = pd.concat([test_df1, test_df3, test_df4], axis=1)\n",
    "test_df_onlyGhost = pd.concat([test_df3, test_df4], axis=1)\n",
    "\n",
    "# 各データフレームの長さを表示\n",
    "print(f'Length of test_df: {len(test_df)}')\n",
    "print(f'Length of test_df_onlyGhost: {len(test_df_onlyGhost)}')\n",
    "print(f'Length of test_df5: {len(test_df5)}')\n",
    "print(f'Length of test_df6: {len(test_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 6000\n",
      "Length of X_train_onlyGhost: 6000\n",
      "Length of Y_train: 6000\n",
      "Length of train_df5_np: 6000\n",
      "Length of FINAL_QP: 6000\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "train_df5_np = train_df5.values\n",
    "FINAL_QP_train = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of train_df5_np: {len(train_df5_np)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 600\n",
      "Length of X_train_onlyGhost: 600\n",
      "Length of Y_train: 600\n",
      "Length of train_df5_np: 600\n",
      "Length of FINAL_QP: 600\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_test = scaler.fit_transform(test_df)\n",
    "X_test_onlyGhost = scaler.fit_transform(test_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "test_df5_np = test_df5.values\n",
    "FINAL_QP_test = test_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_test = test_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_test)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_test_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_test)}')\n",
    "print(f'Length of train_df5_np: {len(test_df5_np)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.44      0.60       300\n",
      "           1       0.64      0.98      0.77       300\n",
      "\n",
      "    accuracy                           0.71       600\n",
      "   macro avg       0.79      0.71      0.69       600\n",
      "weighted avg       0.79      0.71      0.69       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.41      0.58       300\n",
      "           1       0.62      0.98      0.76       300\n",
      "\n",
      "    accuracy                           0.69       600\n",
      "   macro avg       0.79      0.70      0.67       600\n",
      "weighted avg       0.79      0.69      0.67       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.46      0.63       300\n",
      "           1       0.65      0.99      0.79       300\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.82      0.73      0.71       600\n",
      "weighted avg       0.82      0.73      0.71       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.57      0.71       300\n",
      "           1       0.69      0.98      0.81       300\n",
      "\n",
      "    accuracy                           0.77       600\n",
      "   macro avg       0.83      0.77      0.76       600\n",
      "weighted avg       0.83      0.77      0.76       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       300\n",
      "           1       0.93      0.45      0.61       300\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       600\n",
      "   macro avg       0.78      0.71      0.69       600\n",
      "weighted avg       0.78      0.71      0.69       600\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.37      0.52       300\n",
      "           1       0.60      0.93      0.73       300\n",
      "\n",
      "    accuracy                           0.65       600\n",
      "   macro avg       0.72      0.65      0.62       600\n",
      "weighted avg       0.72      0.65      0.62       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.49      0.64       300\n",
      "           1       0.66      0.98      0.78       300\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.80      0.73      0.71       600\n",
      "weighted avg       0.80      0.73      0.71       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.44      0.61       300\n",
      "           1       0.64      0.99      0.78       300\n",
      "\n",
      "    accuracy                           0.71       600\n",
      "   macro avg       0.81      0.71      0.69       600\n",
      "weighted avg       0.81      0.71      0.69       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.61      0.75       300\n",
      "           1       0.71      0.98      0.83       300\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.84      0.79      0.79       600\n",
      "weighted avg       0.84      0.79      0.79       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       300\n",
      "           1       0.93      0.45      0.61       300\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       600\n",
      "   macro avg       0.78      0.71      0.69       600\n",
      "weighted avg       0.78      0.71      0.69       600\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.42      0.56       300\n",
      "           1       0.61      0.92      0.73       300\n",
      "\n",
      "    accuracy                           0.67       600\n",
      "   macro avg       0.72      0.67      0.65       600\n",
      "weighted avg       0.72      0.67      0.65       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.43      0.59       300\n",
      "           1       0.63      0.98      0.77       300\n",
      "\n",
      "    accuracy                           0.71       600\n",
      "   macro avg       0.80      0.71      0.68       600\n",
      "weighted avg       0.80      0.71      0.68       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.37      0.50       300\n",
      "           1       0.58      0.88      0.70       300\n",
      "\n",
      "    accuracy                           0.62       600\n",
      "   macro avg       0.67      0.62      0.60       600\n",
      "weighted avg       0.67      0.62      0.60       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.56      0.71       300\n",
      "           1       0.69      0.98      0.81       300\n",
      "\n",
      "    accuracy                           0.77       600\n",
      "   macro avg       0.83      0.77      0.76       600\n",
      "weighted avg       0.83      0.77      0.76       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       300\n",
      "           1       0.93      0.45      0.61       300\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       600\n",
      "   macro avg       0.78      0.71      0.69       600\n",
      "weighted avg       0.78      0.71      0.69       600\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.44      0.59       300\n",
      "           1       0.63      0.95      0.76       300\n",
      "\n",
      "    accuracy                           0.69       600\n",
      "   macro avg       0.76      0.69      0.67       600\n",
      "weighted avg       0.76      0.69      0.67       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.43      0.59       300\n",
      "           1       0.63      0.98      0.77       300\n",
      "\n",
      "    accuracy                           0.70       600\n",
      "   macro avg       0.79      0.70      0.68       600\n",
      "weighted avg       0.79      0.70      0.68       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.38      0.54       300\n",
      "           1       0.61      0.99      0.76       300\n",
      "\n",
      "    accuracy                           0.69       600\n",
      "   macro avg       0.80      0.68      0.65       600\n",
      "weighted avg       0.80      0.69      0.65       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.56      0.71       300\n",
      "           1       0.69      0.98      0.81       300\n",
      "\n",
      "    accuracy                           0.77       600\n",
      "   macro avg       0.83      0.77      0.76       600\n",
      "weighted avg       0.83      0.77      0.76       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       300\n",
      "           1       0.93      0.45      0.61       300\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       600\n",
      "   macro avg       0.78      0.71      0.69       600\n",
      "weighted avg       0.78      0.71      0.69       600\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.35      0.50       300\n",
      "           1       0.59      0.94      0.73       300\n",
      "\n",
      "    accuracy                           0.65       600\n",
      "   macro avg       0.72      0.65      0.61       600\n",
      "weighted avg       0.72      0.65      0.61       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.41      0.57       300\n",
      "           1       0.62      0.98      0.76       300\n",
      "\n",
      "    accuracy                           0.70       600\n",
      "   macro avg       0.79      0.70      0.67       600\n",
      "weighted avg       0.79      0.70      0.67       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.27      0.42       300\n",
      "           1       0.57      0.98      0.72       300\n",
      "\n",
      "    accuracy                           0.62       600\n",
      "   macro avg       0.75      0.62      0.57       600\n",
      "weighted avg       0.75      0.62      0.57       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.57      0.72       300\n",
      "           1       0.70      0.98      0.81       300\n",
      "\n",
      "    accuracy                           0.78       600\n",
      "   macro avg       0.83      0.78      0.77       600\n",
      "weighted avg       0.83      0.78      0.77       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       300\n",
      "           1       0.93      0.45      0.61       300\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       600\n",
      "   macro avg       0.78      0.71      0.69       600\n",
      "weighted avg       0.78      0.71      0.69       600\n",
      "\n",
      "<Fold-6>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "kfold2 = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Test_Score_RBF', 'C_LINEAR', 'Test_Score_LINEAR', \n",
    "                                'C_onlyGhost_RBF', 'Test_Score_onlyGhost_RBF', 'C_onlyGhost_LINEAR', 'Test_Score_onlyGhost_LINEAR',\n",
    "                                'Threshold', 'Test_Score_old'])\n",
    "\n",
    "original_X_train, original_X_train_onlyGhost = X_train, X_train_onlyGhost\n",
    "original_X_test, original_X_test_onlyGhost = X_test, X_test_onlyGhost\n",
    "original_Y_train, original_Y_test = Y_train, Y_test\n",
    "\n",
    "original_old_train = train_df5_np\n",
    "original_old_test, original_final_QP_test = test_df5_np, FINAL_QP_test\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(original_X_train, original_Y_train)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    results_old = []\n",
    "\n",
    "    # 全体を訓練・検証データに分割\n",
    "    X_train_val, _ = original_X_train[train_ids], original_X_train[test_ids]\n",
    "    X_train_onlyGhost_val, _ = original_X_train_onlyGhost[train_ids], original_X_train_onlyGhost[test_ids]\n",
    "    X_train_old_val, _ = original_old_train[train_ids], original_old_train[test_ids]\n",
    "    Y_train_val, _ = original_Y_train[train_ids], original_Y_train[test_ids]\n",
    "\n",
    "    # 訓練・検証データをさらに訓練データと検証データに分割\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=600, random_state=42)\n",
    "    X_train_onlyGhost, X_val_onlyGhost, _, _ = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=600, random_state=42)\n",
    "    \n",
    "    #テストデータの生成\n",
    "    X_test = original_X_test\n",
    "    X_test_onlyGhost = original_X_test_onlyGhost\n",
    "    X_test_old = original_old_test\n",
    "    final_QP = original_final_QP_test\n",
    "    Y_test = original_Y_test\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "\n",
    "\n",
    "    # 最適な閾値を見つける\n",
    "    for threshold in np.arange(0.01, 1.01, 0.01):        \n",
    "        results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(600)]\n",
    "        predicted_labels = [int(is_double) for is_double in results_old]\n",
    "        ground_truth_labels = [label for label in Y_test]\n",
    "        accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "\n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "\n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "\n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        # 検証データでの精度を評価\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "\n",
    "        # 最も高い精度を持つモデルを選択\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "    # テストデータでの評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(Y_test, test_predictions_RBF)\n",
    "    report_RBF = classification_report(Y_test, test_predictions_RBF)\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "\n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(Y_test, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(Y_test, test_predictions_LINEAR)\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "\n",
    "    # テストデータでの評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(Y_test, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(Y_test, test_predictions_onlyGhost_RBF)\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "\n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0)\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "\n",
    "    # Test結果を保存\n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Test_Score_RBF': test_accuracy_RBF,\n",
    "                  'C_LINEAR': best_c_value_LINEAR, 'Test_Score_LINEAR': test_accuracy_LINEAR,\n",
    "                  'C_onlyGhost_RBF': best_c_value_onlyGhost_RBF, 'Test_Score_onlyGhost_RBF': test_accuracy_onlyGhost_RBF,\n",
    "                  'C_onlyGhost_LINEAR': best_c_value_onlyGhost_LINEAR, 'Test_Score_onlyGhost_LINEAR': test_accuracy_onlyGhost_LINEAR,\n",
    "                  'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(column_name, label):\n",
    "    average = round(results[column_name].mean(), 2)\n",
    "    std_dev = round(results[column_name].std(), 2)\n",
    "    max_value = round(results[column_name].max(), 2)\n",
    "    min_value = round(results[column_name].min(), 2)\n",
    "\n",
    "    print(f'Average Test Score {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {label}: {min_value}')\n",
    "    print()\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "print_stats('Test_Score_RBF', 'with RBF')\n",
    "print_stats('Test_Score_LINEAR', 'with LINEAR')\n",
    "\n",
    "# 'Test_Score_onlyGhost'列に関して統計情報を表示\n",
    "print_stats('Test_Score_onlyGhost_RBF', 'with only Ghost and RBF')\n",
    "print_stats('Test_Score_onlyGhost_LINEAR', 'with only Ghost and LINEAR')\n",
    "\n",
    "# 'Test_Score_old'列に関して統計情報を表示\n",
    "print_stats('Test_Score_old', 'with old model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"font.size\"]=5\n",
    "# plt.rcParams[\"figure.figsize\"]=(2.0, 1.0)\n",
    "# plt.rcParams[\"figure.dpi\"]= 300\n",
    "\n",
    "\n",
    "# # Cの範囲を指定\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # 結果のデータフレームを初期化\n",
    "# results = pd.DataFrame(columns=['C_RBF', 'Test_Score_RBF', 'C_LINEAR', 'Test_Score_LINEAR', \n",
    "#                                 'C_onlyGhost_RBF', 'Test_Score_onlyGhost_RBF', 'C_onlyGhost_LINEAR', 'Test_Score_onlyGhost_LINEAR',\n",
    "#                                 'Threshold', 'Test_Score_old'])\n",
    "\n",
    "# original_X_train, original_X_train_onlyGhost = X_train, X_train_onlyGhost\n",
    "# original_X_test, original_X_test_onlyGhost = X_test, X_test_onlyGhost\n",
    "# original_Y_train, original_Y_test = Y_train, Y_test\n",
    "\n",
    "# original_old_train = train_df5_np\n",
    "# original_old_test, original_final_QP_test = test_df5_np, FINAL_QP_test\n",
    "\n",
    "# # k-fold cross-validation\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(original_X_train, original_Y_train)):\n",
    "#     print(f\"<Fold-{fold+1}>\")\n",
    "#     print()\n",
    "    \n",
    "#     results_old = []\n",
    "\n",
    "#     # 全体を訓練・検証データとテストデータに分割\n",
    "#     X_train_val, _ = original_X_train[train_ids], original_X_train[test_ids]\n",
    "#     X_train_onlyGhost_val, _ = original_X_train_onlyGhost[train_ids], original_X_train_onlyGhost[test_ids]\n",
    "#     X_train_old_val, _ = original_old_train[train_ids], original_old_train[test_ids]\n",
    "    \n",
    "#     # 全体を訓練・検証ラベルとテストラベルに分割\n",
    "#     Y_train_val, _ = original_Y_train[train_ids], original_Y_train[test_ids]\n",
    "    \n",
    "#     # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "#     X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=600, random_state=42)\n",
    "#     X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=600, random_state=42)\n",
    "    \n",
    "#     for fold, (train2_ids, test2_ids) in enumerate(kfold.split(original_X_test, original_Y_test)):\n",
    "#         _, X_test = original_X_test[train2_ids], original_X_test[test2_ids]\n",
    "#         _, X_test_onlyGhost = original_X_test_onlyGhost[train2_ids], original_X_test_onlyGhost[test2_ids]\n",
    "#         _, X_test_old = original_old_test[train2_ids], original_old_test[test2_ids]\n",
    "        \n",
    "#         final_QP = original_final_QP_test[test2_ids]\n",
    "        \n",
    "#         _, Y_test = original_Y_test[train2_ids], original_Y_test[test2_ids]\n",
    "        \n",
    "#         # print(len(X_test), len(X_test_onlyGhost), len(X_test_old), len(Y_test), len(final_QP))\n",
    "        \n",
    "#         best_threshold = 0\n",
    "#         best_accuracy = 0\n",
    "#         best_predicted_labels = []\n",
    "#         best_ground_truth_labels = []\n",
    "    \n",
    "#         for threshold in np.arange(0.00,1.01,0.01):\n",
    "#             results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(60)]\n",
    "#             predicted_labels = [int(is_double) for is_double in results_old]\n",
    "#             ground_truth_labels = [label for label in Y_test]\n",
    "#             accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "\n",
    "#             if accuracy > best_accuracy:\n",
    "#                 best_accuracy = accuracy\n",
    "#                 best_threshold = threshold\n",
    "#                 best_predicted_labels = predicted_labels\n",
    "#                 best_ground_truth_labels = ground_truth_labels\n",
    "\n",
    "#         best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "#         best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "\n",
    "#         best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "#         best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "\n",
    "#         for C_value in C_values['C']:    \n",
    "#             # SVMモデルのインスタンスを作成\n",
    "#             svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "#             svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "\n",
    "#             svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "#             svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "#             # 訓練データで訓練\n",
    "#             svm_model_RBF.fit(X_train, Y_train)\n",
    "#             svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "#             svm_model_LINEAR.fit(X_train, Y_train)\n",
    "#             svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "\n",
    "#             val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "#             val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "\n",
    "#             val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "#             val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "\n",
    "\n",
    "#             # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "#             if val_accuracy_RBF > best_val_score_RBF:\n",
    "#                 best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "#             if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "#                 best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "\n",
    "#             if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "#                 best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "#             if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "#                 best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "\n",
    "#         # テストデータで評価\n",
    "#         test_predictions_RBF = best_svm_model_RBF.predict(X_test)\n",
    "#         test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "#         test_accuracy_RBF = accuracy_score(Y_test, test_predictions_RBF)\n",
    "#         report_RBF = classification_report(Y_test, test_predictions_RBF)\n",
    "#         print(f'Summary_RBF:\\n{report_RBF}')\n",
    "\n",
    "\n",
    "#         test_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test)\n",
    "#         test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "#         test_accuracy_LINEAR = accuracy_score(Y_test, test_predictions_LINEAR)\n",
    "#         report_LINEAR = classification_report(Y_test, test_predictions_LINEAR)\n",
    "#         print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "\n",
    "\n",
    "#         # テストデータで評価\n",
    "#         test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_onlyGhost)\n",
    "#         test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "#         test_accuracy_onlyGhost_RBF = accuracy_score(Y_test, test_predictions_onlyGhost_RBF)\n",
    "#         report_onlyGhost_RBF = classification_report(Y_test, test_predictions_onlyGhost_RBF)\n",
    "#         print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "\n",
    "#         test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_onlyGhost)\n",
    "#         test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "#         test_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "#         report_onlyGhost_LINEAR = classification_report(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "#         print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "\n",
    "\n",
    "#         report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0)\n",
    "#         print(f'Summary old_model:\\n{report_old}')\n",
    "\n",
    "#         # Test結果を保存\n",
    "\n",
    "\n",
    "#         result_row = {'C_RBF': best_c_value_RBF, 'Test_Score_RBF': test_accuracy_RBF,\n",
    "#                   'C_LINEAR': best_c_value_LINEAR, 'Test_Score_LINEAR': test_accuracy_LINEAR,\n",
    "#                   'C_onlyGhost_RBF': best_c_value_onlyGhost_RBF, 'Test_Score_onlyGhost_RBF': test_accuracy_onlyGhost_RBF,\n",
    "#                   'C_onlyGhost_LINEAR': best_c_value_onlyGhost_LINEAR, 'Test_Score_onlyGhost_LINEAR': test_accuracy_onlyGhost_LINEAR,\n",
    "#                   'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "#         results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# # 結果を表示\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
