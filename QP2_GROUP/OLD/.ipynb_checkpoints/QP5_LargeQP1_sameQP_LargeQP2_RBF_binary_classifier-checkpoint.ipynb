{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_csv_path1_list:  308\n",
      "single_train_csv_path2_list:  308\n",
      "second_train_csv_path1_list:  3080\n",
      "second_train_csv_path2_list:  3080\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/DATA_QP2_GROUP/\"\n",
    "\n",
    "single_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_single_csv/QP5')\n",
    "single_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_second_sameQP_csv/2ndQP5')\n",
    "\n",
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_csv/2ndQP5')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_csv/2ndQP5')\n",
    "\n",
    "\n",
    "single_train_csv_path1_list = [os.path.join(single_train_csv_path1, file) for file in sorted(os.listdir(single_train_csv_path1))]\n",
    "single_train_csv_path2_list = [os.path.join(single_train_csv_path2, file) for file in sorted(os.listdir(single_train_csv_path2))]\n",
    "\n",
    "second_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_csv_path1_list: \", len(single_train_csv_path1_list))\n",
    "print(\"single_train_csv_path2_list: \", len(single_train_csv_path2_list))\n",
    "\n",
    "print(\"second_train_csv_path1_list: \", len(second_train_csv_path1_list))\n",
    "print(\"second_train_csv_path2_list: \", len(second_train_csv_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_sameQP_train_csv_path1_list:  308\n",
      "second_sameQP_train_csv_path2_list:  308\n"
     ]
    }
   ],
   "source": [
    "second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_sameQP_csv/2ndQP5')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_sameQP_csv/2ndQP5')\n",
    "\n",
    "second_sameQP_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_sameQP_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "print(\"second_sameQP_train_csv_path1_list: \", len(second_sameQP_train_csv_path1_list))\n",
    "print(\"second_sameQP_train_csv_path2_list: \", len(second_sameQP_train_csv_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_train_csv_path1 = os.path.join(rootpath, 'HEIF_images_second_largeQP_csv/2ndQP5')\n",
    "# second_train_csv_path2 = os.path.join(rootpath, 'HEIF_images_triple_largeQP_csv/2ndQP5')\n",
    "\n",
    "# second_largeQP_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "# second_largeQP_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "# print(\"second_largeQP_train_csv_path1_list: \", len(second_largeQP_train_csv_path1_list))\n",
    "# print(\"second_largeQP_train_csv_path2_list: \", len(second_largeQP_train_csv_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_pkl_path1_list:  308\n",
      "single_train_pkl_path2_list:  308\n",
      "second_train_pkl_path1_list:  3080\n",
      "second_train_pkl_path2_list:  3080\n"
     ]
    }
   ],
   "source": [
    "# rootpath = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "single_train_pkl_path1 = os.path.join(rootpath, 'pkl_single/QP5')\n",
    "single_train_pkl_path2 = os.path.join(rootpath, 'pkl_second_sameQP/2ndQP5')\n",
    "\n",
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second/2ndQP5')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple/2ndQP5')\n",
    "\n",
    "\n",
    "single_train_pkl_path1_list = [os.path.join(single_train_pkl_path1, file) for file in sorted(os.listdir(single_train_pkl_path1))]\n",
    "single_train_pkl_path2_list = [os.path.join(single_train_pkl_path2, file) for file in sorted(os.listdir(single_train_pkl_path2))]\n",
    "\n",
    "second_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_pkl_path1_list: \", len(single_train_pkl_path1_list))\n",
    "print(\"single_train_pkl_path2_list: \", len(single_train_pkl_path2_list))\n",
    "\n",
    "print(\"second_train_pkl_path1_list: \", len(second_train_pkl_path1_list))\n",
    "print(\"second_train_pkl_path2_list: \", len(second_train_pkl_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_sameQP_train_pkl_path1_list:  308\n",
      "second_sameQP_train_pkl_path2_list:  308\n"
     ]
    }
   ],
   "source": [
    "second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second_sameQP/2ndQP5')\n",
    "second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple_sameQP/2ndQP5')\n",
    "\n",
    "second_sameQP_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "second_sameQP_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "print(\"second_sameQP_train_pkl_path1_list: \", len(second_sameQP_train_pkl_path1_list))\n",
    "print(\"second_sameQP_train_pkl_path2_list: \", len(second_sameQP_train_pkl_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_train_pkl_path1 = os.path.join(rootpath, 'pkl_second_largeQP/2ndQP5')\n",
    "# second_train_pkl_path2 = os.path.join(rootpath, 'pkl_triple_largeQP/2ndQP5')\n",
    "\n",
    "# second_largeQP_train_pkl_path1_list = [os.path.join(second_train_pkl_path1, file) for file in sorted(os.listdir(second_train_pkl_path1))]\n",
    "# second_largeQP_train_pkl_path2_list = [os.path.join(second_train_pkl_path2, file) for file in sorted(os.listdir(second_train_pkl_path2))]\n",
    "\n",
    "# print(\"second_largeQP_train_pkl_path1_list: \", len(second_largeQP_train_pkl_path1_list))\n",
    "# print(\"second_largeQP_train_pkl_path2_list: \", len(second_largeQP_train_pkl_path2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "single_train_csv = list(zip(single_train_csv_path1_list, single_train_pkl_path1_list, single_train_csv_path2_list, single_train_pkl_path2_list))\n",
    "\n",
    "second_train_csv = list(zip(second_train_csv_path1_list, second_train_pkl_path1_list, second_train_csv_path2_list, second_train_pkl_path2_list))\n",
    "second_sameQP_train_csv = list(zip(second_sameQP_train_csv_path1_list, second_sameQP_train_pkl_path1_list, second_sameQP_train_csv_path2_list, second_sameQP_train_pkl_path2_list))\n",
    "# second_largeQP_train_csv = list(zip(second_largeQP_train_csv_path1_list, second_largeQP_train_pkl_path1_list, second_largeQP_train_csv_path2_list, second_largeQP_train_pkl_path2_list))\n",
    "\n",
    "single_train_csv = random.sample(single_train_csv, 300)\n",
    "second_train_csv = random.sample(second_train_csv, 150)\n",
    "second_sameQP_train_csv = random.sample(second_sameQP_train_csv, 150)\n",
    "# second_largeQP_train_csv = random.sample(second_largeQP_train_csv, 1000)\n",
    "\n",
    "train_csv_list = single_train_csv + second_train_csv + second_sameQP_train_csv\n",
    "# train_csv_list = single_train_csv + second_train_csv\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def is_double_compressed(mean_difference, final_QP, threshold):    \n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "\n",
    "    energy_clamp = torch.clamp(mean_difference, min=0)\n",
    "    energy = torch.sum(torch.square(energy_clamp))\n",
    "    mean_difference_right_clamp = torch.clamp(mean_difference[final_QP+1:52], min=0)\n",
    "    right_energy = torch.sum(torch.square(mean_difference_right_clamp))\n",
    "    \n",
    "    if energy != 0:\n",
    "        energy_ratio = right_energy / energy\n",
    "        if energy_ratio >= threshold:\n",
    "            return True\n",
    "        elif energy_ratio < threshold:\n",
    "            return False\n",
    "    else:\n",
    "        # energyが0の場合、エラーを処理するか、適切な値を返す\n",
    "        return -1\n",
    "\n",
    "    # if (right_energy / energy) != 0 and (right_energy / energy) >= threshold:\n",
    "    #     return True\n",
    "    # elif (right_energy / energy) != 0 and (right_energy / energy) < threshold:\n",
    "    #     return False\n",
    "    # else:\n",
    "    #     return -1\n",
    "    \n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = loaded_data\n",
    "    shifted_mae = loaded_data_shifted\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = [shifted - original for original, shifted in zip(original_mae, shifted_mae)]\n",
    "    \n",
    "    # mae_differenceをtensorに変換\n",
    "    mae_difference_tensor = torch.tensor(mae_difference)\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    mae_difference_positive = [0 if val <= 0 else val for val in mae_difference]\n",
    "    \n",
    "    return mae_difference_positive, mae_difference_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 600\n",
      "Length of train_df_onlyGhost: 600\n",
      "Length of train_df5: 600\n",
      "Length of train_df6: 600\n"
     ]
    }
   ],
   "source": [
    "# 列名をリストにまとめる\n",
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    train_pkl_list = [path2, path4]\n",
    "    \n",
    "    # pu_columnsの値を取得\n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    train_df1 = pd.concat([train_df1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    \n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "    \n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "    \n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(train_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    \n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "    \n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "    \n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)\n",
    "\n",
    "# 各データフレームの長さを表示\n",
    "print(f'Length of train_df: {len(train_df)}')\n",
    "print(f'Length of train_df_onlyGhost: {len(train_df_onlyGhost)}')\n",
    "print(f'Length of train_df5: {len(train_df5)}')\n",
    "print(f'Length of train_df6: {len(train_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 600\n",
      "Length of X_train_onlyGhost: 600\n",
      "Length of Y_train: 600\n",
      "Length of train_df5_np: 600\n",
      "Length of FINAL_QP: 600\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "train_df5_np = train_df5.values\n",
    "FINAL_QP = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of train_df5_np: {len(train_df5_np)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        30\n",
      "           1       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.81        30\n",
      "           1       0.94      0.57      0.71        30\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.82      0.77      0.76        60\n",
      "weighted avg       0.82      0.77      0.76        60\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.37      0.48        30\n",
      "           1       0.61      0.83      0.70        30\n",
      "\n",
      "   micro avg       0.63      0.60      0.62        60\n",
      "   macro avg       0.65      0.60      0.59        60\n",
      "weighted avg       0.65      0.60      0.59        60\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85        30\n",
      "           1       0.95      0.70      0.81        30\n",
      "\n",
      "    accuracy                           0.83        60\n",
      "   macro avg       0.86      0.83      0.83        60\n",
      "weighted avg       0.86      0.83      0.83        60\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.90      0.72        30\n",
      "           1       0.86      0.40      0.55        30\n",
      "\n",
      "   micro avg       0.66      0.65      0.66        60\n",
      "   macro avg       0.73      0.65      0.63        60\n",
      "weighted avg       0.73      0.65      0.63        60\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72        30\n",
      "           1       0.80      0.53      0.64        30\n",
      "\n",
      "   micro avg       0.69      0.68      0.69        60\n",
      "   macro avg       0.72      0.68      0.68        60\n",
      "weighted avg       0.72      0.68      0.68        60\n",
      "\n",
      "<Fold-6>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68        30\n",
      "           1       0.70      0.70      0.70        30\n",
      "\n",
      "   micro avg       0.69      0.68      0.69        60\n",
      "   macro avg       0.69      0.68      0.69        60\n",
      "weighted avg       0.69      0.68      0.69        60\n",
      "\n",
      "<Fold-7>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        30\n",
      "           1       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84        30\n",
      "           1       1.00      0.70      0.82        30\n",
      "\n",
      "   micro avg       0.84      0.82      0.83        60\n",
      "   macro avg       0.88      0.82      0.83        60\n",
      "weighted avg       0.88      0.82      0.83        60\n",
      "\n",
      "<Fold-8>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        30\n",
      "           1       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95        30\n",
      "           1       0.94      0.97      0.95        30\n",
      "\n",
      "    accuracy                           0.95        60\n",
      "   macro avg       0.95      0.95      0.95        60\n",
      "weighted avg       0.95      0.95      0.95        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.93      0.76        30\n",
      "           1       0.93      0.47      0.62        30\n",
      "\n",
      "   micro avg       0.71      0.70      0.71        60\n",
      "   macro avg       0.78      0.70      0.69        60\n",
      "weighted avg       0.78      0.70      0.69        60\n",
      "\n",
      "<Fold-9>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        30\n",
      "           1       1.00      0.90      0.95        30\n",
      "\n",
      "    accuracy                           0.95        60\n",
      "   macro avg       0.95      0.95      0.95        60\n",
      "weighted avg       0.95      0.95      0.95        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.93      0.79        30\n",
      "           1       0.94      0.57      0.71        30\n",
      "\n",
      "   micro avg       0.76      0.75      0.76        60\n",
      "   macro avg       0.81      0.75      0.75        60\n",
      "weighted avg       0.81      0.75      0.75        60\n",
      "\n",
      "<Fold-10>\n",
      "\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Summary_onlyGhost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74        30\n",
      "           1       0.76      0.63      0.69        30\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.72      0.72      0.71        60\n",
      "weighted avg       0.72      0.72      0.71        60\n",
      "\n",
      "      C Test_Score C_onlyGHost Test_Score_onlyGhost Threshold Test_Score_old\n",
      "0   100   0.983333          10             0.966667      0.24       0.766667\n",
      "1    10   0.983333          10             0.983333      0.02            0.6\n",
      "2    10   0.966667          10             0.966667       0.2       0.833333\n",
      "3  1000   0.966667         100             0.966667      0.21           0.65\n",
      "4    10   0.983333          10                  1.0      0.15       0.683333\n",
      "5   100        1.0          10             0.983333      0.03       0.683333\n",
      "6    10        1.0          10             0.966667       0.4       0.816667\n",
      "7   100   0.966667          10                 0.95      0.38            0.7\n",
      "8    10       0.95         100             0.966667      0.58           0.75\n",
      "9    10   0.966667         100             0.983333      0.18       0.716667\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"font.size\"]=5\n",
    "plt.rcParams[\"figure.figsize\"]=(2.0, 1.0)\n",
    "plt.rcParams[\"figure.dpi\"]= 300\n",
    "\n",
    "\n",
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C', 'Test_Score', 'C_onlyGHost', 'Test_Score_onlyGhost', 'Threshold', 'Test_Score_old'])\n",
    "\n",
    "original_X_train, original_X_train_onlyGhost = X_train, X_train_onlyGhost\n",
    "original_Y_train = Y_train\n",
    "original_old, original_final_QP = train_df5_np, FINAL_QP\n",
    "\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(original_X_train, original_Y_train)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    results_old = []\n",
    "\n",
    "    # 全体を訓練・検証データとテストデータに分割\n",
    "    X_train_val, X_test = original_X_train[train_ids], original_X_train[test_ids]\n",
    "    X_train_onlyGhost_val, X_test_onlyGhost = original_X_train_onlyGhost[train_ids], original_X_train_onlyGhost[test_ids]\n",
    "    X_train_old_val, X_test_old = original_old[train_ids], original_old[test_ids]\n",
    "    \n",
    "    final_QP = original_final_QP[test_ids]\n",
    "    \n",
    "    # 全体を訓練・検証ラベルとテストラベルに分割\n",
    "    Y_train_val, Y_test = original_Y_train[train_ids], original_Y_train[test_ids]\n",
    "    \n",
    "    # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=60, random_state=42)\n",
    "    X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=60, random_state=42)\n",
    "    \n",
    "    # for i in range(600): \n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    for threshold in np.arange(0.01,1.00,0.01):\n",
    "        results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(60)]\n",
    "        # results_old.append((is_double))\n",
    "        \n",
    "        predicted_labels = [int(is_double) for is_double in results_old]\n",
    "        ground_truth_labels = [label for label in Y_test]\n",
    "        accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "        \n",
    "             \n",
    "#     best_val_score = 0  # 最も高い検証データの精度を保存する変数\n",
    "#     best_svm_model = None  # 最も高い検証データの精度を持つモデルを保存する変数\n",
    "#     best_c_value = None\n",
    "    \n",
    "#     best_val_score_onlyGhost = 0  # 最も高い検証データの精度を保存する変数\n",
    "#     best_svm_model_onlyGhost = None  # 最も高い検証データの精度を持つモデルを保存する変数\n",
    "#     best_c_value_onlyGhost = None\n",
    "    \n",
    "    best_val_score, best_svm_model, best_c_value = 0, None, None\n",
    "    best_val_score_onlyGhost, best_svm_model_onlyGhost, best_c_value_onlyGhost = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost = SVC(kernel='rbf', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        # 検証データで評価\n",
    "        # val_predictions = svm_model.predict(X_val)\n",
    "        # val_accuracy = accuracy_score(Y_val, val_predictions)\n",
    "        # val_predictions_onlyGhost = svm_model_onlyGhost.predict(X_val_onlyGhost)\n",
    "        # val_accuracy_onlyGhost = accuracy_score(Y_val, val_predictions_onlyGhost)\n",
    "        \n",
    "        val_accuracy = accuracy_score(Y_val, svm_model.predict(X_val))\n",
    "        val_accuracy_onlyGhost = accuracy_score(Y_val, svm_model_onlyGhost.predict(X_val_onlyGhost))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy > best_val_score:\n",
    "            best_val_score, best_svm_model, best_c_value = val_accuracy, svm_model, C_value\n",
    "            # best_val_score = val_accuracy\n",
    "            # best_svm_model = svm_model\n",
    "            # best_c_value = C_value\n",
    "            \n",
    "        if val_accuracy_onlyGhost > best_val_score_onlyGhost:\n",
    "            best_val_score_onlyGhost, best_svm_model_onlyGhost, best_c_value_onlyGhost = val_accuracy_onlyGhost, svm_model_onlyGhost, C_value\n",
    "            # best_val_score_onlyGhost = val_accuracy_onlyGhost\n",
    "            # best_svm_model_onlyGhost = svm_model_onlyGhost\n",
    "            # best_c_value_onlyGhost = C_value\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions = best_svm_model.predict(X_test)\n",
    "    test_predictions_prob = best_svm_model.decision_function(X_test)\n",
    "    test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "    report = classification_report(Y_test, test_predictions)\n",
    "    print(f'Summary:\\n{report}')\n",
    "        \n",
    "#     # ROCカーブを出力\n",
    "#     fpr, tpr, thresholds = roc_curve(Y_test, test_predictions_prob)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Fold {fold+1} (AUC = {roc_auc:.2f})')\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.savefig(f'ROC_Curves_fold{fold+1}.png', bbox_inches='tight', pad_inches=0.0)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost = best_svm_model_onlyGhost.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost = best_svm_model_onlyGhost.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost = accuracy_score(Y_test, test_predictions_onlyGhost)\n",
    "    report_onlyGhost = classification_report(Y_test, test_predictions_onlyGhost)\n",
    "    print(f'Summary_onlyGhost:\\n{report_onlyGhost}')\n",
    "    \n",
    "#     # ROCカーブを出力\n",
    "    \n",
    "#     print(\"Y_test\", len(Y_test))\n",
    "#     print(\"Prob\", test_predictions_prob_onlyGhost)\n",
    "#     fpr2, tpr2, thresholds = roc_curve(Y_test, test_predictions_prob_onlyGhost)\n",
    "#     roc_auc_onlyGhost = auc(fpr2, tpr2)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr2, tpr2, color='darkorange', lw=2, label=f'Fold {fold+1} (AUC = {roc_auc_onlyGhost:.2f})')\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.savefig(f'ROC_Curves_fold{fold+1}_onlyGhost.png', bbox_inches='tight', pad_inches=0.0)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    # テストデータで評価\n",
    "    # predicted_labels = [int(is_double) for is_double in results_old]\n",
    "    # ground_truth_labels = [label for label in Y_test]\n",
    "    # accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "    # report_old = classification_report(ground_truth_labels, predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0)\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0)\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    result_row = {'C': best_c_value, 'Test_Score': test_accuracy, \n",
    "                  'C_onlyGHost': best_c_value_onlyGhost, 'Test_Score_onlyGhost': test_accuracy_onlyGhost,\n",
    "                  'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Score : 0.98\n",
      "Standard Deviation of Test Score : 0.02\n",
      "Maximum Test Score : 1.0\n",
      "Minimum Test Score : 0.95\n",
      "\n",
      "Average Test Score with only Ghost: 0.97\n",
      "Standard Deviation of Test Score with only Ghost: 0.01\n",
      "Maximum Test Score with only Ghost: 1.0\n",
      "Minimum Test Score with only Ghost: 0.95\n",
      "\n",
      "Average Test Score with old model: 0.72\n",
      "Standard Deviation of Test Score with old model: 0.07\n",
      "Maximum Test Score with old model: 0.83\n",
      "Minimum Test Score with old model: 0.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stats(column_name, label):\n",
    "    average = round(results[column_name].mean(), 2)\n",
    "    std_dev = round(results[column_name].std(), 2)\n",
    "    max_value = round(results[column_name].max(), 2)\n",
    "    min_value = round(results[column_name].min(), 2)\n",
    "\n",
    "    print(f'Average Test Score {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {label}: {min_value}')\n",
    "    print()\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "print_stats('Test_Score', '')\n",
    "\n",
    "# 'Test_Score_onlyGhost'列に関して統計情報を表示\n",
    "print_stats('Test_Score_onlyGhost', 'with only Ghost')\n",
    "\n",
    "# 'Test_Score_old'列に関して統計情報を表示\n",
    "print_stats('Test_Score_old', 'with old model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
