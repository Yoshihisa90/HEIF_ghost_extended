{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_csv_path1_list:  270\n",
      "single_train_csv_path2_list:  270\n",
      "second_train_csv_path1_list:  270\n",
      "second_train_csv_path2_list:  270\n",
      "single_test_csv_path1_list:  30\n",
      "single_test_csv_path2_list:  30\n",
      "second_test_csv_path1_list:  30\n",
      "second_test_csv_path2_list:  30\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/QPD/\"\n",
    "\n",
    "single_train_csv_path1 = os.path.join(rootpath, 'QPD11_HEIF_images_single_csv', 'TRAINING')\n",
    "single_train_csv_path2 = os.path.join(rootpath, 'QPD11_HEIF_images_second_sameQP_csv', 'TRAINING')\n",
    "\n",
    "second_train_csv_path1 = os.path.join(rootpath, 'QPD11_HEIF_images_second_csv', 'TRAINING')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'QPD11_HEIF_images_triple_csv', 'TRAINING')\n",
    "\n",
    "single_test_csv_path1 = os.path.join(rootpath, 'QPD11_HEIF_images_single_csv', 'TEST')\n",
    "single_test_csv_path2 = os.path.join(rootpath, 'QPD11_HEIF_images_second_sameQP_csv', 'TEST')\n",
    "\n",
    "second_test_csv_path1 = os.path.join(rootpath, 'QPD11_HEIF_images_second_csv', 'TEST')\n",
    "second_test_csv_path2 = os.path.join(rootpath, 'QPD11_HEIF_images_triple_csv', 'TEST')\n",
    "\n",
    "\n",
    "single_train_csv_path1_list = [os.path.join(single_train_csv_path1, file) for file in sorted(os.listdir(single_train_csv_path1))]\n",
    "single_train_csv_path2_list = [os.path.join(single_train_csv_path2, file) for file in sorted(os.listdir(single_train_csv_path2))]\n",
    "\n",
    "second_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "single_test_csv_path1_list = [os.path.join(single_test_csv_path1, file) for file in sorted(os.listdir(single_test_csv_path1))]\n",
    "single_test_csv_path2_list = [os.path.join(single_test_csv_path2, file) for file in sorted(os.listdir(single_test_csv_path2))]\n",
    "\n",
    "second_test_csv_path1_list = [os.path.join(second_test_csv_path1, file) for file in sorted(os.listdir(second_test_csv_path1))]\n",
    "second_test_csv_path2_list = [os.path.join(second_test_csv_path2, file) for file in sorted(os.listdir(second_test_csv_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_csv_path1_list: \", len(single_train_csv_path1_list))\n",
    "print(\"single_train_csv_path2_list: \", len(single_train_csv_path2_list))\n",
    "\n",
    "print(\"second_train_csv_path1_list: \", len(second_train_csv_path1_list))\n",
    "print(\"second_train_csv_path2_list: \", len(second_train_csv_path2_list))\n",
    "\n",
    "print(\"single_test_csv_path1_list: \", len(single_test_csv_path1_list))\n",
    "print(\"single_test_csv_path2_list: \", len(single_test_csv_path2_list))\n",
    "\n",
    "print(\"second_test_csv_path1_list: \", len(second_test_csv_path1_list))\n",
    "print(\"second_test_csv_path2_list: \", len(second_test_csv_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  540\n",
      "test_csv_list:  60\n"
     ]
    }
   ],
   "source": [
    "single_train_csv = list(zip(single_train_csv_path1_list, single_train_csv_path2_list))\n",
    "second_train_csv = list(zip(second_train_csv_path1_list, second_train_csv_path2_list))\n",
    "second_train_csv = random.sample(second_train_csv, len(single_train_csv))\n",
    "\n",
    "single_test_csv = list(zip(single_test_csv_path1_list, single_test_csv_path2_list))\n",
    "second_test_csv = list(zip(second_test_csv_path1_list, second_test_csv_path2_list))\n",
    "second_test_csv = random.sample(second_test_csv, len(single_test_csv))\n",
    "\n",
    "train_csv_list = single_train_csv + second_train_csv\n",
    "test_csv_list = single_test_csv + second_test_csv\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list))\n",
    "print(\"test_csv_list: \", len(test_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n"
     ]
    }
   ],
   "source": [
    "# new_df = pd.DataFrame(columns=[\"QP\", \"CU_64\", \"CU_32\", \"CU_16\", \"CU_8\", \"PU_64\", \"PU_32\", \"PU_16\", \"PU_8\", \"PU_4\", \"LUM_A\", \"LUM_B\", \"LUM_C\", \"CRM_34\", \"LABEL\"])\n",
    "# train_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\", \"LABEL\"])\n",
    "train_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"])\n",
    "train_df2 = pd.DataFrame(columns=[\"LABEL\"])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path2) else 0\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path2)\n",
    "    \n",
    "#     cu1_64 = df1.loc[0, \"depth_counts\"]\n",
    "#     cu1_32 = df1.loc[1, \"depth_counts\"]\n",
    "#     cu1_16 = df1.loc[2, \"depth_counts\"]\n",
    "#     cu1_8 = df1.loc[3, \"depth_counts\"]\n",
    "    \n",
    "#     cu2_64 = df2.loc[0, \"depth_counts\"]\n",
    "#     cu2_32 = df2.loc[1, \"depth_counts\"]\n",
    "#     cu2_16 = df2.loc[2, \"depth_counts\"]\n",
    "#     cu2_8 = df2.loc[3, \"depth_counts\"]\n",
    "\n",
    "    pu1_64 = df1.loc[0, \"pu_counts\"]\n",
    "    pu1_32 = df1.loc[1, \"pu_counts\"]\n",
    "    pu1_16 = df1.loc[2, \"pu_counts\"]\n",
    "    pu1_8 = df1.loc[3, \"pu_counts\"]\n",
    "    pu1_4 = df1.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    pu2_64 = df2.loc[0, \"pu_counts\"]\n",
    "    pu2_32 = df2.loc[1, \"pu_counts\"]\n",
    "    pu2_16 = df2.loc[2, \"pu_counts\"]\n",
    "    pu2_8 = df2.loc[3, \"pu_counts\"]\n",
    "    pu2_4 = df2.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    train_df1 = pd.concat([train_df1, pd.DataFrame({\n",
    "                                          \"PU1_64\": [pu1_64],\n",
    "                                          \"PU1_32\": [pu1_32],\n",
    "                                          \"PU1_16\": [pu1_16],\n",
    "                                          \"PU1_8\": [pu1_8],\n",
    "                                          \"PU1_4\": [pu1_4],\n",
    "                                          \n",
    "                                          \"PU2_64\": [pu2_64],\n",
    "                                          \"PU2_32\": [pu2_32],\n",
    "                                          \"PU2_16\": [pu2_16],\n",
    "                                          \"PU2_8\": [pu2_8],\n",
    "                                          \"PU2_4\": [pu2_4],\n",
    "\n",
    "                                          # \"LABEL\": [label]\n",
    "    })], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\n",
    "\n",
    "                                          \"LABEL\": [label]})], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "print(len(train_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "# test_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\", \"LABEL\"])\n",
    "test_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"])\n",
    "test_df2 = pd.DataFrame(columns=[\"LABEL\"])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2 in test_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path2) else 0\n",
    "\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path2)\n",
    "\n",
    "#     cu1_64 = df1.loc[0, \"depth_counts\"]\n",
    "#     cu1_32 = df1.loc[1, \"depth_counts\"]\n",
    "#     cu1_16 = df1.loc[2, \"depth_counts\"]\n",
    "#     cu1_8 = df1.loc[3, \"depth_counts\"]\n",
    "    \n",
    "#     cu2_64 = df2.loc[0, \"depth_counts\"]\n",
    "#     cu2_32 = df2.loc[1, \"depth_counts\"]\n",
    "#     cu2_16 = df2.loc[2, \"depth_counts\"]\n",
    "#     cu2_8 = df2.loc[3, \"depth_counts\"]\n",
    "\n",
    "    pu1_64 = df1.loc[0, \"pu_counts\"]\n",
    "    pu1_32 = df1.loc[1, \"pu_counts\"]\n",
    "    pu1_16 = df1.loc[2, \"pu_counts\"]\n",
    "    pu1_8 = df1.loc[3, \"pu_counts\"]\n",
    "    pu1_4 = df1.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    pu2_64 = df2.loc[0, \"pu_counts\"]\n",
    "    pu2_32 = df2.loc[1, \"pu_counts\"]\n",
    "    pu2_16 = df2.loc[2, \"pu_counts\"]\n",
    "    pu2_8 = df2.loc[3, \"pu_counts\"]\n",
    "    pu2_4 = df2.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    test_df1 = pd.concat([test_df1, pd.DataFrame({\n",
    "                                          \"PU1_64\": [pu1_64],\n",
    "                                          \"PU1_32\": [pu1_32],\n",
    "                                          \"PU1_16\": [pu1_16],\n",
    "                                          \"PU1_8\": [pu1_8],\n",
    "                                          \"PU1_4\": [pu1_4],\n",
    "                                          \n",
    "                                          \"PU2_64\": [pu2_64],\n",
    "                                          \"PU2_32\": [pu2_32],\n",
    "                                          \"PU2_16\": [pu2_16],\n",
    "                                          \"PU2_8\": [pu2_8],\n",
    "                                          \"PU2_4\": [pu2_4],\n",
    "\n",
    "                                          # \"LABEL\": [label]\n",
    "    })], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    \n",
    "    test_df2 = pd.concat([test_df2, pd.DataFrame({\n",
    "\n",
    "                                          \"LABEL\": [label]})], \n",
    "                   ignore_index=True)\n",
    "\n",
    "print(len(test_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df1, test_df1], ignore_index=True)\n",
    "# print(len(combined_df))\n",
    "\n",
    "# スケーラーを使って結合したデータをスケーリング\n",
    "combined_scaled_data = scaler.fit_transform(combined_df)\n",
    "\n",
    "# トレーニングデータとテストデータに再分割\n",
    "X_train = combined_scaled_data[:len(train_df1)]\n",
    "X_test = combined_scaled_data[len(train_df1):]\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "Y_test = test_df2['LABEL'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.1715847  0.46609125 0.63167104 0.42520103 0.\n",
      "  0.17030568 0.46994536 0.62080975 0.41602625]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:1])\n",
    "print(X_train[1350:1351])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.1442623  0.72133169 0.6746865  0.28815057 0.\n",
      "  0.1419214  0.73649059 0.66216805 0.27571022]\n",
      " [0.         0.37377049 0.89889026 0.41586468 0.04630202 0.\n",
      "  0.37991266 0.90892532 0.38441445 0.04170624]\n",
      " [0.         0.53005464 0.44636252 0.29760863 0.17522312 0.\n",
      "  0.53275109 0.44080146 0.28892759 0.17140143]\n",
      " [0.         0.78579235 0.26942047 0.14158647 0.04038173 0.\n",
      "  0.79148472 0.26168792 0.13147584 0.03954753]\n",
      " [0.         0.65245902 0.25462392 0.28331875 0.13545993 0.\n",
      "  0.64519651 0.26958106 0.27862429 0.13263103]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(test_df1[0:5])\n",
    "# print(test_df1[150:155])\n",
    "\n",
    "# X_test = scaler.fit_transform(test_df1)  \n",
    "# Y_test = test_df2['LABEL'].astype(int)\n",
    "\n",
    "print(X_test[0:5])\n",
    "print(X_test[150:155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       C       k=1       k=2       k=3       k=4       k=5       k=6       k=7       k=8       k=9  Mean_Val_Score\n",
      "0   0.01  0.783333  0.616667  0.550000  0.733333  0.683333  0.666667  0.600000  0.600000  0.633333        0.651852\n",
      "1    0.1  0.766667  0.616667  0.566667  0.716667  0.733333  0.683333  0.566667  0.600000  0.666667        0.657407\n",
      "2      1  0.783333  0.683333  0.600000  0.683333  0.733333  0.633333  0.683333  0.583333  0.650000        0.670370\n",
      "3     10  0.766667  0.766667  0.633333  0.666667  0.733333  0.616667  0.716667  0.600000  0.733333        0.692593\n",
      "4    100  0.833333  0.900000  0.716667  0.800000  0.883333  0.733333  0.783333  0.783333  0.833333        0.807407\n",
      "5   1000  0.866667  0.916667  0.766667  0.916667  0.883333  0.816667  0.900000  0.850000  0.866667        0.864815\n",
      "6   1500  0.866667  0.933333  0.783333  0.933333  0.900000  0.850000  0.933333  0.833333  0.883333        0.879630\n",
      "7   2000  0.900000  0.900000  0.783333  0.933333  0.916667  0.850000  0.933333  0.816667  0.883333        0.879630\n",
      "8   2500  0.900000  0.933333  0.783333  0.950000  0.916667  0.850000  0.933333  0.833333  0.883333        0.887037\n",
      "9   3000  0.900000  0.950000  0.783333  0.950000  0.916667  0.850000  0.950000  0.833333  0.883333        0.890741\n",
      "10  3500  0.916667  0.933333  0.783333  0.950000  0.883333  0.866667  0.950000  0.833333  0.883333        0.888889\n",
      "11  4000  0.900000  0.933333  0.766667  0.950000  0.900000  0.883333  0.950000  0.833333  0.883333        0.888889\n",
      "12  4500  0.883333  0.950000  0.783333  0.966667  0.900000  0.900000  0.950000  0.833333  0.883333        0.894444\n",
      "13  5000  0.883333  0.950000  0.783333  0.966667  0.900000  0.900000  0.950000  0.833333  0.883333        0.894444\n",
      "Best Parameters:  {'C': 4500}\n",
      "Accuracy on Test Set: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]}\n",
    "\n",
    "# SVMモデルのインスタンスを作成\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# グリッドサーチのインスタンスを作成\n",
    "grid_search = GridSearchCV(svm_model, C_values, cv=9, scoring='accuracy')\n",
    "\n",
    "# グリッドサーチを実行\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 結果のデータフレームを作成\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# print(results)\n",
    "\n",
    "# 新しい列名のマッピングを作成\n",
    "new_column_names = {\n",
    "    'param_C': 'C',\n",
    "    'split0_test_score': 'k=1',\n",
    "    'split1_test_score': 'k=2',\n",
    "    'split2_test_score': 'k=3',\n",
    "    'split3_test_score': 'k=4',\n",
    "    'split4_test_score': 'k=5',\n",
    "    'split5_test_score': 'k=6',\n",
    "    'split6_test_score': 'k=7',\n",
    "    'split7_test_score': 'k=8',\n",
    "    'split8_test_score': 'k=9',\n",
    "    'mean_test_score': 'Mean_Val_Score'\n",
    "}\n",
    "\n",
    "# 列名を変更\n",
    "results = results.rename(columns=new_column_names)\n",
    "\n",
    "# 変更後の表を表示\n",
    "print(results[['C', 'k=1', 'k=2', 'k=3', 'k=4', 'k=5', 'k=6', 'k=7', 'k=8', 'k=9', 'Mean_Val_Score']])\n",
    "    \n",
    "# # 最適なハイパーパラメータを表示\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "# # 最適なモデルを取得\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# # テストデータで評価\n",
    "accuracy = best_svm_model.score(X_test, Y_test)\n",
    "print(\"Accuracy on Test Set: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation performance of 9-fold: 0.8944\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        30\n",
      "           1       0.90      0.90      0.90        30\n",
      "\n",
      "    accuracy                           0.90        60\n",
      "   macro avg       0.90      0.90      0.90        60\n",
      "weighted avg       0.90      0.90      0.90        60\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2563589942375788"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PU1 + PU2\n",
    "\n",
    "k = 9\n",
    "\n",
    "# SVMモデルを初期化（RBFカーネルを使用）\n",
    "svm_model = SVC(kernel='rbf', C=4500, gamma='scale')  # Cとgammaはハイパーパラメータで調整可能\n",
    "\n",
    "\n",
    "# K-fold cross validation\n",
    "cv_scores = cross_val_score(svm_model, X_train, Y_train, cv=k)\n",
    "average_accuracy = np.round(cv_scores.mean(), 4)\n",
    "print(f'Average validation performance of {k}-fold: {average_accuracy}')\n",
    "\n",
    "svm_model.fit(X_train, Y_train)\n",
    "Y_pred = svm_model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "print(f'Summary:\\n{report}')\n",
    "\n",
    "svm_model._gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
