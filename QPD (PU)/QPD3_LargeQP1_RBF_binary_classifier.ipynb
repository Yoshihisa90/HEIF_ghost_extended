{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_csv_path1_list:  405\n",
      "single_train_csv_path2_list:  405\n",
      "second_train_csv_path1_list:  405\n",
      "second_train_csv_path2_list:  405\n",
      "single_test_csv_path1_list:  45\n",
      "single_test_csv_path2_list:  45\n",
      "second_test_csv_path1_list:  45\n",
      "second_test_csv_path2_list:  45\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/QPD/\"\n",
    "\n",
    "single_train_csv_path1 = os.path.join(rootpath, 'QPD3_HEIF_images_single_csv', 'TRAINING')\n",
    "single_train_csv_path2 = os.path.join(rootpath, 'QPD3_HEIF_images_second_sameQP_csv', 'TRAINING')\n",
    "\n",
    "second_train_csv_path1 = os.path.join(rootpath, 'QPD3_HEIF_images_second_csv', 'TRAINING')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'QPD3_HEIF_images_triple_csv', 'TRAINING')\n",
    "\n",
    "single_test_csv_path1 = os.path.join(rootpath, 'QPD3_HEIF_images_single_csv', 'TEST')\n",
    "single_test_csv_path2 = os.path.join(rootpath, 'QPD3_HEIF_images_second_sameQP_csv', 'TEST')\n",
    "\n",
    "second_test_csv_path1 = os.path.join(rootpath, 'QPD3_HEIF_images_second_csv', 'TEST')\n",
    "second_test_csv_path2 = os.path.join(rootpath, 'QPD3_HEIF_images_triple_csv', 'TEST')\n",
    "\n",
    "\n",
    "single_train_csv_path1_list = [os.path.join(single_train_csv_path1, file) for file in sorted(os.listdir(single_train_csv_path1))]\n",
    "single_train_csv_path2_list = [os.path.join(single_train_csv_path2, file) for file in sorted(os.listdir(single_train_csv_path2))]\n",
    "\n",
    "second_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "single_test_csv_path1_list = [os.path.join(single_test_csv_path1, file) for file in sorted(os.listdir(single_test_csv_path1))]\n",
    "single_test_csv_path2_list = [os.path.join(single_test_csv_path2, file) for file in sorted(os.listdir(single_test_csv_path2))]\n",
    "\n",
    "second_test_csv_path1_list = [os.path.join(second_test_csv_path1, file) for file in sorted(os.listdir(second_test_csv_path1))]\n",
    "second_test_csv_path2_list = [os.path.join(second_test_csv_path2, file) for file in sorted(os.listdir(second_test_csv_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_csv_path1_list: \", len(single_train_csv_path1_list))\n",
    "print(\"single_train_csv_path2_list: \", len(single_train_csv_path2_list))\n",
    "\n",
    "print(\"second_train_csv_path1_list: \", len(second_train_csv_path1_list))\n",
    "print(\"second_train_csv_path2_list: \", len(second_train_csv_path2_list))\n",
    "\n",
    "print(\"single_test_csv_path1_list: \", len(single_test_csv_path1_list))\n",
    "print(\"single_test_csv_path2_list: \", len(single_test_csv_path2_list))\n",
    "\n",
    "print(\"second_test_csv_path1_list: \", len(second_test_csv_path1_list))\n",
    "print(\"second_test_csv_path2_list: \", len(second_test_csv_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  810\n",
      "test_csv_list:  90\n"
     ]
    }
   ],
   "source": [
    "single_train_csv = list(zip(single_train_csv_path1_list, single_train_csv_path2_list))\n",
    "second_train_csv = list(zip(second_train_csv_path1_list, second_train_csv_path2_list))\n",
    "second_train_csv = random.sample(second_train_csv, len(single_train_csv))\n",
    "\n",
    "single_test_csv = list(zip(single_test_csv_path1_list, single_test_csv_path2_list))\n",
    "second_test_csv = list(zip(second_test_csv_path1_list, second_test_csv_path2_list))\n",
    "second_test_csv = random.sample(second_test_csv, len(single_test_csv))\n",
    "\n",
    "train_csv_list = single_train_csv + second_train_csv\n",
    "test_csv_list = single_test_csv + second_test_csv\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list))\n",
    "print(\"test_csv_list: \", len(test_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n"
     ]
    }
   ],
   "source": [
    "# new_df = pd.DataFrame(columns=[\"QP\", \"CU_64\", \"CU_32\", \"CU_16\", \"CU_8\", \"PU_64\", \"PU_32\", \"PU_16\", \"PU_8\", \"PU_4\", \"LUM_A\", \"LUM_B\", \"LUM_C\", \"CRM_34\", \"LABEL\"])\n",
    "# train_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\", \"LABEL\"])\n",
    "train_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"])\n",
    "train_df2 = pd.DataFrame(columns=[\"LABEL\"])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path2) else 0\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path2)\n",
    "    \n",
    "#     cu1_64 = df1.loc[0, \"depth_counts\"]\n",
    "#     cu1_32 = df1.loc[1, \"depth_counts\"]\n",
    "#     cu1_16 = df1.loc[2, \"depth_counts\"]\n",
    "#     cu1_8 = df1.loc[3, \"depth_counts\"]\n",
    "    \n",
    "#     cu2_64 = df2.loc[0, \"depth_counts\"]\n",
    "#     cu2_32 = df2.loc[1, \"depth_counts\"]\n",
    "#     cu2_16 = df2.loc[2, \"depth_counts\"]\n",
    "#     cu2_8 = df2.loc[3, \"depth_counts\"]\n",
    "\n",
    "    pu1_64 = df1.loc[0, \"pu_counts\"]\n",
    "    pu1_32 = df1.loc[1, \"pu_counts\"]\n",
    "    pu1_16 = df1.loc[2, \"pu_counts\"]\n",
    "    pu1_8 = df1.loc[3, \"pu_counts\"]\n",
    "    pu1_4 = df1.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    pu2_64 = df2.loc[0, \"pu_counts\"]\n",
    "    pu2_32 = df2.loc[1, \"pu_counts\"]\n",
    "    pu2_16 = df2.loc[2, \"pu_counts\"]\n",
    "    pu2_8 = df2.loc[3, \"pu_counts\"]\n",
    "    pu2_4 = df2.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    train_df1 = pd.concat([train_df1, pd.DataFrame({\n",
    "                                          \"PU1_64\": [pu1_64],\n",
    "                                          \"PU1_32\": [pu1_32],\n",
    "                                          \"PU1_16\": [pu1_16],\n",
    "                                          \"PU1_8\": [pu1_8],\n",
    "                                          \"PU1_4\": [pu1_4],\n",
    "                                          \n",
    "                                          \"PU2_64\": [pu2_64],\n",
    "                                          \"PU2_32\": [pu2_32],\n",
    "                                          \"PU2_16\": [pu2_16],\n",
    "                                          \"PU2_8\": [pu2_8],\n",
    "                                          \"PU2_4\": [pu2_4],\n",
    "\n",
    "                                          # \"LABEL\": [label]\n",
    "    })], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\n",
    "\n",
    "                                          \"LABEL\": [label]})], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "print(len(train_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# test_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\", \"LABEL\"])\n",
    "test_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"])\n",
    "test_df2 = pd.DataFrame(columns=[\"LABEL\"])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2 in test_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path2) else 0\n",
    "\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path2)\n",
    "\n",
    "#     cu1_64 = df1.loc[0, \"depth_counts\"]\n",
    "#     cu1_32 = df1.loc[1, \"depth_counts\"]\n",
    "#     cu1_16 = df1.loc[2, \"depth_counts\"]\n",
    "#     cu1_8 = df1.loc[3, \"depth_counts\"]\n",
    "    \n",
    "#     cu2_64 = df2.loc[0, \"depth_counts\"]\n",
    "#     cu2_32 = df2.loc[1, \"depth_counts\"]\n",
    "#     cu2_16 = df2.loc[2, \"depth_counts\"]\n",
    "#     cu2_8 = df2.loc[3, \"depth_counts\"]\n",
    "\n",
    "    pu1_64 = df1.loc[0, \"pu_counts\"]\n",
    "    pu1_32 = df1.loc[1, \"pu_counts\"]\n",
    "    pu1_16 = df1.loc[2, \"pu_counts\"]\n",
    "    pu1_8 = df1.loc[3, \"pu_counts\"]\n",
    "    pu1_4 = df1.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    pu2_64 = df2.loc[0, \"pu_counts\"]\n",
    "    pu2_32 = df2.loc[1, \"pu_counts\"]\n",
    "    pu2_16 = df2.loc[2, \"pu_counts\"]\n",
    "    pu2_8 = df2.loc[3, \"pu_counts\"]\n",
    "    pu2_4 = df2.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    test_df1 = pd.concat([test_df1, pd.DataFrame({\n",
    "                                          \"PU1_64\": [pu1_64],\n",
    "                                          \"PU1_32\": [pu1_32],\n",
    "                                          \"PU1_16\": [pu1_16],\n",
    "                                          \"PU1_8\": [pu1_8],\n",
    "                                          \"PU1_4\": [pu1_4],\n",
    "                                          \n",
    "                                          \"PU2_64\": [pu2_64],\n",
    "                                          \"PU2_32\": [pu2_32],\n",
    "                                          \"PU2_16\": [pu2_16],\n",
    "                                          \"PU2_8\": [pu2_8],\n",
    "                                          \"PU2_4\": [pu2_4],\n",
    "\n",
    "                                          # \"LABEL\": [label]\n",
    "    })], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    \n",
    "    test_df2 = pd.concat([test_df2, pd.DataFrame({\n",
    "\n",
    "                                          \"LABEL\": [label]})], \n",
    "                   ignore_index=True)\n",
    "\n",
    "print(len(test_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df1, test_df1], ignore_index=True)\n",
    "# print(len(combined_df))\n",
    "\n",
    "# スケーラーを使って結合したデータをスケーリング\n",
    "combined_scaled_data = scaler.fit_transform(combined_df)\n",
    "\n",
    "# トレーニングデータとテストデータに再分割\n",
    "X_train = combined_scaled_data[:len(train_df1)]\n",
    "X_test = combined_scaled_data[len(train_df1):]\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "Y_test = test_df2['LABEL'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.20130577 0.47576602 0.61196363 0.38760362 0.\n",
      "  0.20108696 0.49084859 0.62782507 0.37220281]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:1])\n",
    "print(X_train[1350:1351])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.16322089 0.74373259 0.614237   0.25457875 0.\n",
      "  0.1673913  0.75429839 0.62503669 0.2414314 ]\n",
      " [0.         0.25244831 0.78050139 0.54276783 0.1511471  0.\n",
      "  0.275      0.76372712 0.53845025 0.14115759]\n",
      " [0.         0.41784548 0.8005571  0.35848252 0.02785811 0.\n",
      "  0.45217391 0.77925679 0.32286469 0.02303843]\n",
      " [0.         0.58215452 0.4        0.23884626 0.15336418 0.\n",
      "  0.58586957 0.40654465 0.22688582 0.15277122]\n",
      " [0.         0.69423286 0.27799443 0.19309463 0.1099865  0.\n",
      "  0.69456522 0.27897948 0.19327854 0.1101879 ]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(test_df1[0:5])\n",
    "# print(test_df1[150:155])\n",
    "\n",
    "# X_test = scaler.fit_transform(test_df1)  \n",
    "# Y_test = test_df2['LABEL'].astype(int)\n",
    "\n",
    "print(X_test[0:5])\n",
    "print(X_test[150:155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       C       k=1       k=2       k=3       k=4       k=5       k=6       k=7       k=8       k=9  Mean_Val_Score\n",
      "0   0.01  0.600000  0.533333  0.544444  0.544444  0.644444  0.488889  0.455556  0.511111  0.566667        0.543210\n",
      "1    0.1  0.611111  0.533333  0.533333  0.555556  0.633333  0.500000  0.466667  0.500000  0.555556        0.543210\n",
      "2      1  0.622222  0.544444  0.533333  0.600000  0.611111  0.466667  0.455556  0.455556  0.555556        0.538272\n",
      "3     10  0.611111  0.566667  0.555556  0.588889  0.644444  0.522222  0.477778  0.444444  0.566667        0.553086\n",
      "4    100  0.777778  0.633333  0.677778  0.777778  0.777778  0.711111  0.700000  0.655556  0.666667        0.708642\n",
      "5   1000  0.866667  0.711111  0.766667  0.744444  0.844444  0.755556  0.766667  0.644444  0.744444        0.760494\n",
      "6   1500  0.855556  0.722222  0.766667  0.744444  0.800000  0.744444  0.777778  0.622222  0.711111        0.749383\n",
      "7   2000  0.855556  0.744444  0.766667  0.733333  0.811111  0.722222  0.744444  0.655556  0.733333        0.751852\n",
      "8   2500  0.855556  0.755556  0.755556  0.744444  0.800000  0.733333  0.766667  0.677778  0.733333        0.758025\n",
      "9   3000  0.855556  0.755556  0.755556  0.733333  0.811111  0.744444  0.766667  0.666667  0.722222        0.756790\n",
      "10  3500  0.855556  0.766667  0.733333  0.733333  0.811111  0.744444  0.777778  0.677778  0.700000        0.755556\n",
      "11  4000  0.866667  0.777778  0.744444  0.733333  0.822222  0.733333  0.777778  0.677778  0.711111        0.760494\n",
      "12  4500  0.866667  0.788889  0.733333  0.722222  0.800000  0.722222  0.777778  0.700000  0.711111        0.758025\n",
      "13  5000  0.866667  0.788889  0.733333  0.722222  0.800000  0.722222  0.777778  0.700000  0.711111        0.758025\n",
      "Best Parameters:  {'C': 1000}\n",
      "Accuracy on Test Set: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]}\n",
    "\n",
    "# SVMモデルのインスタンスを作成\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# グリッドサーチのインスタンスを作成\n",
    "grid_search = GridSearchCV(svm_model, C_values, cv=9, scoring='accuracy')\n",
    "\n",
    "# グリッドサーチを実行\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 結果のデータフレームを作成\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# print(results)\n",
    "\n",
    "# 新しい列名のマッピングを作成\n",
    "new_column_names = {\n",
    "    'param_C': 'C',\n",
    "    'split0_test_score': 'k=1',\n",
    "    'split1_test_score': 'k=2',\n",
    "    'split2_test_score': 'k=3',\n",
    "    'split3_test_score': 'k=4',\n",
    "    'split4_test_score': 'k=5',\n",
    "    'split5_test_score': 'k=6',\n",
    "    'split6_test_score': 'k=7',\n",
    "    'split7_test_score': 'k=8',\n",
    "    'split8_test_score': 'k=9',\n",
    "    'mean_test_score': 'Mean_Val_Score'\n",
    "}\n",
    "\n",
    "# 列名を変更\n",
    "results = results.rename(columns=new_column_names)\n",
    "\n",
    "# 変更後の表を表示\n",
    "print(results[['C', 'k=1', 'k=2', 'k=3', 'k=4', 'k=5', 'k=6', 'k=7', 'k=8', 'k=9', 'Mean_Val_Score']])\n",
    "    \n",
    "# # 最適なハイパーパラメータを表示\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "# # 最適なモデルを取得\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# # テストデータで評価\n",
    "accuracy = best_svm_model.score(X_test, Y_test)\n",
    "print(\"Accuracy on Test Set: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation performance of 9-fold: 0.7605\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89        45\n",
      "           1       0.89      0.89      0.89        45\n",
      "\n",
      "    accuracy                           0.89        90\n",
      "   macro avg       0.89      0.89      0.89        90\n",
      "weighted avg       0.89      0.89      0.89        90\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.447400971202581"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PU1 + PU2\n",
    "\n",
    "k = 9\n",
    "\n",
    "# SVMモデルを初期化（RBFカーネルを使用）\n",
    "svm_model = SVC(kernel='rbf', C=1000, gamma='scale')  # Cとgammaはハイパーパラメータで調整可能\n",
    "\n",
    "\n",
    "# K-fold cross validation\n",
    "cv_scores = cross_val_score(svm_model, X_train, Y_train, cv=k)\n",
    "average_accuracy = np.round(cv_scores.mean(), 4)\n",
    "print(f'Average validation performance of {k}-fold: {average_accuracy}')\n",
    "\n",
    "svm_model.fit(X_train, Y_train)\n",
    "Y_pred = svm_model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "print(f'Summary:\\n{report}')\n",
    "\n",
    "svm_model._gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
