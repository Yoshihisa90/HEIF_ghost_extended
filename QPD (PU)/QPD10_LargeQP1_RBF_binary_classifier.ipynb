{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_train_csv_path1_list:  405\n",
      "single_train_csv_path2_list:  405\n",
      "second_train_csv_path1_list:  405\n",
      "second_train_csv_path2_list:  405\n",
      "single_test_csv_path1_list:  45\n",
      "single_test_csv_path2_list:  45\n",
      "second_test_csv_path1_list:  45\n",
      "second_test_csv_path2_list:  45\n"
     ]
    }
   ],
   "source": [
    "rootpath = \"/Prove/Yoshihisa/HEIF_ghost/QPD/\"\n",
    "\n",
    "single_train_csv_path1 = os.path.join(rootpath, 'QPD10_HEIF_images_single_csv', 'TRAINING')\n",
    "single_train_csv_path2 = os.path.join(rootpath, 'QPD10_HEIF_images_second_sameQP_csv', 'TRAINING')\n",
    "\n",
    "second_train_csv_path1 = os.path.join(rootpath, 'QPD10_HEIF_images_second_csv', 'TRAINING')\n",
    "second_train_csv_path2 = os.path.join(rootpath, 'QPD10_HEIF_images_triple_csv', 'TRAINING')\n",
    "\n",
    "single_test_csv_path1 = os.path.join(rootpath, 'QPD10_HEIF_images_single_csv', 'TEST')\n",
    "single_test_csv_path2 = os.path.join(rootpath, 'QPD10_HEIF_images_second_sameQP_csv', 'TEST')\n",
    "\n",
    "second_test_csv_path1 = os.path.join(rootpath, 'QPD10_HEIF_images_second_csv', 'TEST')\n",
    "second_test_csv_path2 = os.path.join(rootpath, 'QPD10_HEIF_images_triple_csv', 'TEST')\n",
    "\n",
    "\n",
    "single_train_csv_path1_list = [os.path.join(single_train_csv_path1, file) for file in sorted(os.listdir(single_train_csv_path1))]\n",
    "single_train_csv_path2_list = [os.path.join(single_train_csv_path2, file) for file in sorted(os.listdir(single_train_csv_path2))]\n",
    "\n",
    "second_train_csv_path1_list = [os.path.join(second_train_csv_path1, file) for file in sorted(os.listdir(second_train_csv_path1))]\n",
    "second_train_csv_path2_list = [os.path.join(second_train_csv_path2, file) for file in sorted(os.listdir(second_train_csv_path2))]\n",
    "\n",
    "single_test_csv_path1_list = [os.path.join(single_test_csv_path1, file) for file in sorted(os.listdir(single_test_csv_path1))]\n",
    "single_test_csv_path2_list = [os.path.join(single_test_csv_path2, file) for file in sorted(os.listdir(single_test_csv_path2))]\n",
    "\n",
    "second_test_csv_path1_list = [os.path.join(second_test_csv_path1, file) for file in sorted(os.listdir(second_test_csv_path1))]\n",
    "second_test_csv_path2_list = [os.path.join(second_test_csv_path2, file) for file in sorted(os.listdir(second_test_csv_path2))]\n",
    "\n",
    "\n",
    "print(\"single_train_csv_path1_list: \", len(single_train_csv_path1_list))\n",
    "print(\"single_train_csv_path2_list: \", len(single_train_csv_path2_list))\n",
    "\n",
    "print(\"second_train_csv_path1_list: \", len(second_train_csv_path1_list))\n",
    "print(\"second_train_csv_path2_list: \", len(second_train_csv_path2_list))\n",
    "\n",
    "print(\"single_test_csv_path1_list: \", len(single_test_csv_path1_list))\n",
    "print(\"single_test_csv_path2_list: \", len(single_test_csv_path2_list))\n",
    "\n",
    "print(\"second_test_csv_path1_list: \", len(second_test_csv_path1_list))\n",
    "print(\"second_test_csv_path2_list: \", len(second_test_csv_path2_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  810\n",
      "test_csv_list:  90\n"
     ]
    }
   ],
   "source": [
    "single_train_csv = list(zip(single_train_csv_path1_list, single_train_csv_path2_list))\n",
    "second_train_csv = list(zip(second_train_csv_path1_list, second_train_csv_path2_list))\n",
    "second_train_csv = random.sample(second_train_csv, len(single_train_csv))\n",
    "\n",
    "single_test_csv = list(zip(single_test_csv_path1_list, single_test_csv_path2_list))\n",
    "second_test_csv = list(zip(second_test_csv_path1_list, second_test_csv_path2_list))\n",
    "second_test_csv = random.sample(second_test_csv, len(single_test_csv))\n",
    "\n",
    "train_csv_list = single_train_csv + second_train_csv\n",
    "test_csv_list = single_test_csv + second_test_csv\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list))\n",
    "print(\"test_csv_list: \", len(test_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n"
     ]
    }
   ],
   "source": [
    "# new_df = pd.DataFrame(columns=[\"QP\", \"CU_64\", \"CU_32\", \"CU_16\", \"CU_8\", \"PU_64\", \"PU_32\", \"PU_16\", \"PU_8\", \"PU_4\", \"LUM_A\", \"LUM_B\", \"LUM_C\", \"CRM_34\", \"LABEL\"])\n",
    "# train_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\", \"LABEL\"])\n",
    "train_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"])\n",
    "train_df2 = pd.DataFrame(columns=[\"LABEL\"])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path2) else 0\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path2)\n",
    "    \n",
    "#     cu1_64 = df1.loc[0, \"depth_counts\"]\n",
    "#     cu1_32 = df1.loc[1, \"depth_counts\"]\n",
    "#     cu1_16 = df1.loc[2, \"depth_counts\"]\n",
    "#     cu1_8 = df1.loc[3, \"depth_counts\"]\n",
    "    \n",
    "#     cu2_64 = df2.loc[0, \"depth_counts\"]\n",
    "#     cu2_32 = df2.loc[1, \"depth_counts\"]\n",
    "#     cu2_16 = df2.loc[2, \"depth_counts\"]\n",
    "#     cu2_8 = df2.loc[3, \"depth_counts\"]\n",
    "\n",
    "    pu1_64 = df1.loc[0, \"pu_counts\"]\n",
    "    pu1_32 = df1.loc[1, \"pu_counts\"]\n",
    "    pu1_16 = df1.loc[2, \"pu_counts\"]\n",
    "    pu1_8 = df1.loc[3, \"pu_counts\"]\n",
    "    pu1_4 = df1.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    pu2_64 = df2.loc[0, \"pu_counts\"]\n",
    "    pu2_32 = df2.loc[1, \"pu_counts\"]\n",
    "    pu2_16 = df2.loc[2, \"pu_counts\"]\n",
    "    pu2_8 = df2.loc[3, \"pu_counts\"]\n",
    "    pu2_4 = df2.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    train_df1 = pd.concat([train_df1, pd.DataFrame({\n",
    "                                          \"PU1_64\": [pu1_64],\n",
    "                                          \"PU1_32\": [pu1_32],\n",
    "                                          \"PU1_16\": [pu1_16],\n",
    "                                          \"PU1_8\": [pu1_8],\n",
    "                                          \"PU1_4\": [pu1_4],\n",
    "                                          \n",
    "                                          \"PU2_64\": [pu2_64],\n",
    "                                          \"PU2_32\": [pu2_32],\n",
    "                                          \"PU2_16\": [pu2_16],\n",
    "                                          \"PU2_8\": [pu2_8],\n",
    "                                          \"PU2_4\": [pu2_4],\n",
    "\n",
    "                                          # \"LABEL\": [label]\n",
    "    })], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\n",
    "\n",
    "                                          \"LABEL\": [label]})], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "print(len(train_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# test_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\", \"LABEL\"])\n",
    "test_df1 = pd.DataFrame(columns=[\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"])\n",
    "test_df2 = pd.DataFrame(columns=[\"LABEL\"])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2 in test_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path2) else 0\n",
    "\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path2)\n",
    "\n",
    "#     cu1_64 = df1.loc[0, \"depth_counts\"]\n",
    "#     cu1_32 = df1.loc[1, \"depth_counts\"]\n",
    "#     cu1_16 = df1.loc[2, \"depth_counts\"]\n",
    "#     cu1_8 = df1.loc[3, \"depth_counts\"]\n",
    "    \n",
    "#     cu2_64 = df2.loc[0, \"depth_counts\"]\n",
    "#     cu2_32 = df2.loc[1, \"depth_counts\"]\n",
    "#     cu2_16 = df2.loc[2, \"depth_counts\"]\n",
    "#     cu2_8 = df2.loc[3, \"depth_counts\"]\n",
    "\n",
    "    pu1_64 = df1.loc[0, \"pu_counts\"]\n",
    "    pu1_32 = df1.loc[1, \"pu_counts\"]\n",
    "    pu1_16 = df1.loc[2, \"pu_counts\"]\n",
    "    pu1_8 = df1.loc[3, \"pu_counts\"]\n",
    "    pu1_4 = df1.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    pu2_64 = df2.loc[0, \"pu_counts\"]\n",
    "    pu2_32 = df2.loc[1, \"pu_counts\"]\n",
    "    pu2_16 = df2.loc[2, \"pu_counts\"]\n",
    "    pu2_8 = df2.loc[3, \"pu_counts\"]\n",
    "    pu2_4 = df2.loc[4, \"pu_counts\"]\n",
    "    \n",
    "    test_df1 = pd.concat([test_df1, pd.DataFrame({\n",
    "                                          \"PU1_64\": [pu1_64],\n",
    "                                          \"PU1_32\": [pu1_32],\n",
    "                                          \"PU1_16\": [pu1_16],\n",
    "                                          \"PU1_8\": [pu1_8],\n",
    "                                          \"PU1_4\": [pu1_4],\n",
    "                                          \n",
    "                                          \"PU2_64\": [pu2_64],\n",
    "                                          \"PU2_32\": [pu2_32],\n",
    "                                          \"PU2_16\": [pu2_16],\n",
    "                                          \"PU2_8\": [pu2_8],\n",
    "                                          \"PU2_4\": [pu2_4],\n",
    "\n",
    "                                          # \"LABEL\": [label]\n",
    "    })], \n",
    "                   ignore_index=True)\n",
    "    \n",
    "    \n",
    "    test_df2 = pd.concat([test_df2, pd.DataFrame({\n",
    "\n",
    "                                          \"LABEL\": [label]})], \n",
    "                   ignore_index=True)\n",
    "\n",
    "print(len(test_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df1, test_df1], ignore_index=True)\n",
    "# print(len(combined_df))\n",
    "\n",
    "# スケーラーを使って結合したデータをスケーリング\n",
    "combined_scaled_data = scaler.fit_transform(combined_df)\n",
    "\n",
    "# トレーニングデータとテストデータに再分割\n",
    "X_train = combined_scaled_data[:len(train_df1)]\n",
    "X_test = combined_scaled_data[len(train_df1):]\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "Y_test = test_df2['LABEL'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.07223476 0.21170905 0.51526558 0.6320303  0.\n",
      "  0.07344633 0.21090047 0.47472678 0.6312818 ]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:1])\n",
    "print(X_train[1350:1351])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.05079007 0.23950325 0.61257493 0.59043808 0.\n",
      "  0.04858757 0.24348341 0.59672131 0.57823935]\n",
      " [0.         0.11512415 0.58663513 0.7171337  0.30386622 0.\n",
      "  0.11186441 0.60959716 0.69357923 0.29406845]\n",
      " [0.         0.0248307  0.12300414 0.69203959 0.63231616 0.\n",
      "  0.02485876 0.12559242 0.64972678 0.63009688]\n",
      " [0.         0.13205418 0.27735068 0.54370556 0.5251197  0.\n",
      "  0.13220339 0.27725118 0.52076503 0.51857531]\n",
      " [0.         0.41534989 0.65345949 0.23351457 0.21532195 0.\n",
      "  0.41468927 0.66291469 0.22199454 0.21070607]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(test_df1[0:5])\n",
    "# print(test_df1[150:155])\n",
    "\n",
    "# X_test = scaler.fit_transform(test_df1)  \n",
    "# Y_test = test_df2['LABEL'].astype(int)\n",
    "\n",
    "print(X_test[0:5])\n",
    "print(X_test[150:155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       C       k=1       k=2       k=3       k=4       k=5       k=6       k=7       k=8       k=9  Mean_Val_Score\n",
      "0   0.01  0.700000  0.677778  0.666667  0.788889  0.700000  0.655556  0.644444  0.666667  0.700000        0.688889\n",
      "1    0.1  0.677778  0.644444  0.677778  0.788889  0.722222  0.666667  0.644444  0.688889  0.711111        0.691358\n",
      "2      1  0.677778  0.655556  0.644444  0.811111  0.733333  0.666667  0.644444  0.677778  0.733333        0.693827\n",
      "3     10  0.688889  0.633333  0.666667  0.800000  0.711111  0.677778  0.622222  0.688889  0.688889        0.686420\n",
      "4    100  0.711111  0.633333  0.655556  0.766667  0.722222  0.700000  0.644444  0.700000  0.677778        0.690123\n",
      "5   1000  0.744444  0.666667  0.688889  0.800000  0.766667  0.666667  0.633333  0.711111  0.700000        0.708642\n",
      "6   1500  0.755556  0.644444  0.688889  0.800000  0.800000  0.677778  0.611111  0.700000  0.688889        0.707407\n",
      "7   2000  0.744444  0.644444  0.688889  0.777778  0.822222  0.700000  0.622222  0.688889  0.677778        0.707407\n",
      "8   2500  0.744444  0.644444  0.722222  0.766667  0.800000  0.711111  0.622222  0.677778  0.677778        0.707407\n",
      "9   3000  0.733333  0.655556  0.722222  0.766667  0.811111  0.733333  0.622222  0.666667  0.677778        0.709877\n",
      "10  3500  0.733333  0.655556  0.733333  0.777778  0.822222  0.733333  0.622222  0.677778  0.677778        0.714815\n",
      "11  4000  0.733333  0.655556  0.733333  0.766667  0.811111  0.755556  0.633333  0.666667  0.677778        0.714815\n",
      "12  4500  0.744444  0.644444  0.733333  0.766667  0.811111  0.766667  0.622222  0.666667  0.677778        0.714815\n",
      "13  5000  0.744444  0.644444  0.722222  0.777778  0.811111  0.766667  0.633333  0.655556  0.677778        0.714815\n",
      "Best Parameters:  {'C': 3500}\n",
      "Accuracy on Test Set: 0.7222\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]}\n",
    "\n",
    "# SVMモデルのインスタンスを作成\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# グリッドサーチのインスタンスを作成\n",
    "grid_search = GridSearchCV(svm_model, C_values, cv=9, scoring='accuracy')\n",
    "\n",
    "# グリッドサーチを実行\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 結果のデータフレームを作成\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# print(results)\n",
    "\n",
    "# 新しい列名のマッピングを作成\n",
    "new_column_names = {\n",
    "    'param_C': 'C',\n",
    "    'split0_test_score': 'k=1',\n",
    "    'split1_test_score': 'k=2',\n",
    "    'split2_test_score': 'k=3',\n",
    "    'split3_test_score': 'k=4',\n",
    "    'split4_test_score': 'k=5',\n",
    "    'split5_test_score': 'k=6',\n",
    "    'split6_test_score': 'k=7',\n",
    "    'split7_test_score': 'k=8',\n",
    "    'split8_test_score': 'k=9',\n",
    "    'mean_test_score': 'Mean_Val_Score'\n",
    "}\n",
    "\n",
    "# 列名を変更\n",
    "results = results.rename(columns=new_column_names)\n",
    "\n",
    "# 変更後の表を表示\n",
    "print(results[['C', 'k=1', 'k=2', 'k=3', 'k=4', 'k=5', 'k=6', 'k=7', 'k=8', 'k=9', 'Mean_Val_Score']])\n",
    "    \n",
    "# # 最適なハイパーパラメータを表示\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "# # 最適なモデルを取得\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# # テストデータで評価\n",
    "accuracy = best_svm_model.score(X_test, Y_test)\n",
    "print(\"Accuracy on Test Set: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation performance of 9-fold: 0.7148\n",
      "Summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.73        45\n",
      "           1       0.73      0.71      0.72        45\n",
      "\n",
      "    accuracy                           0.72        90\n",
      "   macro avg       0.72      0.72      0.72        90\n",
      "weighted avg       0.72      0.72      0.72        90\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4513926302232407"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PU1 + PU2\n",
    "\n",
    "k = 9\n",
    "\n",
    "# SVMモデルを初期化（RBFカーネルを使用）\n",
    "svm_model = SVC(kernel='rbf', C=3500, gamma='scale')  # Cとgammaはハイパーパラメータで調整可能\n",
    "\n",
    "\n",
    "# K-fold cross validation\n",
    "cv_scores = cross_val_score(svm_model, X_train, Y_train, cv=k)\n",
    "average_accuracy = np.round(cv_scores.mean(), 4)\n",
    "print(f'Average validation performance of {k}-fold: {average_accuracy}')\n",
    "\n",
    "svm_model.fit(X_train, Y_train)\n",
    "Y_pred = svm_model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "print(f'Summary:\\n{report}')\n",
    "\n",
    "svm_model._gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
