{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示\n",
    "\n",
    "plt.rcParams[\"font.size\"]=5\n",
    "plt.rcParams[\"figure.figsize\"]=(2.0, 1.0)\n",
    "plt.rcParams[\"figure.dpi\"]= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def is_double_compressed(mean_difference, final_QP, threshold):    \n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "\n",
    "    # energy_clamp = torch.clamp(mean_difference, min=0)\n",
    "    # energy_clamp = torch.clamp(mean_difference)\n",
    "    # energy = torch.sum(torch.square(energy_clamp))\n",
    "    # mean_difference_right_clamp = torch.clamp(mean_difference[final_QP+1:52], min=0)\n",
    "    energy = torch.sum(torch.square(mean_difference))\n",
    "    # mean_difference_right_clamp = torch.clamp(mean_difference[final_QP+1:52])\n",
    "    right_energy = torch.sum(torch.square(mean_difference[final_QP+1:52]))\n",
    "        \n",
    "    if energy > 0:\n",
    "        energy_ratio = right_energy / energy\n",
    "        if energy_ratio > threshold:\n",
    "            return True\n",
    "        elif energy_ratio <= threshold:\n",
    "            return False\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = loaded_data\n",
    "    shifted_mae = loaded_data_shifted\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = [shifted - original for original, shifted in zip(original_mae, shifted_mae)]\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    mae_difference_positive = [0 if val < 0 else val for val in mae_difference]\n",
    "    \n",
    "    # mae_difference_positiveをtensorに変換\n",
    "    mae_difference_tensor = torch.tensor(mae_difference_positive)\n",
    "    \n",
    "    \n",
    "    return mae_difference_positive, mae_difference_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_list1:  3080\n",
      "single_list2:  3080\n",
      "\n",
      "second_largeQP1_list1:  17556\n",
      "second_largeQP1_list2:  17556\n",
      "second_sameQP_list1:  3080\n",
      "second_sameQP_list2:  3080\n",
      "second_largeQP_list1:  12012\n",
      "second_largeQP_list2:  12012\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "\n",
    "single_path1 = os.path.join(rootpath_csv, 'HEIF_images_single_csv')\n",
    "single_path2 = os.path.join(rootpath_csv, 'HEIF_images_second_sameQP_csv')\n",
    "single_list1 = [os.path.join(single_path1, file) for file in sorted(os.listdir(single_path1))]\n",
    "single_list2 = [os.path.join(single_path2, file) for file in sorted(os.listdir(single_path2))]\n",
    "\n",
    "second_largeQP1_path1 = os.path.join(rootpath_csv, 'HEIF_images_second_csv')\n",
    "second_largeQP1_path2 = os.path.join(rootpath_csv, 'HEIF_images_triple_csv')\n",
    "second_largeQP1_list1 = [os.path.join(second_largeQP1_path1, file) for file in sorted(os.listdir(second_largeQP1_path1))]\n",
    "second_largeQP1_list2 = [os.path.join(second_largeQP1_path2, file) for file in sorted(os.listdir(second_largeQP1_path2))]\n",
    "\n",
    "second_sameQP_path1 = os.path.join(rootpath_csv, 'HEIF_images_second_sameQP_csv')\n",
    "second_sameQP_path2 = os.path.join(rootpath_csv, 'HEIF_images_triple_sameQP_csv')\n",
    "second_sameQP_list1 = [os.path.join(second_sameQP_path1, file) for file in sorted(os.listdir(second_sameQP_path1))]\n",
    "second_sameQP_list2 = [os.path.join(second_sameQP_path2, file) for file in sorted(os.listdir(second_sameQP_path2))]\n",
    "\n",
    "second_largeQP2_path1 = os.path.join(rootpath_csv, 'HEIF_images_second_largeQP_csv')\n",
    "second_largeQP2_path2 = os.path.join(rootpath_csv, 'HEIF_images_triple_largeQP_csv')\n",
    "second_largeQP2_list1 = [os.path.join(second_largeQP2_path1, file) for file in sorted(os.listdir(second_largeQP2_path1))]\n",
    "second_largeQP2_list2 = [os.path.join(second_largeQP2_path2, file) for file in sorted(os.listdir(second_largeQP2_path2))]\n",
    "\n",
    "print(\"single_list1: \", len(single_list1))\n",
    "print(\"single_list2: \", len(single_list2))\n",
    "print()\n",
    "print(\"second_largeQP1_list1: \", len(second_largeQP1_list1))\n",
    "print(\"second_largeQP1_list2: \", len(second_largeQP1_list2))\n",
    "print(\"second_sameQP_list1: \", len(second_sameQP_list1))\n",
    "print(\"second_sameQP_list2: \", len(second_sameQP_list2))\n",
    "print(\"second_largeQP_list1: \", len(second_largeQP2_list1))\n",
    "print(\"second_largeQP_list2: \", len(second_largeQP2_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_listA:  3080\n",
      "single_listB:  3080\n",
      "\n",
      "second_largeQP1_listA:  17556\n",
      "second_largeQP1_listB:  17556\n",
      "second_sameQP_listA:  3080\n",
      "second_sameQP_listB:  3080\n",
      "second_largeQP2_listA:  12012\n",
      "second_largeQP2_listB:  12012\n"
     ]
    }
   ],
   "source": [
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "single_pathA = os.path.join(rootpath_pkl, 'pkl_single')\n",
    "single_pathB = os.path.join(rootpath_pkl, 'pkl_second_sameQP')\n",
    "single_listA = [os.path.join(single_pathA, file) for file in sorted(os.listdir(single_pathA))]\n",
    "single_listB = [os.path.join(single_pathB, file) for file in sorted(os.listdir(single_pathB))]\n",
    "\n",
    "second_largeQP1_pathA = os.path.join(rootpath_pkl, 'pkl_second')\n",
    "second_largeQP1_pathB = os.path.join(rootpath_pkl, 'pkl_triple')\n",
    "second_largeQP1_listA = [os.path.join(second_largeQP1_pathA, file) for file in sorted(os.listdir(second_largeQP1_pathA))]\n",
    "second_largeQP1_listB = [os.path.join(second_largeQP1_pathB, file) for file in sorted(os.listdir(second_largeQP1_pathB))]\n",
    "\n",
    "second_sameQP_pathA = os.path.join(rootpath_pkl, 'pkl_second_sameQP')\n",
    "second_sameQP_pathB = os.path.join(rootpath_pkl, 'pkl_triple_sameQP')\n",
    "second_sameQP_listA = [os.path.join(second_sameQP_pathA, file) for file in sorted(os.listdir(second_sameQP_pathA))]\n",
    "second_sameQP_listB = [os.path.join(second_sameQP_pathB, file) for file in sorted(os.listdir(second_sameQP_pathB))]\n",
    "\n",
    "second_largeQP2_pathA = os.path.join(rootpath_pkl, 'pkl_second_largeQP')\n",
    "second_largeQP2_pathB = os.path.join(rootpath_pkl, 'pkl_triple_largeQP')\n",
    "second_largeQP2_listA = [os.path.join(second_largeQP2_pathA, file) for file in sorted(os.listdir(second_largeQP2_pathA))]\n",
    "second_largeQP2_listB = [os.path.join(second_largeQP2_pathB, file) for file in sorted(os.listdir(second_largeQP2_pathB))]\n",
    "\n",
    "print(\"single_listA: \", len(single_listA))\n",
    "print(\"single_listB: \", len(single_listB))\n",
    "print()\n",
    "print(\"second_largeQP1_listA: \", len(second_largeQP1_listA))\n",
    "print(\"second_largeQP1_listB: \", len(second_largeQP1_listB))\n",
    "print(\"second_sameQP_listA: \", len(second_sameQP_listA))\n",
    "print(\"second_sameQP_listB: \", len(second_sameQP_listB))\n",
    "print(\"second_largeQP2_listA: \", len(second_largeQP2_listA))\n",
    "print(\"second_largeQP2_listB: \", len(second_largeQP2_listB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  6000\n"
     ]
    }
   ],
   "source": [
    "single_csv = list(zip(single_list1, single_listA, single_list2, single_listB))\n",
    "single_csv = random.sample(single_csv, 3000)\n",
    "\n",
    "second_largeQP1_csv = list(zip(second_largeQP1_list1, second_largeQP1_listA, second_largeQP1_list2, second_largeQP1_listB))\n",
    "second_largeQP1_csv = random.sample(second_largeQP1_csv, 1000)\n",
    "second_sameQP_csv = list(zip(second_sameQP_list1, second_sameQP_listA, second_sameQP_list2, second_sameQP_listB))\n",
    "second_sameQP_csv = random.sample(second_sameQP_csv, 1000)\n",
    "second_largeQP2_csv = list(zip(second_largeQP2_list1, second_largeQP2_listA, second_largeQP2_list2, second_largeQP2_listB))\n",
    "second_largeQP2_csv = random.sample(second_largeQP2_csv, 1000)\n",
    "\n",
    "train_csv_list = single_csv + second_largeQP1_csv + second_sameQP_csv + second_largeQP2_csv\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "              \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "luminance_columns = [\"LU1_0\", \"LU1_1\", \"LU1_9\", \"LU1_10\", \"LU1_11\", \"LU1_25\", \"LU1_26\", \"LU1_27\", \n",
    "                     \"LU2_0\", \"LU2_1\", \"LU2_9\", \"LU2_10\", \"LU2_11\", \"LU2_25\", \"LU2_26\", \"LU2_27\"]\n",
    "chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                       \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    train_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "    \n",
    "    train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "\n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(train_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1_1.reset_index(drop=True, inplace=True)\n",
    "train_df1_2.reset_index(drop=True, inplace=True)\n",
    "train_df1_3.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 6000\n",
      "Length of train_df_onlyGhost: 6000\n",
      "Length of train_df5: 6000\n",
      "Length of train_df6: 6000\n"
     ]
    }
   ],
   "source": [
    "# 各データフレームの長さを表示\n",
    "print(f'Length of train_df: {len(train_df)}')\n",
    "print(f'Length of train_df_onlyGhost: {len(train_df_onlyGhost)}')\n",
    "print(f'Length of train_df5: {len(train_df5)}')\n",
    "print(f'Length of train_df6: {len(train_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 6000\n",
      "Length of X_train_onlyGhost: 6000\n",
      "Length of Y_train: 6000\n",
      "Length of MAE: 6000\n",
      "Length of FINAL_QP: 6000\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "MAE = train_df5.values\n",
    "FINAL_QP = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of MAE: {len(MAE)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7771    0.8367    0.8058       300\n",
      "           1     0.8231    0.7600    0.7903       300\n",
      "\n",
      "    accuracy                         0.7983       600\n",
      "   macro avg     0.8001    0.7983    0.7980       600\n",
      "weighted avg     0.8001    0.7983    0.7980       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8194    0.8467    0.8328       300\n",
      "           1     0.8414    0.8133    0.8271       300\n",
      "\n",
      "    accuracy                         0.8300       600\n",
      "   macro avg     0.8304    0.8300    0.8300       600\n",
      "weighted avg     0.8304    0.8300    0.8300       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7714    0.8100    0.7902       300\n",
      "           1     0.8000    0.7600    0.7795       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7857    0.7850    0.7849       600\n",
      "weighted avg     0.7857    0.7850    0.7849       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7836    0.7967    0.7901       300\n",
      "           1     0.7932    0.7800    0.7866       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7884    0.7883    0.7883       600\n",
      "weighted avg     0.7884    0.7883    0.7883       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5887    0.9733    0.7337       300\n",
      "           1     0.9412    0.3200    0.4776       300\n",
      "\n",
      "   micro avg     0.6488    0.6467    0.6477       600\n",
      "   macro avg     0.7649    0.6467    0.6056       600\n",
      "weighted avg     0.7649    0.6467    0.6056       600\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8284    0.8367    0.8325       300\n",
      "           1     0.8350    0.8267    0.8308       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8317    0.8317    0.8317       600\n",
      "weighted avg     0.8317    0.8317    0.8317       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8350    0.8600    0.8473       300\n",
      "           1     0.8557    0.8300    0.8426       300\n",
      "\n",
      "    accuracy                         0.8450       600\n",
      "   macro avg     0.8453    0.8450    0.8450       600\n",
      "weighted avg     0.8453    0.8450    0.8450       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7665    0.8533    0.8076       300\n",
      "           1     0.8346    0.7400    0.7845       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8005    0.7967    0.7960       600\n",
      "weighted avg     0.8005    0.7967    0.7960       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7738    0.8667    0.8176       300\n",
      "           1     0.8485    0.7467    0.7943       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8111    0.8067    0.8060       600\n",
      "weighted avg     0.8111    0.8067    0.8060       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5766    0.9533    0.7186       300\n",
      "           1     0.8824    0.3000    0.4478       300\n",
      "\n",
      "   micro avg     0.6288    0.6267    0.6277       600\n",
      "   macro avg     0.7295    0.6267    0.5832       600\n",
      "weighted avg     0.7295    0.6267    0.5832       600\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8092    0.8767    0.8416       300\n",
      "           1     0.8655    0.7933    0.8278       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8373    0.8350    0.8347       600\n",
      "weighted avg     0.8373    0.8350    0.8347       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8302    0.8967    0.8622       300\n",
      "           1     0.8877    0.8167    0.8507       300\n",
      "\n",
      "    accuracy                         0.8567       600\n",
      "   macro avg     0.8590    0.8567    0.8564       600\n",
      "weighted avg     0.8590    0.8567    0.8564       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7794    0.8833    0.8281       300\n",
      "           1     0.8654    0.7500    0.8036       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8224    0.8167    0.8158       600\n",
      "weighted avg     0.8224    0.8167    0.8158       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7963    0.8600    0.8269       300\n",
      "           1     0.8478    0.7800    0.8125       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8221    0.8200    0.8197       600\n",
      "weighted avg     0.8221    0.8200    0.8197       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5953    0.7600    0.6676       300\n",
      "           1     0.6713    0.4833    0.5620       300\n",
      "\n",
      "   micro avg     0.6227    0.6217    0.6222       600\n",
      "   macro avg     0.6333    0.6217    0.6148       600\n",
      "weighted avg     0.6333    0.6217    0.6148       600\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8141    0.8467    0.8301       300\n",
      "           1     0.8403    0.8067    0.8231       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8272    0.8267    0.8266       600\n",
      "weighted avg     0.8272    0.8267    0.8266       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8187    0.9033    0.8590       300\n",
      "           1     0.8922    0.8000    0.8436       300\n",
      "\n",
      "    accuracy                         0.8517       600\n",
      "   macro avg     0.8555    0.8517    0.8513       600\n",
      "weighted avg     0.8555    0.8517    0.8513       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7584    0.8267    0.7911       300\n",
      "           1     0.8095    0.7367    0.7714       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7840    0.7817    0.7812       600\n",
      "weighted avg     0.7840    0.7817    0.7812       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7725    0.8600    0.8139       300\n",
      "           1     0.8421    0.7467    0.7915       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8073    0.8033    0.8027       600\n",
      "weighted avg     0.8073    0.8033    0.8027       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6761    0.5567    0.6106       300\n",
      "           1     0.6250    0.7333    0.6748       300\n",
      "\n",
      "   micro avg     0.6461    0.6450    0.6455       600\n",
      "   macro avg     0.6506    0.6450    0.6427       600\n",
      "weighted avg     0.6506    0.6450    0.6427       600\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8104    0.8833    0.8453       300\n",
      "           1     0.8718    0.7933    0.8307       300\n",
      "\n",
      "    accuracy                         0.8383       600\n",
      "   macro avg     0.8411    0.8383    0.8380       600\n",
      "weighted avg     0.8411    0.8383    0.8380       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8297    0.8933    0.8604       300\n",
      "           1     0.8845    0.8167    0.8492       300\n",
      "\n",
      "    accuracy                         0.8550       600\n",
      "   macro avg     0.8571    0.8550    0.8548       600\n",
      "weighted avg     0.8571    0.8550    0.8548       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7660    0.8400    0.8013       300\n",
      "           1     0.8229    0.7433    0.7811       300\n",
      "\n",
      "    accuracy                         0.7917       600\n",
      "   macro avg     0.7944    0.7917    0.7912       600\n",
      "weighted avg     0.7944    0.7917    0.7912       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7906    0.8433    0.8161       300\n",
      "           1     0.8321    0.7767    0.8034       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8114    0.8100    0.8098       600\n",
      "weighted avg     0.8114    0.8100    0.8098       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5948    0.8467    0.6988       300\n",
      "           1     0.7326    0.4200    0.5339       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6637    0.6333    0.6163       600\n",
      "weighted avg     0.6637    0.6333    0.6163       600\n",
      "\n",
      "<Fold-6>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7784    0.9133    0.8405       300\n",
      "           1     0.8952    0.7400    0.8102       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8368    0.8267    0.8254       600\n",
      "weighted avg     0.8368    0.8267    0.8254       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8043    0.8767    0.8389       300\n",
      "           1     0.8645    0.7867    0.8237       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8344    0.8317    0.8313       600\n",
      "weighted avg     0.8344    0.8317    0.8313       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7485    0.8133    0.7796       300\n",
      "           1     0.7956    0.7267    0.7596       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7720    0.7700    0.7696       600\n",
      "weighted avg     0.7720    0.7700    0.7696       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7664    0.8200    0.7923       300\n",
      "           1     0.8065    0.7500    0.7772       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7864    0.7850    0.7847       600\n",
      "weighted avg     0.7864    0.7850    0.7847       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5669    0.9467    0.7091       300\n",
      "           1     0.8557    0.2767    0.4181       300\n",
      "\n",
      "   micro avg     0.6137    0.6117    0.6127       600\n",
      "   macro avg     0.7113    0.6117    0.5636       600\n",
      "weighted avg     0.7113    0.6117    0.5636       600\n",
      "\n",
      "<Fold-7>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8095    0.8500    0.8293       300\n",
      "           1     0.8421    0.8000    0.8205       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8258    0.8250    0.8249       600\n",
      "weighted avg     0.8258    0.8250    0.8249       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7918    0.9000    0.8424       300\n",
      "           1     0.8842    0.7633    0.8193       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8380    0.8317    0.8309       600\n",
      "weighted avg     0.8380    0.8317    0.8309       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7441    0.8433    0.7906       300\n",
      "           1     0.8192    0.7100    0.7607       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7817    0.7767    0.7757       600\n",
      "weighted avg     0.7817    0.7767    0.7757       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7370    0.8500    0.7895       300\n",
      "           1     0.8228    0.6967    0.7545       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7799    0.7733    0.7720       600\n",
      "weighted avg     0.7799    0.7733    0.7720       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6930    0.5267    0.5985       300\n",
      "           1     0.6199    0.7667    0.6855       300\n",
      "\n",
      "   micro avg     0.6477    0.6467    0.6472       600\n",
      "   macro avg     0.6565    0.6467    0.6420       600\n",
      "weighted avg     0.6565    0.6467    0.6420       600\n",
      "\n",
      "<Fold-8>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7645    0.8333    0.7974       300\n",
      "           1     0.8168    0.7433    0.7784       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7907    0.7883    0.7879       600\n",
      "weighted avg     0.7907    0.7883    0.7879       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8073    0.8100    0.8087       300\n",
      "           1     0.8094    0.8067    0.8080       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8083    0.8083    0.8083       600\n",
      "weighted avg     0.8083    0.8083    0.8083       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7360    0.7900    0.7621       300\n",
      "           1     0.7734    0.7167    0.7439       300\n",
      "\n",
      "    accuracy                         0.7533       600\n",
      "   macro avg     0.7547    0.7533    0.7530       600\n",
      "weighted avg     0.7547    0.7533    0.7530       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7579    0.8033    0.7799       300\n",
      "           1     0.7908    0.7433    0.7663       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7743    0.7733    0.7731       600\n",
      "weighted avg     0.7743    0.7733    0.7731       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5853    0.9267    0.7174       300\n",
      "           1     0.8306    0.3433    0.4858       300\n",
      "\n",
      "   micro avg     0.6361    0.6350    0.6355       600\n",
      "   macro avg     0.7080    0.6350    0.6016       600\n",
      "weighted avg     0.7080    0.6350    0.6016       600\n",
      "\n",
      "<Fold-9>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7697    0.8467    0.8063       300\n",
      "           1     0.8296    0.7467    0.7860       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.7997    0.7967    0.7962       600\n",
      "weighted avg     0.7997    0.7967    0.7962       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8125    0.8667    0.8387       300\n",
      "           1     0.8571    0.8000    0.8276       300\n",
      "\n",
      "    accuracy                         0.8333       600\n",
      "   macro avg     0.8348    0.8333    0.8331       600\n",
      "weighted avg     0.8348    0.8333    0.8331       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7325    0.8033    0.7663       300\n",
      "           1     0.7823    0.7067    0.7426       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.7574    0.7550    0.7544       600\n",
      "weighted avg     0.7574    0.7550    0.7544       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7618    0.8100    0.7851       300\n",
      "           1     0.7972    0.7467    0.7711       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7795    0.7783    0.7781       600\n",
      "weighted avg     0.7795    0.7783    0.7781       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.6400    0.6358       300\n",
      "           1     0.6351    0.6267    0.6309       300\n",
      "\n",
      "    accuracy                         0.6333       600\n",
      "   macro avg     0.6334    0.6333    0.6333       600\n",
      "weighted avg     0.6334    0.6333    0.6333       600\n",
      "\n",
      "<Fold-10>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8317    0.8567    0.8440       300\n",
      "           1     0.8522    0.8267    0.8393       300\n",
      "\n",
      "    accuracy                         0.8417       600\n",
      "   macro avg     0.8420    0.8417    0.8416       600\n",
      "weighted avg     0.8420    0.8417    0.8416       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8138    0.9033    0.8562       300\n",
      "           1     0.8914    0.7933    0.8395       300\n",
      "\n",
      "    accuracy                         0.8483       600\n",
      "   macro avg     0.8526    0.8483    0.8479       600\n",
      "weighted avg     0.8526    0.8483    0.8479       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7633    0.8600    0.8088       300\n",
      "           1     0.8397    0.7333    0.7829       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8015    0.7967    0.7958       600\n",
      "weighted avg     0.8015    0.7967    0.7958       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7846    0.8500    0.8160       300\n",
      "           1     0.8364    0.7667    0.8000       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8105    0.8083    0.8080       600\n",
      "weighted avg     0.8105    0.8083    0.8080       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5851    0.9167    0.7143       300\n",
      "           1     0.8140    0.3500    0.4895       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6995    0.6333    0.6019       600\n",
      "weighted avg     0.6995    0.6333    0.6019       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Test_Score_RBF', 'C_LINEAR', 'Test_Score_LINEAR', \n",
    "                                'C_onlyGhost_RBF', 'Test_Score_onlyGhost_RBF', 'C_onlyGhost_LINEAR', 'Test_Score_onlyGhost_LINEAR',\n",
    "                                'Threshold', 'Test_Score_old'])\n",
    "\n",
    "initial_X, initial_X_onlyGhost = X_train, X_train_onlyGhost\n",
    "initial_Y = Y_train\n",
    "initial_old, initial_final_QP = MAE, FINAL_QP\n",
    "\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(initial_X, initial_Y)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    results_old = []\n",
    "\n",
    "    # 全体を訓練・検証データとテストデータに分割\n",
    "    X_train_val, X_test = initial_X[train_ids], initial_X[test_ids]\n",
    "    X_train_onlyGhost_val, X_test_onlyGhost = initial_X_onlyGhost[train_ids], initial_X_onlyGhost[test_ids]\n",
    "    X_train_old_val, X_test_old = initial_old[train_ids], initial_old[test_ids]\n",
    "    \n",
    "    final_QP = initial_final_QP[test_ids]\n",
    "    \n",
    "    # 全体を訓練・検証ラベルとテストラベルに分割\n",
    "    Y_train_val, Y_test = initial_Y[train_ids], initial_Y[test_ids]\n",
    "    \n",
    "    # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=600, random_state=42)\n",
    "    X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=600, random_state=42)\n",
    "    \n",
    "    # for i in range(600): \n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    for threshold in np.arange(0.01,1.01,0.01):\n",
    "        results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(600)]\n",
    "        # results_old.append((is_double))\n",
    "        \n",
    "        predicted_labels = [int(is_double) for is_double in results_old]\n",
    "        ground_truth_labels = [label for label in Y_test]\n",
    "        accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(X_test)\n",
    "    test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(Y_test, test_predictions_RBF)\n",
    "    report_RBF = classification_report(Y_test, test_predictions_RBF, digits=4)\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test)\n",
    "    test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(Y_test, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(Y_test, test_predictions_LINEAR, digits=4)\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(Y_test, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(Y_test, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(Y_test, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Test_Score_RBF': test_accuracy_RBF,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Test_Score_LINEAR': test_accuracy_LINEAR,\n",
    "              'C_onlyGhost_RBF': best_c_value_onlyGhost_RBF, 'Test_Score_onlyGhost_RBF': test_accuracy_onlyGhost_RBF,\n",
    "              'C_onlyGhost_LINEAR': best_c_value_onlyGhost_LINEAR, 'Test_Score_onlyGhost_LINEAR': test_accuracy_onlyGhost_LINEAR,\n",
    "              'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Score with RBF: 0.82\n",
      "Standard Deviation of Test Score with RBF: 0.02\n",
      "Maximum Test Score with RBF: 0.84\n",
      "Minimum Test Score with RBF: 0.79\n",
      "\n",
      "Average Test Score with LINEAR: 0.84\n",
      "Standard Deviation of Test Score with LINEAR: 0.01\n",
      "Maximum Test Score with LINEAR: 0.86\n",
      "Minimum Test Score with LINEAR: 0.81\n",
      "\n",
      "Average Test Score with only Ghost and RBF: 0.78\n",
      "Standard Deviation of Test Score with only Ghost and RBF: 0.02\n",
      "Maximum Test Score with only Ghost and RBF: 0.82\n",
      "Minimum Test Score with only Ghost and RBF: 0.75\n",
      "\n",
      "Average Test Score with only Ghost and LINEAR: 0.79\n",
      "Standard Deviation of Test Score with only Ghost and LINEAR: 0.02\n",
      "Maximum Test Score with only Ghost and LINEAR: 0.82\n",
      "Minimum Test Score with only Ghost and LINEAR: 0.77\n",
      "\n",
      "Average Test Score with old model: 0.63\n",
      "Standard Deviation of Test Score with old model: 0.01\n",
      "Maximum Test Score with old model: 0.65\n",
      "Minimum Test Score with old model: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stats(column_name, label):\n",
    "    average = round(results[column_name].mean(), 2)\n",
    "    std_dev = round(results[column_name].std(), 2)\n",
    "    max_value = round(results[column_name].max(), 2)\n",
    "    min_value = round(results[column_name].min(), 2)\n",
    "\n",
    "    print(f'Average Test Score {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {label}: {min_value}')\n",
    "    print()\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "print_stats('Test_Score_RBF', 'with RBF')\n",
    "print_stats('Test_Score_LINEAR', 'with LINEAR')\n",
    "\n",
    "# 'Test_Score_onlyGhost'列に関して統計情報を表示\n",
    "print_stats('Test_Score_onlyGhost_RBF', 'with only Ghost and RBF')\n",
    "print_stats('Test_Score_onlyGhost_LINEAR', 'with only Ghost and LINEAR')\n",
    "\n",
    "# 'Test_Score_old'列に関して統計情報を表示\n",
    "print_stats('Test_Score_old', 'with old model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed PU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "              \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "luminance_columns = [\"LU1_0\", \"LU1_1\", \"LU1_9\", \"LU1_10\", \"LU1_11\", \"LU1_25\", \"LU1_26\", \"LU1_27\", \n",
    "                     \"LU2_0\", \"LU2_1\", \"LU2_9\", \"LU2_10\", \"LU2_11\", \"LU2_25\", \"LU2_26\", \"LU2_27\"]\n",
    "chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                       \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    train_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "    \n",
    "    train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "\n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(train_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1_1.reset_index(drop=True, inplace=True)\n",
    "train_df1_2.reset_index(drop=True, inplace=True)\n",
    "train_df1_3.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 6000\n",
      "Length of train_df_onlyGhost: 6000\n",
      "Length of train_df5: 6000\n",
      "Length of train_df6: 6000\n"
     ]
    }
   ],
   "source": [
    "# 各データフレームの長さを表示\n",
    "print(f'Length of train_df: {len(train_df)}')\n",
    "print(f'Length of train_df_onlyGhost: {len(train_df_onlyGhost)}')\n",
    "print(f'Length of train_df5: {len(train_df5)}')\n",
    "print(f'Length of train_df6: {len(train_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 6000\n",
      "Length of X_train_onlyGhost: 6000\n",
      "Length of Y_train: 6000\n",
      "Length of MAE: 6000\n",
      "Length of FINAL_QP: 6000\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "MAE = train_df5.values\n",
    "FINAL_QP = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of MAE: {len(MAE)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7853    0.8533    0.8179       300\n",
      "           1     0.8394    0.7667    0.8014       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8123    0.8100    0.8096       600\n",
      "weighted avg     0.8123    0.8100    0.8096       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8051    0.8400    0.8222       300\n",
      "           1     0.8328    0.7967    0.8143       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8189    0.8183    0.8182       600\n",
      "weighted avg     0.8189    0.8183    0.8182       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7714    0.8100    0.7902       300\n",
      "           1     0.8000    0.7600    0.7795       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7857    0.7850    0.7849       600\n",
      "weighted avg     0.7857    0.7850    0.7849       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7836    0.7967    0.7901       300\n",
      "           1     0.7932    0.7800    0.7866       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7884    0.7883    0.7883       600\n",
      "weighted avg     0.7884    0.7883    0.7883       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5887    0.9733    0.7337       300\n",
      "           1     0.9412    0.3200    0.4776       300\n",
      "\n",
      "   micro avg     0.6488    0.6467    0.6477       600\n",
      "   macro avg     0.7649    0.6467    0.6056       600\n",
      "weighted avg     0.7649    0.6467    0.6056       600\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8154    0.8100    0.8127       300\n",
      "           1     0.8113    0.8167    0.8140       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8133    0.8133    0.8133       600\n",
      "weighted avg     0.8133    0.8133    0.8133       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8219    0.8767    0.8484       300\n",
      "           1     0.8679    0.8100    0.8379       300\n",
      "\n",
      "    accuracy                         0.8433       600\n",
      "   macro avg     0.8449    0.8433    0.8432       600\n",
      "weighted avg     0.8449    0.8433    0.8432       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7665    0.8533    0.8076       300\n",
      "           1     0.8346    0.7400    0.7845       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8005    0.7967    0.7960       600\n",
      "weighted avg     0.8005    0.7967    0.7960       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7738    0.8667    0.8176       300\n",
      "           1     0.8485    0.7467    0.7943       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8111    0.8067    0.8060       600\n",
      "weighted avg     0.8111    0.8067    0.8060       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5766    0.9533    0.7186       300\n",
      "           1     0.8824    0.3000    0.4478       300\n",
      "\n",
      "   micro avg     0.6288    0.6267    0.6277       600\n",
      "   macro avg     0.7295    0.6267    0.5832       600\n",
      "weighted avg     0.7295    0.6267    0.5832       600\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7915    0.8733    0.8304       300\n",
      "           1     0.8587    0.7700    0.8120       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8251    0.8217    0.8212       600\n",
      "weighted avg     0.8251    0.8217    0.8212       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8292    0.8900    0.8585       300\n",
      "           1     0.8813    0.8167    0.8478       300\n",
      "\n",
      "    accuracy                         0.8533       600\n",
      "   macro avg     0.8552    0.8533    0.8531       600\n",
      "weighted avg     0.8552    0.8533    0.8531       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7794    0.8833    0.8281       300\n",
      "           1     0.8654    0.7500    0.8036       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8224    0.8167    0.8158       600\n",
      "weighted avg     0.8224    0.8167    0.8158       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7963    0.8600    0.8269       300\n",
      "           1     0.8478    0.7800    0.8125       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8221    0.8200    0.8197       600\n",
      "weighted avg     0.8221    0.8200    0.8197       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5953    0.7600    0.6676       300\n",
      "           1     0.6713    0.4833    0.5620       300\n",
      "\n",
      "   micro avg     0.6227    0.6217    0.6222       600\n",
      "   macro avg     0.6333    0.6217    0.6148       600\n",
      "weighted avg     0.6333    0.6217    0.6148       600\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8044    0.8500    0.8266       300\n",
      "           1     0.8410    0.7933    0.8165       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8227    0.8217    0.8215       600\n",
      "weighted avg     0.8227    0.8217    0.8215       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8185    0.8867    0.8512       300\n",
      "           1     0.8764    0.8033    0.8383       300\n",
      "\n",
      "    accuracy                         0.8450       600\n",
      "   macro avg     0.8474    0.8450    0.8447       600\n",
      "weighted avg     0.8474    0.8450    0.8447       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7584    0.8267    0.7911       300\n",
      "           1     0.8095    0.7367    0.7714       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7840    0.7817    0.7812       600\n",
      "weighted avg     0.7840    0.7817    0.7812       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7725    0.8600    0.8139       300\n",
      "           1     0.8421    0.7467    0.7915       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8073    0.8033    0.8027       600\n",
      "weighted avg     0.8073    0.8033    0.8027       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6761    0.5567    0.6106       300\n",
      "           1     0.6250    0.7333    0.6748       300\n",
      "\n",
      "   micro avg     0.6461    0.6450    0.6455       600\n",
      "   macro avg     0.6506    0.6450    0.6427       600\n",
      "weighted avg     0.6506    0.6450    0.6427       600\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8168    0.8767    0.8457       300\n",
      "           1     0.8669    0.8033    0.8339       300\n",
      "\n",
      "    accuracy                         0.8400       600\n",
      "   macro avg     0.8418    0.8400    0.8398       600\n",
      "weighted avg     0.8418    0.8400    0.8398       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8344    0.8733    0.8534       300\n",
      "           1     0.8671    0.8267    0.8464       300\n",
      "\n",
      "    accuracy                         0.8500       600\n",
      "   macro avg     0.8508    0.8500    0.8499       600\n",
      "weighted avg     0.8508    0.8500    0.8499       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7660    0.8400    0.8013       300\n",
      "           1     0.8229    0.7433    0.7811       300\n",
      "\n",
      "    accuracy                         0.7917       600\n",
      "   macro avg     0.7944    0.7917    0.7912       600\n",
      "weighted avg     0.7944    0.7917    0.7912       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7906    0.8433    0.8161       300\n",
      "           1     0.8321    0.7767    0.8034       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8114    0.8100    0.8098       600\n",
      "weighted avg     0.8114    0.8100    0.8098       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5948    0.8467    0.6988       300\n",
      "           1     0.7326    0.4200    0.5339       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6637    0.6333    0.6163       600\n",
      "weighted avg     0.6637    0.6333    0.6163       600\n",
      "\n",
      "<Fold-6>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7730    0.8967    0.8302       300\n",
      "           1     0.8770    0.7367    0.8007       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8250    0.8167    0.8155       600\n",
      "weighted avg     0.8250    0.8167    0.8155       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7909    0.8700    0.8286       300\n",
      "           1     0.8556    0.7700    0.8105       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8232    0.8200    0.8195       600\n",
      "weighted avg     0.8232    0.8200    0.8195       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7485    0.8133    0.7796       300\n",
      "           1     0.7956    0.7267    0.7596       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7720    0.7700    0.7696       600\n",
      "weighted avg     0.7720    0.7700    0.7696       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7664    0.8200    0.7923       300\n",
      "           1     0.8065    0.7500    0.7772       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7864    0.7850    0.7847       600\n",
      "weighted avg     0.7864    0.7850    0.7847       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5669    0.9467    0.7091       300\n",
      "           1     0.8557    0.2767    0.4181       300\n",
      "\n",
      "   micro avg     0.6137    0.6117    0.6127       600\n",
      "   macro avg     0.7113    0.6117    0.5636       600\n",
      "weighted avg     0.7113    0.6117    0.5636       600\n",
      "\n",
      "<Fold-7>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7630    0.8800    0.8173       300\n",
      "           1     0.8583    0.7267    0.7870       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8106    0.8033    0.8022       600\n",
      "weighted avg     0.8106    0.8033    0.8022       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8036    0.9000    0.8491       300\n",
      "           1     0.8864    0.7800    0.8298       300\n",
      "\n",
      "    accuracy                         0.8400       600\n",
      "   macro avg     0.8450    0.8400    0.8394       600\n",
      "weighted avg     0.8450    0.8400    0.8394       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7441    0.8433    0.7906       300\n",
      "           1     0.8192    0.7100    0.7607       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7817    0.7767    0.7757       600\n",
      "weighted avg     0.7817    0.7767    0.7757       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7370    0.8500    0.7895       300\n",
      "           1     0.8228    0.6967    0.7545       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7799    0.7733    0.7720       600\n",
      "weighted avg     0.7799    0.7733    0.7720       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6930    0.5267    0.5985       300\n",
      "           1     0.6199    0.7667    0.6855       300\n",
      "\n",
      "   micro avg     0.6477    0.6467    0.6472       600\n",
      "   macro avg     0.6565    0.6467    0.6420       600\n",
      "weighted avg     0.6565    0.6467    0.6420       600\n",
      "\n",
      "<Fold-8>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7578    0.8133    0.7846       300\n",
      "           1     0.7986    0.7400    0.7682       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7782    0.7767    0.7764       600\n",
      "weighted avg     0.7782    0.7767    0.7764       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8072    0.8233    0.8152       300\n",
      "           1     0.8197    0.8033    0.8114       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8135    0.8133    0.8133       600\n",
      "weighted avg     0.8135    0.8133    0.8133       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7360    0.7900    0.7621       300\n",
      "           1     0.7734    0.7167    0.7439       300\n",
      "\n",
      "    accuracy                         0.7533       600\n",
      "   macro avg     0.7547    0.7533    0.7530       600\n",
      "weighted avg     0.7547    0.7533    0.7530       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7579    0.8033    0.7799       300\n",
      "           1     0.7908    0.7433    0.7663       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7743    0.7733    0.7731       600\n",
      "weighted avg     0.7743    0.7733    0.7731       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5853    0.9267    0.7174       300\n",
      "           1     0.8306    0.3433    0.4858       300\n",
      "\n",
      "   micro avg     0.6361    0.6350    0.6355       600\n",
      "   macro avg     0.7080    0.6350    0.6016       600\n",
      "weighted avg     0.7080    0.6350    0.6016       600\n",
      "\n",
      "<Fold-9>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8400    0.8195       300\n",
      "           1     0.8316    0.7900    0.8103       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8158    0.8150    0.8149       600\n",
      "weighted avg     0.8158    0.8150    0.8149       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.8700    0.8433       300\n",
      "           1     0.8612    0.8067    0.8330       300\n",
      "\n",
      "    accuracy                         0.8383       600\n",
      "   macro avg     0.8397    0.8383    0.8382       600\n",
      "weighted avg     0.8397    0.8383    0.8382       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7325    0.8033    0.7663       300\n",
      "           1     0.7823    0.7067    0.7426       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.7574    0.7550    0.7544       600\n",
      "weighted avg     0.7574    0.7550    0.7544       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7618    0.8100    0.7851       300\n",
      "           1     0.7972    0.7467    0.7711       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7795    0.7783    0.7781       600\n",
      "weighted avg     0.7795    0.7783    0.7781       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.6400    0.6358       300\n",
      "           1     0.6351    0.6267    0.6309       300\n",
      "\n",
      "    accuracy                         0.6333       600\n",
      "   macro avg     0.6334    0.6333    0.6333       600\n",
      "weighted avg     0.6334    0.6333    0.6333       600\n",
      "\n",
      "<Fold-10>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8339    0.8700    0.8515       300\n",
      "           1     0.8641    0.8267    0.8450       300\n",
      "\n",
      "    accuracy                         0.8483       600\n",
      "   macro avg     0.8490    0.8483    0.8483       600\n",
      "weighted avg     0.8490    0.8483    0.8483       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8080    0.8700    0.8379       300\n",
      "           1     0.8592    0.7933    0.8250       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8336    0.8317    0.8314       600\n",
      "weighted avg     0.8336    0.8317    0.8314       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7633    0.8600    0.8088       300\n",
      "           1     0.8397    0.7333    0.7829       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8015    0.7967    0.7958       600\n",
      "weighted avg     0.8015    0.7967    0.7958       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7846    0.8500    0.8160       300\n",
      "           1     0.8364    0.7667    0.8000       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8105    0.8083    0.8080       600\n",
      "weighted avg     0.8105    0.8083    0.8080       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5851    0.9167    0.7143       300\n",
      "           1     0.8140    0.3500    0.4895       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6995    0.6333    0.6019       600\n",
      "weighted avg     0.6995    0.6333    0.6019       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Test_Score_RBF', 'C_LINEAR', 'Test_Score_LINEAR', \n",
    "                                'C_onlyGhost_RBF', 'Test_Score_onlyGhost_RBF', 'C_onlyGhost_LINEAR', 'Test_Score_onlyGhost_LINEAR',\n",
    "                                'Threshold', 'Test_Score_old'])\n",
    "\n",
    "initial_X, initial_X_onlyGhost = X_train, X_train_onlyGhost\n",
    "initial_Y = Y_train\n",
    "initial_old, initial_final_QP = MAE, FINAL_QP\n",
    "\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(initial_X, initial_Y)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    results_old = []\n",
    "\n",
    "    # 全体を訓練・検証データとテストデータに分割\n",
    "    X_train_val, X_test = initial_X[train_ids], initial_X[test_ids]\n",
    "    X_train_onlyGhost_val, X_test_onlyGhost = initial_X_onlyGhost[train_ids], initial_X_onlyGhost[test_ids]\n",
    "    X_train_old_val, X_test_old = initial_old[train_ids], initial_old[test_ids]\n",
    "    \n",
    "    final_QP = initial_final_QP[test_ids]\n",
    "    \n",
    "    # 全体を訓練・検証ラベルとテストラベルに分割\n",
    "    Y_train_val, Y_test = initial_Y[train_ids], initial_Y[test_ids]\n",
    "    \n",
    "    # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=600, random_state=42)\n",
    "    X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=600, random_state=42)\n",
    "    \n",
    "    # for i in range(600): \n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    for threshold in np.arange(0.01,1.01,0.01):\n",
    "        results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(600)]\n",
    "        # results_old.append((is_double))\n",
    "        \n",
    "        predicted_labels = [int(is_double) for is_double in results_old]\n",
    "        ground_truth_labels = [label for label in Y_test]\n",
    "        accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(X_test)\n",
    "    test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(Y_test, test_predictions_RBF)\n",
    "    report_RBF = classification_report(Y_test, test_predictions_RBF, digits=4)\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test)\n",
    "    test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(Y_test, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(Y_test, test_predictions_LINEAR, digits=4)\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(Y_test, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(Y_test, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(Y_test, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Test_Score_RBF': test_accuracy_RBF,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Test_Score_LINEAR': test_accuracy_LINEAR,\n",
    "              'C_onlyGhost_RBF': best_c_value_onlyGhost_RBF, 'Test_Score_onlyGhost_RBF': test_accuracy_onlyGhost_RBF,\n",
    "              'C_onlyGhost_LINEAR': best_c_value_onlyGhost_LINEAR, 'Test_Score_onlyGhost_LINEAR': test_accuracy_onlyGhost_LINEAR,\n",
    "              'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Score with RBF: 0.82\n",
      "Standard Deviation of Test Score with RBF: 0.02\n",
      "Maximum Test Score with RBF: 0.85\n",
      "Minimum Test Score with RBF: 0.78\n",
      "\n",
      "Average Test Score with LINEAR: 0.84\n",
      "Standard Deviation of Test Score with LINEAR: 0.01\n",
      "Maximum Test Score with LINEAR: 0.85\n",
      "Minimum Test Score with LINEAR: 0.81\n",
      "\n",
      "Average Test Score with only Ghost and RBF: 0.78\n",
      "Standard Deviation of Test Score with only Ghost and RBF: 0.02\n",
      "Maximum Test Score with only Ghost and RBF: 0.82\n",
      "Minimum Test Score with only Ghost and RBF: 0.75\n",
      "\n",
      "Average Test Score with only Ghost and LINEAR: 0.79\n",
      "Standard Deviation of Test Score with only Ghost and LINEAR: 0.02\n",
      "Maximum Test Score with only Ghost and LINEAR: 0.82\n",
      "Minimum Test Score with only Ghost and LINEAR: 0.77\n",
      "\n",
      "Average Test Score with old model: 0.63\n",
      "Standard Deviation of Test Score with old model: 0.01\n",
      "Maximum Test Score with old model: 0.65\n",
      "Minimum Test Score with old model: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stats(column_name, label):\n",
    "    average = round(results[column_name].mean(), 2)\n",
    "    std_dev = round(results[column_name].std(), 2)\n",
    "    max_value = round(results[column_name].max(), 2)\n",
    "    min_value = round(results[column_name].min(), 2)\n",
    "\n",
    "    print(f'Average Test Score {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {label}: {min_value}')\n",
    "    print()\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "print_stats('Test_Score_RBF', 'with RBF')\n",
    "print_stats('Test_Score_LINEAR', 'with LINEAR')\n",
    "\n",
    "# 'Test_Score_onlyGhost'列に関して統計情報を表示\n",
    "print_stats('Test_Score_onlyGhost_RBF', 'with only Ghost and RBF')\n",
    "print_stats('Test_Score_onlyGhost_LINEAR', 'with only Ghost and LINEAR')\n",
    "\n",
    "# 'Test_Score_old'列に関して統計情報を表示\n",
    "print_stats('Test_Score_old', 'with old model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed LUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "              \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "luminance_columns = [\"LU1_0\", \"LU1_1\", \"LU1_9\", \"LU1_10\", \"LU1_11\", \"LU1_25\", \"LU1_26\", \"LU1_27\", \n",
    "                     \"LU2_0\", \"LU2_1\", \"LU2_9\", \"LU2_10\", \"LU2_11\", \"LU2_25\", \"LU2_26\", \"LU2_27\"]\n",
    "chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                       \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    train_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "    \n",
    "    train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "\n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(train_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1_1.reset_index(drop=True, inplace=True)\n",
    "train_df1_2.reset_index(drop=True, inplace=True)\n",
    "train_df1_3.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1_1, train_df1_3, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 6000\n",
      "Length of train_df_onlyGhost: 6000\n",
      "Length of train_df5: 6000\n",
      "Length of train_df6: 6000\n"
     ]
    }
   ],
   "source": [
    "# 各データフレームの長さを表示\n",
    "print(f'Length of train_df: {len(train_df)}')\n",
    "print(f'Length of train_df_onlyGhost: {len(train_df_onlyGhost)}')\n",
    "print(f'Length of train_df5: {len(train_df5)}')\n",
    "print(f'Length of train_df6: {len(train_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 6000\n",
      "Length of X_train_onlyGhost: 6000\n",
      "Length of Y_train: 6000\n",
      "Length of MAE: 6000\n",
      "Length of FINAL_QP: 6000\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "MAE = train_df5.values\n",
    "FINAL_QP = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of MAE: {len(MAE)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7734    0.8533    0.8114       300\n",
      "           1     0.8364    0.7500    0.7909       300\n",
      "\n",
      "    accuracy                         0.8017       600\n",
      "   macro avg     0.8049    0.8017    0.8011       600\n",
      "weighted avg     0.8049    0.8017    0.8011       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8052    0.8267    0.8158       300\n",
      "           1     0.8219    0.8000    0.8108       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8136    0.8133    0.8133       600\n",
      "weighted avg     0.8136    0.8133    0.8133       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7714    0.8100    0.7902       300\n",
      "           1     0.8000    0.7600    0.7795       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7857    0.7850    0.7849       600\n",
      "weighted avg     0.7857    0.7850    0.7849       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7836    0.7967    0.7901       300\n",
      "           1     0.7932    0.7800    0.7866       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7884    0.7883    0.7883       600\n",
      "weighted avg     0.7884    0.7883    0.7883       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5887    0.9733    0.7337       300\n",
      "           1     0.9412    0.3200    0.4776       300\n",
      "\n",
      "   micro avg     0.6488    0.6467    0.6477       600\n",
      "   macro avg     0.7649    0.6467    0.6056       600\n",
      "weighted avg     0.7649    0.6467    0.6056       600\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8088    0.8600    0.8336       300\n",
      "           1     0.8505    0.7967    0.8227       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8297    0.8283    0.8282       600\n",
      "weighted avg     0.8297    0.8283    0.8282       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8098    0.8800    0.8435       300\n",
      "           1     0.8686    0.7933    0.8293       300\n",
      "\n",
      "    accuracy                         0.8367       600\n",
      "   macro avg     0.8392    0.8367    0.8364       600\n",
      "weighted avg     0.8392    0.8367    0.8364       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7665    0.8533    0.8076       300\n",
      "           1     0.8346    0.7400    0.7845       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8005    0.7967    0.7960       600\n",
      "weighted avg     0.8005    0.7967    0.7960       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7738    0.8667    0.8176       300\n",
      "           1     0.8485    0.7467    0.7943       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8111    0.8067    0.8060       600\n",
      "weighted avg     0.8111    0.8067    0.8060       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5766    0.9533    0.7186       300\n",
      "           1     0.8824    0.3000    0.4478       300\n",
      "\n",
      "   micro avg     0.6288    0.6267    0.6277       600\n",
      "   macro avg     0.7295    0.6267    0.5832       600\n",
      "weighted avg     0.7295    0.6267    0.5832       600\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8084    0.9000    0.8517       300\n",
      "           1     0.8872    0.7867    0.8339       300\n",
      "\n",
      "    accuracy                         0.8433       600\n",
      "   macro avg     0.8478    0.8433    0.8428       600\n",
      "weighted avg     0.8478    0.8433    0.8428       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8176    0.8967    0.8553       300\n",
      "           1     0.8856    0.8000    0.8406       300\n",
      "\n",
      "    accuracy                         0.8483       600\n",
      "   macro avg     0.8516    0.8483    0.8480       600\n",
      "weighted avg     0.8516    0.8483    0.8480       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7794    0.8833    0.8281       300\n",
      "           1     0.8654    0.7500    0.8036       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8224    0.8167    0.8158       600\n",
      "weighted avg     0.8224    0.8167    0.8158       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7963    0.8600    0.8269       300\n",
      "           1     0.8478    0.7800    0.8125       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8221    0.8200    0.8197       600\n",
      "weighted avg     0.8221    0.8200    0.8197       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5953    0.7600    0.6676       300\n",
      "           1     0.6713    0.4833    0.5620       300\n",
      "\n",
      "   micro avg     0.6227    0.6217    0.6222       600\n",
      "   macro avg     0.6333    0.6217    0.6148       600\n",
      "weighted avg     0.6333    0.6217    0.6148       600\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7676    0.8367    0.8006       300\n",
      "           1     0.8205    0.7467    0.7818       300\n",
      "\n",
      "    accuracy                         0.7917       600\n",
      "   macro avg     0.7940    0.7917    0.7912       600\n",
      "weighted avg     0.7940    0.7917    0.7912       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7904    0.8800    0.8328       300\n",
      "           1     0.8647    0.7667    0.8127       300\n",
      "\n",
      "    accuracy                         0.8233       600\n",
      "   macro avg     0.8275    0.8233    0.8228       600\n",
      "weighted avg     0.8275    0.8233    0.8228       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7584    0.8267    0.7911       300\n",
      "           1     0.8095    0.7367    0.7714       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7840    0.7817    0.7812       600\n",
      "weighted avg     0.7840    0.7817    0.7812       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7725    0.8600    0.8139       300\n",
      "           1     0.8421    0.7467    0.7915       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8073    0.8033    0.8027       600\n",
      "weighted avg     0.8073    0.8033    0.8027       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6761    0.5567    0.6106       300\n",
      "           1     0.6250    0.7333    0.6748       300\n",
      "\n",
      "   micro avg     0.6461    0.6450    0.6455       600\n",
      "   macro avg     0.6506    0.6450    0.6427       600\n",
      "weighted avg     0.6506    0.6450    0.6427       600\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8150    0.8667    0.8401       300\n",
      "           1     0.8577    0.8033    0.8296       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8363    0.8350    0.8348       600\n",
      "weighted avg     0.8363    0.8350    0.8348       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8006    0.8700    0.8339       300\n",
      "           1     0.8577    0.7833    0.8188       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8291    0.8267    0.8263       600\n",
      "weighted avg     0.8291    0.8267    0.8263       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7660    0.8400    0.8013       300\n",
      "           1     0.8229    0.7433    0.7811       300\n",
      "\n",
      "    accuracy                         0.7917       600\n",
      "   macro avg     0.7944    0.7917    0.7912       600\n",
      "weighted avg     0.7944    0.7917    0.7912       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7906    0.8433    0.8161       300\n",
      "           1     0.8321    0.7767    0.8034       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8114    0.8100    0.8098       600\n",
      "weighted avg     0.8114    0.8100    0.8098       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5948    0.8467    0.6988       300\n",
      "           1     0.7326    0.4200    0.5339       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6637    0.6333    0.6163       600\n",
      "weighted avg     0.6637    0.6333    0.6163       600\n",
      "\n",
      "<Fold-6>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7920    0.8633    0.8262       300\n",
      "           1     0.8498    0.7733    0.8098       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8209    0.8183    0.8180       600\n",
      "weighted avg     0.8209    0.8183    0.8180       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7825    0.8633    0.8209       300\n",
      "           1     0.8476    0.7600    0.8014       300\n",
      "\n",
      "    accuracy                         0.8117       600\n",
      "   macro avg     0.8150    0.8117    0.8112       600\n",
      "weighted avg     0.8150    0.8117    0.8112       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7485    0.8133    0.7796       300\n",
      "           1     0.7956    0.7267    0.7596       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7720    0.7700    0.7696       600\n",
      "weighted avg     0.7720    0.7700    0.7696       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7664    0.8200    0.7923       300\n",
      "           1     0.8065    0.7500    0.7772       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7864    0.7850    0.7847       600\n",
      "weighted avg     0.7864    0.7850    0.7847       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5669    0.9467    0.7091       300\n",
      "           1     0.8557    0.2767    0.4181       300\n",
      "\n",
      "   micro avg     0.6137    0.6117    0.6127       600\n",
      "   macro avg     0.7113    0.6117    0.5636       600\n",
      "weighted avg     0.7113    0.6117    0.5636       600\n",
      "\n",
      "<Fold-7>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7925    0.8400    0.8155       300\n",
      "           1     0.8298    0.7800    0.8041       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8111    0.8100    0.8098       600\n",
      "weighted avg     0.8111    0.8100    0.8098       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7781    0.8767    0.8245       300\n",
      "           1     0.8588    0.7500    0.8007       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8184    0.8133    0.8126       600\n",
      "weighted avg     0.8184    0.8133    0.8126       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7441    0.8433    0.7906       300\n",
      "           1     0.8192    0.7100    0.7607       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7817    0.7767    0.7757       600\n",
      "weighted avg     0.7817    0.7767    0.7757       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7370    0.8500    0.7895       300\n",
      "           1     0.8228    0.6967    0.7545       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7799    0.7733    0.7720       600\n",
      "weighted avg     0.7799    0.7733    0.7720       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6930    0.5267    0.5985       300\n",
      "           1     0.6199    0.7667    0.6855       300\n",
      "\n",
      "   micro avg     0.6477    0.6467    0.6472       600\n",
      "   macro avg     0.6565    0.6467    0.6420       600\n",
      "weighted avg     0.6565    0.6467    0.6420       600\n",
      "\n",
      "<Fold-8>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7539    0.8067    0.7794       300\n",
      "           1     0.7921    0.7367    0.7634       300\n",
      "\n",
      "    accuracy                         0.7717       600\n",
      "   macro avg     0.7730    0.7717    0.7714       600\n",
      "weighted avg     0.7730    0.7717    0.7714       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7937    0.8333    0.8130       300\n",
      "           1     0.8246    0.7833    0.8034       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8091    0.8083    0.8082       600\n",
      "weighted avg     0.8091    0.8083    0.8082       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7360    0.7900    0.7621       300\n",
      "           1     0.7734    0.7167    0.7439       300\n",
      "\n",
      "    accuracy                         0.7533       600\n",
      "   macro avg     0.7547    0.7533    0.7530       600\n",
      "weighted avg     0.7547    0.7533    0.7530       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7579    0.8033    0.7799       300\n",
      "           1     0.7908    0.7433    0.7663       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7743    0.7733    0.7731       600\n",
      "weighted avg     0.7743    0.7733    0.7731       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5853    0.9267    0.7174       300\n",
      "           1     0.8306    0.3433    0.4858       300\n",
      "\n",
      "   micro avg     0.6361    0.6350    0.6355       600\n",
      "   macro avg     0.7080    0.6350    0.6016       600\n",
      "weighted avg     0.7080    0.6350    0.6016       600\n",
      "\n",
      "<Fold-9>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8039    0.8333    0.8183       300\n",
      "           1     0.8270    0.7967    0.8115       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8154    0.8150    0.8149       600\n",
      "weighted avg     0.8154    0.8150    0.8149       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8133    0.8066       300\n",
      "           1     0.8102    0.7967    0.8034       300\n",
      "\n",
      "    accuracy                         0.8050       600\n",
      "   macro avg     0.8051    0.8050    0.8050       600\n",
      "weighted avg     0.8051    0.8050    0.8050       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7325    0.8033    0.7663       300\n",
      "           1     0.7823    0.7067    0.7426       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.7574    0.7550    0.7544       600\n",
      "weighted avg     0.7574    0.7550    0.7544       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7618    0.8100    0.7851       300\n",
      "           1     0.7972    0.7467    0.7711       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7795    0.7783    0.7781       600\n",
      "weighted avg     0.7795    0.7783    0.7781       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.6400    0.6358       300\n",
      "           1     0.6351    0.6267    0.6309       300\n",
      "\n",
      "    accuracy                         0.6333       600\n",
      "   macro avg     0.6334    0.6333    0.6333       600\n",
      "weighted avg     0.6334    0.6333    0.6333       600\n",
      "\n",
      "<Fold-10>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8055    0.8833    0.8426       300\n",
      "           1     0.8708    0.7867    0.8266       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8382    0.8350    0.8346       600\n",
      "weighted avg     0.8382    0.8350    0.8346       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8092    0.8767    0.8416       300\n",
      "           1     0.8655    0.7933    0.8278       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8373    0.8350    0.8347       600\n",
      "weighted avg     0.8373    0.8350    0.8347       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7633    0.8600    0.8088       300\n",
      "           1     0.8397    0.7333    0.7829       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8015    0.7967    0.7958       600\n",
      "weighted avg     0.8015    0.7967    0.7958       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7846    0.8500    0.8160       300\n",
      "           1     0.8364    0.7667    0.8000       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8105    0.8083    0.8080       600\n",
      "weighted avg     0.8105    0.8083    0.8080       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5851    0.9167    0.7143       300\n",
      "           1     0.8140    0.3500    0.4895       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6995    0.6333    0.6019       600\n",
      "weighted avg     0.6995    0.6333    0.6019       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Test_Score_RBF', 'C_LINEAR', 'Test_Score_LINEAR', \n",
    "                                'C_onlyGhost_RBF', 'Test_Score_onlyGhost_RBF', 'C_onlyGhost_LINEAR', 'Test_Score_onlyGhost_LINEAR',\n",
    "                                'Threshold', 'Test_Score_old'])\n",
    "\n",
    "initial_X, initial_X_onlyGhost = X_train, X_train_onlyGhost\n",
    "initial_Y = Y_train\n",
    "initial_old, initial_final_QP = MAE, FINAL_QP\n",
    "\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(initial_X, initial_Y)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    results_old = []\n",
    "\n",
    "    # 全体を訓練・検証データとテストデータに分割\n",
    "    X_train_val, X_test = initial_X[train_ids], initial_X[test_ids]\n",
    "    X_train_onlyGhost_val, X_test_onlyGhost = initial_X_onlyGhost[train_ids], initial_X_onlyGhost[test_ids]\n",
    "    X_train_old_val, X_test_old = initial_old[train_ids], initial_old[test_ids]\n",
    "    \n",
    "    final_QP = initial_final_QP[test_ids]\n",
    "    \n",
    "    # 全体を訓練・検証ラベルとテストラベルに分割\n",
    "    Y_train_val, Y_test = initial_Y[train_ids], initial_Y[test_ids]\n",
    "    \n",
    "    # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=600, random_state=42)\n",
    "    X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=600, random_state=42)\n",
    "    \n",
    "    # for i in range(600): \n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    for threshold in np.arange(0.01,1.01,0.01):\n",
    "        results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(600)]\n",
    "        # results_old.append((is_double))\n",
    "        \n",
    "        predicted_labels = [int(is_double) for is_double in results_old]\n",
    "        ground_truth_labels = [label for label in Y_test]\n",
    "        accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(X_test)\n",
    "    test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(Y_test, test_predictions_RBF)\n",
    "    report_RBF = classification_report(Y_test, test_predictions_RBF, digits=4)\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test)\n",
    "    test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(Y_test, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(Y_test, test_predictions_LINEAR, digits=4)\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(Y_test, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(Y_test, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(Y_test, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Test_Score_RBF': test_accuracy_RBF,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Test_Score_LINEAR': test_accuracy_LINEAR,\n",
    "              'C_onlyGhost_RBF': best_c_value_onlyGhost_RBF, 'Test_Score_onlyGhost_RBF': test_accuracy_onlyGhost_RBF,\n",
    "              'C_onlyGhost_LINEAR': best_c_value_onlyGhost_LINEAR, 'Test_Score_onlyGhost_LINEAR': test_accuracy_onlyGhost_LINEAR,\n",
    "              'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Score with RBF: 0.81\n",
      "Standard Deviation of Test Score with RBF: 0.02\n",
      "Maximum Test Score with RBF: 0.84\n",
      "Minimum Test Score with RBF: 0.77\n",
      "\n",
      "Average Test Score with LINEAR: 0.82\n",
      "Standard Deviation of Test Score with LINEAR: 0.01\n",
      "Maximum Test Score with LINEAR: 0.85\n",
      "Minimum Test Score with LINEAR: 0.81\n",
      "\n",
      "Average Test Score with only Ghost and RBF: 0.78\n",
      "Standard Deviation of Test Score with only Ghost and RBF: 0.02\n",
      "Maximum Test Score with only Ghost and RBF: 0.82\n",
      "Minimum Test Score with only Ghost and RBF: 0.75\n",
      "\n",
      "Average Test Score with only Ghost and LINEAR: 0.79\n",
      "Standard Deviation of Test Score with only Ghost and LINEAR: 0.02\n",
      "Maximum Test Score with only Ghost and LINEAR: 0.82\n",
      "Minimum Test Score with only Ghost and LINEAR: 0.77\n",
      "\n",
      "Average Test Score with old model: 0.63\n",
      "Standard Deviation of Test Score with old model: 0.01\n",
      "Maximum Test Score with old model: 0.65\n",
      "Minimum Test Score with old model: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stats(column_name, label):\n",
    "    average = round(results[column_name].mean(), 2)\n",
    "    std_dev = round(results[column_name].std(), 2)\n",
    "    max_value = round(results[column_name].max(), 2)\n",
    "    min_value = round(results[column_name].min(), 2)\n",
    "\n",
    "    print(f'Average Test Score {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {label}: {min_value}')\n",
    "    print()\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "print_stats('Test_Score_RBF', 'with RBF')\n",
    "print_stats('Test_Score_LINEAR', 'with LINEAR')\n",
    "\n",
    "# 'Test_Score_onlyGhost'列に関して統計情報を表示\n",
    "print_stats('Test_Score_onlyGhost_RBF', 'with only Ghost and RBF')\n",
    "print_stats('Test_Score_onlyGhost_LINEAR', 'with only Ghost and LINEAR')\n",
    "\n",
    "# 'Test_Score_old'列に関して統計情報を表示\n",
    "print_stats('Test_Score_old', 'with old model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "              \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "luminance_columns = [\"LU1_0\", \"LU1_1\", \"LU1_9\", \"LU1_10\", \"LU1_11\", \"LU1_25\", \"LU1_26\", \"LU1_27\", \n",
    "                     \"LU2_0\", \"LU2_1\", \"LU2_9\", \"LU2_10\", \"LU2_11\", \"LU2_25\", \"LU2_26\", \"LU2_27\"]\n",
    "chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                       \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    train_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "    \n",
    "    train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "\n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_old = calculate_mae(train_pkl_list[0])\n",
    "    mae_d2, _ = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1_old]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1_1.reset_index(drop=True, inplace=True)\n",
    "train_df1_2.reset_index(drop=True, inplace=True)\n",
    "train_df1_3.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1_1, train_df1_2, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_df: 6000\n",
      "Length of train_df_onlyGhost: 6000\n",
      "Length of train_df5: 6000\n",
      "Length of train_df6: 6000\n"
     ]
    }
   ],
   "source": [
    "# 各データフレームの長さを表示\n",
    "print(f'Length of train_df: {len(train_df)}')\n",
    "print(f'Length of train_df_onlyGhost: {len(train_df_onlyGhost)}')\n",
    "print(f'Length of train_df5: {len(train_df5)}')\n",
    "print(f'Length of train_df6: {len(train_df6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 6000\n",
      "Length of X_train_onlyGhost: 6000\n",
      "Length of Y_train: 6000\n",
      "Length of MAE: 6000\n",
      "Length of FINAL_QP: 6000\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "MAE = train_df5.values\n",
    "FINAL_QP = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of MAE: {len(MAE)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7576    0.8333    0.7937       300\n",
      "           1     0.8148    0.7333    0.7719       300\n",
      "\n",
      "    accuracy                         0.7833       600\n",
      "   macro avg     0.7862    0.7833    0.7828       600\n",
      "weighted avg     0.7862    0.7833    0.7828       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8091    0.8333    0.8210       300\n",
      "           1     0.8282    0.8033    0.8156       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8186    0.8183    0.8183       600\n",
      "weighted avg     0.8186    0.8183    0.8183       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7714    0.8100    0.7902       300\n",
      "           1     0.8000    0.7600    0.7795       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7857    0.7850    0.7849       600\n",
      "weighted avg     0.7857    0.7850    0.7849       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7836    0.7967    0.7901       300\n",
      "           1     0.7932    0.7800    0.7866       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7884    0.7883    0.7883       600\n",
      "weighted avg     0.7884    0.7883    0.7883       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5887    0.9733    0.7337       300\n",
      "           1     0.9412    0.3200    0.4776       300\n",
      "\n",
      "   micro avg     0.6488    0.6467    0.6477       600\n",
      "   macro avg     0.7649    0.6467    0.6056       600\n",
      "weighted avg     0.7649    0.6467    0.6056       600\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7898    0.8767    0.8310       300\n",
      "           1     0.8614    0.7667    0.8113       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8256    0.8217    0.8211       600\n",
      "weighted avg     0.8256    0.8217    0.8211       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8061    0.8867    0.8444       300\n",
      "           1     0.8741    0.7867    0.8281       300\n",
      "\n",
      "    accuracy                         0.8367       600\n",
      "   macro avg     0.8401    0.8367    0.8363       600\n",
      "weighted avg     0.8401    0.8367    0.8363       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7665    0.8533    0.8076       300\n",
      "           1     0.8346    0.7400    0.7845       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8005    0.7967    0.7960       600\n",
      "weighted avg     0.8005    0.7967    0.7960       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7738    0.8667    0.8176       300\n",
      "           1     0.8485    0.7467    0.7943       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8111    0.8067    0.8060       600\n",
      "weighted avg     0.8111    0.8067    0.8060       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5766    0.9533    0.7186       300\n",
      "           1     0.8824    0.3000    0.4478       300\n",
      "\n",
      "   micro avg     0.6288    0.6267    0.6277       600\n",
      "   macro avg     0.7295    0.6267    0.5832       600\n",
      "weighted avg     0.7295    0.6267    0.5832       600\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7676    0.8700    0.8156       300\n",
      "           1     0.8500    0.7367    0.7893       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8088    0.8033    0.8025       600\n",
      "weighted avg     0.8088    0.8033    0.8025       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8079    0.8833    0.8439       300\n",
      "           1     0.8713    0.7900    0.8287       300\n",
      "\n",
      "    accuracy                         0.8367       600\n",
      "   macro avg     0.8396    0.8367    0.8363       600\n",
      "weighted avg     0.8396    0.8367    0.8363       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7794    0.8833    0.8281       300\n",
      "           1     0.8654    0.7500    0.8036       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8224    0.8167    0.8158       600\n",
      "weighted avg     0.8224    0.8167    0.8158       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7963    0.8600    0.8269       300\n",
      "           1     0.8478    0.7800    0.8125       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8221    0.8200    0.8197       600\n",
      "weighted avg     0.8221    0.8200    0.8197       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5953    0.7600    0.6676       300\n",
      "           1     0.6713    0.4833    0.5620       300\n",
      "\n",
      "   micro avg     0.6227    0.6217    0.6222       600\n",
      "   macro avg     0.6333    0.6217    0.6148       600\n",
      "weighted avg     0.6333    0.6217    0.6148       600\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7950    0.8400    0.8169       300\n",
      "           1     0.8304    0.7833    0.8062       300\n",
      "\n",
      "    accuracy                         0.8117       600\n",
      "   macro avg     0.8127    0.8117    0.8115       600\n",
      "weighted avg     0.8127    0.8117    0.8115       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8201    0.8967    0.8567       300\n",
      "           1     0.8860    0.8033    0.8427       300\n",
      "\n",
      "    accuracy                         0.8500       600\n",
      "   macro avg     0.8531    0.8500    0.8497       600\n",
      "weighted avg     0.8531    0.8500    0.8497       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7584    0.8267    0.7911       300\n",
      "           1     0.8095    0.7367    0.7714       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7840    0.7817    0.7812       600\n",
      "weighted avg     0.7840    0.7817    0.7812       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7725    0.8600    0.8139       300\n",
      "           1     0.8421    0.7467    0.7915       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8073    0.8033    0.8027       600\n",
      "weighted avg     0.8073    0.8033    0.8027       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6761    0.5567    0.6106       300\n",
      "           1     0.6250    0.7333    0.6748       300\n",
      "\n",
      "   micro avg     0.6461    0.6450    0.6455       600\n",
      "   macro avg     0.6506    0.6450    0.6427       600\n",
      "weighted avg     0.6506    0.6450    0.6427       600\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7808    0.8667    0.8215       300\n",
      "           1     0.8502    0.7567    0.8007       300\n",
      "\n",
      "    accuracy                         0.8117       600\n",
      "   macro avg     0.8155    0.8117    0.8111       600\n",
      "weighted avg     0.8155    0.8117    0.8111       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8201    0.8967    0.8567       300\n",
      "           1     0.8860    0.8033    0.8427       300\n",
      "\n",
      "    accuracy                         0.8500       600\n",
      "   macro avg     0.8531    0.8500    0.8497       600\n",
      "weighted avg     0.8531    0.8500    0.8497       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7660    0.8400    0.8013       300\n",
      "           1     0.8229    0.7433    0.7811       300\n",
      "\n",
      "    accuracy                         0.7917       600\n",
      "   macro avg     0.7944    0.7917    0.7912       600\n",
      "weighted avg     0.7944    0.7917    0.7912       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7906    0.8433    0.8161       300\n",
      "           1     0.8321    0.7767    0.8034       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8114    0.8100    0.8098       600\n",
      "weighted avg     0.8114    0.8100    0.8098       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5948    0.8467    0.6988       300\n",
      "           1     0.7326    0.4200    0.5339       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6637    0.6333    0.6163       600\n",
      "weighted avg     0.6637    0.6333    0.6163       600\n",
      "\n",
      "<Fold-6>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7564    0.8800    0.8136       300\n",
      "           1     0.8566    0.7167    0.7804       300\n",
      "\n",
      "    accuracy                         0.7983       600\n",
      "   macro avg     0.8065    0.7983    0.7970       600\n",
      "weighted avg     0.8065    0.7983    0.7970       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7908    0.8567    0.8224       300\n",
      "           1     0.8436    0.7733    0.8070       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8172    0.8150    0.8147       600\n",
      "weighted avg     0.8172    0.8150    0.8147       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7485    0.8133    0.7796       300\n",
      "           1     0.7956    0.7267    0.7596       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7720    0.7700    0.7696       600\n",
      "weighted avg     0.7720    0.7700    0.7696       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7664    0.8200    0.7923       300\n",
      "           1     0.8065    0.7500    0.7772       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7864    0.7850    0.7847       600\n",
      "weighted avg     0.7864    0.7850    0.7847       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5669    0.9467    0.7091       300\n",
      "           1     0.8557    0.2767    0.4181       300\n",
      "\n",
      "   micro avg     0.6137    0.6117    0.6127       600\n",
      "   macro avg     0.7113    0.6117    0.5636       600\n",
      "weighted avg     0.7113    0.6117    0.5636       600\n",
      "\n",
      "<Fold-7>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7554    0.8133    0.7833       300\n",
      "           1     0.7978    0.7367    0.7660       300\n",
      "\n",
      "    accuracy                         0.7750       600\n",
      "   macro avg     0.7766    0.7750    0.7747       600\n",
      "weighted avg     0.7766    0.7750    0.7747       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7734    0.9100    0.8361       300\n",
      "           1     0.8907    0.7333    0.8044       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8320    0.8217    0.8203       600\n",
      "weighted avg     0.8320    0.8217    0.8203       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7441    0.8433    0.7906       300\n",
      "           1     0.8192    0.7100    0.7607       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7817    0.7767    0.7757       600\n",
      "weighted avg     0.7817    0.7767    0.7757       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7370    0.8500    0.7895       300\n",
      "           1     0.8228    0.6967    0.7545       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7799    0.7733    0.7720       600\n",
      "weighted avg     0.7799    0.7733    0.7720       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6930    0.5267    0.5985       300\n",
      "           1     0.6199    0.7667    0.6855       300\n",
      "\n",
      "   micro avg     0.6477    0.6467    0.6472       600\n",
      "   macro avg     0.6565    0.6467    0.6420       600\n",
      "weighted avg     0.6565    0.6467    0.6420       600\n",
      "\n",
      "<Fold-8>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7372    0.8133    0.7734       300\n",
      "           1     0.7918    0.7100    0.7487       300\n",
      "\n",
      "    accuracy                         0.7617       600\n",
      "   macro avg     0.7645    0.7617    0.7610       600\n",
      "weighted avg     0.7645    0.7617    0.7610       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7866    0.8233    0.8046       300\n",
      "           1     0.8147    0.7767    0.7952       300\n",
      "\n",
      "    accuracy                         0.8000       600\n",
      "   macro avg     0.8007    0.8000    0.7999       600\n",
      "weighted avg     0.8007    0.8000    0.7999       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7360    0.7900    0.7621       300\n",
      "           1     0.7734    0.7167    0.7439       300\n",
      "\n",
      "    accuracy                         0.7533       600\n",
      "   macro avg     0.7547    0.7533    0.7530       600\n",
      "weighted avg     0.7547    0.7533    0.7530       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7579    0.8033    0.7799       300\n",
      "           1     0.7908    0.7433    0.7663       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7743    0.7733    0.7731       600\n",
      "weighted avg     0.7743    0.7733    0.7731       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5853    0.9267    0.7174       300\n",
      "           1     0.8306    0.3433    0.4858       300\n",
      "\n",
      "   micro avg     0.6361    0.6350    0.6355       600\n",
      "   macro avg     0.7080    0.6350    0.6016       600\n",
      "weighted avg     0.7080    0.6350    0.6016       600\n",
      "\n",
      "<Fold-9>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7609    0.8167    0.7878       300\n",
      "           1     0.8022    0.7433    0.7716       300\n",
      "\n",
      "    accuracy                         0.7800       600\n",
      "   macro avg     0.7815    0.7800    0.7797       600\n",
      "weighted avg     0.7815    0.7800    0.7797       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7958    0.8833    0.8373       300\n",
      "           1     0.8689    0.7733    0.8183       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8324    0.8283    0.8278       600\n",
      "weighted avg     0.8324    0.8283    0.8278       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7325    0.8033    0.7663       300\n",
      "           1     0.7823    0.7067    0.7426       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.7574    0.7550    0.7544       600\n",
      "weighted avg     0.7574    0.7550    0.7544       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7618    0.8100    0.7851       300\n",
      "           1     0.7972    0.7467    0.7711       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7795    0.7783    0.7781       600\n",
      "weighted avg     0.7795    0.7783    0.7781       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.6400    0.6358       300\n",
      "           1     0.6351    0.6267    0.6309       300\n",
      "\n",
      "    accuracy                         0.6333       600\n",
      "   macro avg     0.6334    0.6333    0.6333       600\n",
      "weighted avg     0.6334    0.6333    0.6333       600\n",
      "\n",
      "<Fold-10>\n",
      "\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8161    0.8433    0.8295       300\n",
      "           1     0.8379    0.8100    0.8237       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8270    0.8267    0.8266       600\n",
      "weighted avg     0.8270    0.8267    0.8266       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8049    0.8800    0.8408       300\n",
      "           1     0.8676    0.7867    0.8252       300\n",
      "\n",
      "    accuracy                         0.8333       600\n",
      "   macro avg     0.8363    0.8333    0.8330       600\n",
      "weighted avg     0.8363    0.8333    0.8330       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7633    0.8600    0.8088       300\n",
      "           1     0.8397    0.7333    0.7829       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8015    0.7967    0.7958       600\n",
      "weighted avg     0.8015    0.7967    0.7958       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7846    0.8500    0.8160       300\n",
      "           1     0.8364    0.7667    0.8000       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8105    0.8083    0.8080       600\n",
      "weighted avg     0.8105    0.8083    0.8080       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5851    0.9167    0.7143       300\n",
      "           1     0.8140    0.3500    0.4895       300\n",
      "\n",
      "   micro avg     0.6344    0.6333    0.6339       600\n",
      "   macro avg     0.6995    0.6333    0.6019       600\n",
      "weighted avg     0.6995    0.6333    0.6019       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Test_Score_RBF', 'C_LINEAR', 'Test_Score_LINEAR', \n",
    "                                'C_onlyGhost_RBF', 'Test_Score_onlyGhost_RBF', 'C_onlyGhost_LINEAR', 'Test_Score_onlyGhost_LINEAR',\n",
    "                                'Threshold', 'Test_Score_old'])\n",
    "\n",
    "initial_X, initial_X_onlyGhost = X_train, X_train_onlyGhost\n",
    "initial_Y = Y_train\n",
    "initial_old, initial_final_QP = MAE, FINAL_QP\n",
    "\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(initial_X, initial_Y)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    results_old = []\n",
    "\n",
    "    # 全体を訓練・検証データとテストデータに分割\n",
    "    X_train_val, X_test = initial_X[train_ids], initial_X[test_ids]\n",
    "    X_train_onlyGhost_val, X_test_onlyGhost = initial_X_onlyGhost[train_ids], initial_X_onlyGhost[test_ids]\n",
    "    X_train_old_val, X_test_old = initial_old[train_ids], initial_old[test_ids]\n",
    "    \n",
    "    final_QP = initial_final_QP[test_ids]\n",
    "    \n",
    "    # 全体を訓練・検証ラベルとテストラベルに分割\n",
    "    Y_train_val, Y_test = initial_Y[train_ids], initial_Y[test_ids]\n",
    "    \n",
    "    # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=600, random_state=42)\n",
    "    X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_onlyGhost_val, Y_train_val, test_size=600, random_state=42)\n",
    "    \n",
    "    # for i in range(600): \n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    for threshold in np.arange(0.01,1.01,0.01):\n",
    "        results_old = [is_double_compressed(X_test_old[i], final_QP[i], threshold) for i in range(600)]\n",
    "        # results_old.append((is_double))\n",
    "        \n",
    "        predicted_labels = [int(is_double) for is_double in results_old]\n",
    "        ground_truth_labels = [label for label in Y_test]\n",
    "        accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(X_test)\n",
    "    test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(Y_test, test_predictions_RBF)\n",
    "    report_RBF = classification_report(Y_test, test_predictions_RBF, digits=4)\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test)\n",
    "    test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(Y_test, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(Y_test, test_predictions_LINEAR, digits=4)\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(Y_test, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(Y_test, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_onlyGhost)\n",
    "    test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(Y_test, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Test_Score_RBF': test_accuracy_RBF,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Test_Score_LINEAR': test_accuracy_LINEAR,\n",
    "              'C_onlyGhost_RBF': best_c_value_onlyGhost_RBF, 'Test_Score_onlyGhost_RBF': test_accuracy_onlyGhost_RBF,\n",
    "              'C_onlyGhost_LINEAR': best_c_value_onlyGhost_LINEAR, 'Test_Score_onlyGhost_LINEAR': test_accuracy_onlyGhost_LINEAR,\n",
    "              'Threshold': best_threshold, 'Test_Score_old': best_accuracy}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Score with RBF: 0.8\n",
      "Standard Deviation of Test Score with RBF: 0.02\n",
      "Maximum Test Score with RBF: 0.83\n",
      "Minimum Test Score with RBF: 0.76\n",
      "\n",
      "Average Test Score with LINEAR: 0.83\n",
      "Standard Deviation of Test Score with LINEAR: 0.02\n",
      "Maximum Test Score with LINEAR: 0.85\n",
      "Minimum Test Score with LINEAR: 0.8\n",
      "\n",
      "Average Test Score with only Ghost and RBF: 0.78\n",
      "Standard Deviation of Test Score with only Ghost and RBF: 0.02\n",
      "Maximum Test Score with only Ghost and RBF: 0.82\n",
      "Minimum Test Score with only Ghost and RBF: 0.75\n",
      "\n",
      "Average Test Score with only Ghost and LINEAR: 0.79\n",
      "Standard Deviation of Test Score with only Ghost and LINEAR: 0.02\n",
      "Maximum Test Score with only Ghost and LINEAR: 0.82\n",
      "Minimum Test Score with only Ghost and LINEAR: 0.77\n",
      "\n",
      "Average Test Score with old model: 0.63\n",
      "Standard Deviation of Test Score with old model: 0.01\n",
      "Maximum Test Score with old model: 0.65\n",
      "Minimum Test Score with old model: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stats(column_name, label):\n",
    "    average = round(results[column_name].mean(), 2)\n",
    "    std_dev = round(results[column_name].std(), 2)\n",
    "    max_value = round(results[column_name].max(), 2)\n",
    "    min_value = round(results[column_name].min(), 2)\n",
    "\n",
    "    print(f'Average Test Score {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {label}: {min_value}')\n",
    "    print()\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "print_stats('Test_Score_RBF', 'with RBF')\n",
    "print_stats('Test_Score_LINEAR', 'with LINEAR')\n",
    "\n",
    "# 'Test_Score_onlyGhost'列に関して統計情報を表示\n",
    "print_stats('Test_Score_onlyGhost_RBF', 'with only Ghost and RBF')\n",
    "print_stats('Test_Score_onlyGhost_LINEAR', 'with only Ghost and LINEAR')\n",
    "\n",
    "# 'Test_Score_old'列に関して統計情報を表示\n",
    "print_stats('Test_Score_old', 'with old model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
