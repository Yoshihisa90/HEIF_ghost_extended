{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['191', '10', '55', '118', '235', '241', '244', '200', '293', '273', '193', '71', '278', '212', '143', '79', '84', '275', '245', '46', '78', '268', '83', '276', '227', '230', '181', '122', '44', '188'], ['234', '163', '190', '152', '283', '90', '186', '286', '4', '231', '209', '109', '120', '194', '259', '51', '61', '136', '99', '290', '112', '104', '172', '158', '300', '284', '20', '165', '210', '72'], ['36', '68', '86', '221', '236', '295', '239', '94', '133', '149', '106', '285', '73', '141', '3', '96', '258', '57', '93', '13', '243', '110', '50', '170', '35', '113', '211', '82', '288', '129'], ['18', '228', '144', '117', '242', '107', '162', '132', '11', '176', '229', '166', '157', '98', '67', '294', '167', '29', '108', '253', '292', '33', '105', '277', '260', '192', '2', '184', '220', '246'], ['37', '75', '142', '48', '272', '19', '178', '289', '248', '199', '185', '77', '238', '64', '76', '8', '135', '124', '15', '134', '125', '223', '52', '269', '92', '196', '251', '203', '41', '213'], ['151', '128', '257', '216', '261', '195', '7', '95', '63', '30', '222', '282', '217', '89', '168', '16', '182', '153', '201', '198', '218', '159', '60', '287', '174', '49', '179', '119', '250', '171'], ['69', '265', '279', '24', '65', '266', '47', '146', '205', '126', '226', '264', '25', '237', '6', '270', '206', '130', '59', '175', '204', '254', '280', '219', '56', '148', '138', '80', '123', '70'], ['27', '169', '154', '42', '62', '102', '271', '164', '147', '145', '23', '225', '32', '9', '45', '14', '139', '111', '252', '267', '88', '207', '296', '100', '240', '187', '12', '291', '224', '54'], ['34', '85', '1', '180', '5', '208', '137', '263', '232', '121', '281', '28', '22', '256', '38', '127', '114', '87', '274', '202', '156', '214', '262', '197', '115', '97', '43', '189', '173', '31'], ['150', '177', '26', '183', '74', '249', '40', '233', '103', '58', '299', '39', '53', '297', '101', '66', '161', '160', '116', '17', '91', '298', '21', '131', '140', '247', '155', '81', '215', '255']]\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "single_list1 = []\n",
    "single_list2 = []\n",
    "single_list3 = []\n",
    "single_list4 = []\n",
    "single_list5 = []\n",
    "single_list6 = []\n",
    "single_list7 = []\n",
    "single_list8 = []\n",
    "single_list9 = []\n",
    "single_list10 = []\n",
    "\n",
    "single_recompress_list1 = []\n",
    "single_recompress_list2 = []\n",
    "single_recompress_list3 = []\n",
    "single_recompress_list4 = []\n",
    "single_recompress_list5 = []\n",
    "single_recompress_list6 = []\n",
    "single_recompress_list7 = []\n",
    "single_recompress_list8 = []\n",
    "single_recompress_list9 = []\n",
    "single_recompress_list10 = []\n",
    "\n",
    "second_largeQP1_list1 = []\n",
    "second_largeQP1_list2 = []\n",
    "second_largeQP1_list3 = []\n",
    "second_largeQP1_list4 = []\n",
    "second_largeQP1_list5 = []\n",
    "second_largeQP1_list6 = []\n",
    "second_largeQP1_list7 = []\n",
    "second_largeQP1_list8 = []\n",
    "second_largeQP1_list9 = []\n",
    "second_largeQP1_list10 = []\n",
    "\n",
    "second_recompress_largeQP1_list1 = []\n",
    "second_recompress_largeQP1_list2 = []\n",
    "second_recompress_largeQP1_list3 = []\n",
    "second_recompress_largeQP1_list4 = []\n",
    "second_recompress_largeQP1_list5 = []\n",
    "second_recompress_largeQP1_list6 = []\n",
    "second_recompress_largeQP1_list7 = []\n",
    "second_recompress_largeQP1_list8 = []\n",
    "second_recompress_largeQP1_list9 = []\n",
    "second_recompress_largeQP1_list10 = []\n",
    "\n",
    "second_sameQP_list1 = []\n",
    "second_sameQP_list2 = []\n",
    "second_sameQP_list3 = []\n",
    "second_sameQP_list4 = []\n",
    "second_sameQP_list5 = []\n",
    "second_sameQP_list6 = []\n",
    "second_sameQP_list7 = []\n",
    "second_sameQP_list8 = []\n",
    "second_sameQP_list9 = []\n",
    "second_sameQP_list10 = []\n",
    "\n",
    "second_recompress_sameQP_list1 = []\n",
    "second_recompress_sameQP_list2 = []\n",
    "second_recompress_sameQP_list3 = []\n",
    "second_recompress_sameQP_list4 = []\n",
    "second_recompress_sameQP_list5 = []\n",
    "second_recompress_sameQP_list6 = []\n",
    "second_recompress_sameQP_list7 = []\n",
    "second_recompress_sameQP_list8 = []\n",
    "second_recompress_sameQP_list9 = []\n",
    "second_recompress_sameQP_list10 = []\n",
    "\n",
    "second_largeQP2_list1 = []\n",
    "second_largeQP2_list2 = []\n",
    "second_largeQP2_list3 = []\n",
    "second_largeQP2_list4 = []\n",
    "second_largeQP2_list5 = []\n",
    "second_largeQP2_list6 = []\n",
    "second_largeQP2_list7 = []\n",
    "second_largeQP2_list8 = []\n",
    "second_largeQP2_list9 = []\n",
    "second_largeQP2_list10 = []\n",
    "\n",
    "second_recompress_largeQP2_list1 = []\n",
    "second_recompress_largeQP2_list2 = []\n",
    "second_recompress_largeQP2_list3 = []\n",
    "second_recompress_largeQP2_list4 = []\n",
    "second_recompress_largeQP2_list5 = []\n",
    "second_recompress_largeQP2_list6 = []\n",
    "second_recompress_largeQP2_list7 = []\n",
    "second_recompress_largeQP2_list8 = []\n",
    "second_recompress_largeQP2_list9 = []\n",
    "second_recompress_largeQP2_list10 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_lists(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath_csv+f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath_csv+f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath_csv+f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath_csv+f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath_csv+f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath_csv+f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath_csv+f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath_csv+f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "            \n",
    "\n",
    "# train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5, train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "single_lists = [single_list1, single_list2,\n",
    "                single_list3, single_list4,\n",
    "                single_list5, single_list6,\n",
    "                single_list7, single_list8,\n",
    "                single_list9, single_list10]\n",
    "\n",
    "single_recompress_lists = [single_recompress_list1, single_recompress_list2,\n",
    "                           single_recompress_list3, single_recompress_list4,\n",
    "                           single_recompress_list5, single_recompress_list6,\n",
    "                           single_recompress_list7, single_recompress_list8,\n",
    "                           single_recompress_list9, single_recompress_list10]\n",
    "\n",
    "\n",
    "second_largeQP1_lists = [second_largeQP1_list1, second_largeQP1_list2,\n",
    "                        second_largeQP1_list3, second_largeQP1_list4,\n",
    "                        second_largeQP1_list5, second_largeQP1_list6,\n",
    "                        second_largeQP1_list7, second_largeQP1_list8,\n",
    "                        second_largeQP1_list9, second_largeQP1_list10]\n",
    "\n",
    "second_recompress_largeQP1_lists = [second_recompress_largeQP1_list1, second_recompress_largeQP1_list2,\n",
    "                           second_recompress_largeQP1_list3, second_recompress_largeQP1_list4,\n",
    "                           second_recompress_largeQP1_list5, second_recompress_largeQP1_list6,\n",
    "                           second_recompress_largeQP1_list7, second_recompress_largeQP1_list8,\n",
    "                           second_recompress_largeQP1_list9, second_recompress_largeQP1_list10]\n",
    "\n",
    "\n",
    "second_sameQP_lists = [second_sameQP_list1, second_sameQP_list2,\n",
    "                        second_sameQP_list3, second_sameQP_list4,\n",
    "                        second_sameQP_list5, second_sameQP_list6,\n",
    "                        second_sameQP_list7, second_sameQP_list8,\n",
    "                        second_sameQP_list9, second_sameQP_list10]\n",
    "\n",
    "second_recompress_sameQP_lists = [second_recompress_sameQP_list1, second_recompress_sameQP_list2,\n",
    "                           second_recompress_sameQP_list3, second_recompress_sameQP_list4,\n",
    "                           second_recompress_sameQP_list5, second_recompress_sameQP_list6,\n",
    "                           second_recompress_sameQP_list7, second_recompress_sameQP_list8,\n",
    "                           second_recompress_sameQP_list9, second_recompress_sameQP_list10]\n",
    "\n",
    "\n",
    "second_largeQP2_lists = [second_largeQP2_list1, second_largeQP2_list2,\n",
    "                        second_largeQP2_list3, second_largeQP2_list4,\n",
    "                        second_largeQP2_list5, second_largeQP2_list6,\n",
    "                        second_largeQP2_list7, second_largeQP2_list8,\n",
    "                        second_largeQP2_list9, second_largeQP2_list10]\n",
    "\n",
    "second_recompress_largeQP2_lists = [second_recompress_largeQP2_list1, second_recompress_largeQP2_list2,\n",
    "                           second_recompress_largeQP2_list3, second_recompress_largeQP2_list4,\n",
    "                           second_recompress_largeQP2_list5, second_recompress_largeQP2_list6,\n",
    "                           second_recompress_largeQP2_list7, second_recompress_largeQP2_list8,\n",
    "                           second_recompress_largeQP2_list9, second_recompress_largeQP2_list10]\n",
    "\n",
    "\n",
    "\n",
    "for train_list, single_list, single_recompress_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                           single_lists,\n",
    "                                                                                                                                                                                                                           single_recompress_lists,\n",
    "                                                                                                                                                                                                                           second_largeQP1_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP1_lists,\n",
    "                                                                                                                                                                                                                           second_sameQP_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_sameQP_lists,\n",
    "                                                                                                                                                                                                                           second_largeQP2_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP2_lists\n",
    "                                                                                                                                                                                                                          ):\n",
    "    process_train_lists(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "1710\n",
      "1710\n",
      "300\n",
      "300\n",
      "1170\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(single_lists[6]))\n",
    "print(len(single_recompress_lists[6]))\n",
    "print(len(second_largeQP1_lists[6]))\n",
    "print(len(second_recompress_largeQP1_lists[6]))\n",
    "print(len(second_sameQP_lists[6]))\n",
    "print(len(second_recompress_sameQP_lists[6]))\n",
    "print(len(second_largeQP2_lists[6]))\n",
    "print(len(second_recompress_largeQP2_lists[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list1 = []\n",
    "single_list2 = []\n",
    "single_list3 = []\n",
    "single_list4 = []\n",
    "single_list5 = []\n",
    "single_list6 = []\n",
    "single_list7 = []\n",
    "single_list8 = []\n",
    "single_list9 = []\n",
    "single_list10 = []\n",
    "\n",
    "single_recompress_list1 = []\n",
    "single_recompress_list2 = []\n",
    "single_recompress_list3 = []\n",
    "single_recompress_list4 = []\n",
    "single_recompress_list5 = []\n",
    "single_recompress_list6 = []\n",
    "single_recompress_list7 = []\n",
    "single_recompress_list8 = []\n",
    "single_recompress_list9 = []\n",
    "single_recompress_list10 = []\n",
    "\n",
    "second_largeQP1_list1 = []\n",
    "second_largeQP1_list2 = []\n",
    "second_largeQP1_list3 = []\n",
    "second_largeQP1_list4 = []\n",
    "second_largeQP1_list5 = []\n",
    "second_largeQP1_list6 = []\n",
    "second_largeQP1_list7 = []\n",
    "second_largeQP1_list8 = []\n",
    "second_largeQP1_list9 = []\n",
    "second_largeQP1_list10 = []\n",
    "\n",
    "second_recompress_largeQP1_list1 = []\n",
    "second_recompress_largeQP1_list2 = []\n",
    "second_recompress_largeQP1_list3 = []\n",
    "second_recompress_largeQP1_list4 = []\n",
    "second_recompress_largeQP1_list5 = []\n",
    "second_recompress_largeQP1_list6 = []\n",
    "second_recompress_largeQP1_list7 = []\n",
    "second_recompress_largeQP1_list8 = []\n",
    "second_recompress_largeQP1_list9 = []\n",
    "second_recompress_largeQP1_list10 = []\n",
    "\n",
    "second_sameQP_list1 = []\n",
    "second_sameQP_list2 = []\n",
    "second_sameQP_list3 = []\n",
    "second_sameQP_list4 = []\n",
    "second_sameQP_list5 = []\n",
    "second_sameQP_list6 = []\n",
    "second_sameQP_list7 = []\n",
    "second_sameQP_list8 = []\n",
    "second_sameQP_list9 = []\n",
    "second_sameQP_list10 = []\n",
    "\n",
    "second_recompress_sameQP_list1 = []\n",
    "second_recompress_sameQP_list2 = []\n",
    "second_recompress_sameQP_list3 = []\n",
    "second_recompress_sameQP_list4 = []\n",
    "second_recompress_sameQP_list5 = []\n",
    "second_recompress_sameQP_list6 = []\n",
    "second_recompress_sameQP_list7 = []\n",
    "second_recompress_sameQP_list8 = []\n",
    "second_recompress_sameQP_list9 = []\n",
    "second_recompress_sameQP_list10 = []\n",
    "\n",
    "second_largeQP2_list1 = []\n",
    "second_largeQP2_list2 = []\n",
    "second_largeQP2_list3 = []\n",
    "second_largeQP2_list4 = []\n",
    "second_largeQP2_list5 = []\n",
    "second_largeQP2_list6 = []\n",
    "second_largeQP2_list7 = []\n",
    "second_largeQP2_list8 = []\n",
    "second_largeQP2_list9 = []\n",
    "second_largeQP2_list10 = []\n",
    "\n",
    "second_recompress_largeQP2_list1 = []\n",
    "second_recompress_largeQP2_list2 = []\n",
    "second_recompress_largeQP2_list3 = []\n",
    "second_recompress_largeQP2_list4 = []\n",
    "second_recompress_largeQP2_list5 = []\n",
    "second_recompress_largeQP2_list6 = []\n",
    "second_recompress_largeQP2_list7 = []\n",
    "second_recompress_largeQP2_list8 = []\n",
    "second_recompress_largeQP2_list9 = []\n",
    "second_recompress_largeQP2_list10 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "\n",
    "def process_train_lists2(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath_pkl+f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath_pkl+f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath_pkl+f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath_pkl+f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath_pkl+f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath_pkl+f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath_pkl+f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath_pkl+f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "            \n",
    "\n",
    "# train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5, train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "single_listsA = [single_list1, single_list2,\n",
    "                single_list3, single_list4,\n",
    "                single_list5, single_list6,\n",
    "                single_list7, single_list8,\n",
    "                single_list9, single_list10]\n",
    "\n",
    "single_recompress_listsA = [single_recompress_list1, single_recompress_list2,\n",
    "                           single_recompress_list3, single_recompress_list4,\n",
    "                           single_recompress_list5, single_recompress_list6,\n",
    "                           single_recompress_list7, single_recompress_list8,\n",
    "                           single_recompress_list9, single_recompress_list10]\n",
    "\n",
    "\n",
    "second_largeQP1_listsA = [second_largeQP1_list1, second_largeQP1_list2,\n",
    "                        second_largeQP1_list3, second_largeQP1_list4,\n",
    "                        second_largeQP1_list5, second_largeQP1_list6,\n",
    "                        second_largeQP1_list7, second_largeQP1_list8,\n",
    "                        second_largeQP1_list9, second_largeQP1_list10]\n",
    "\n",
    "second_recompress_largeQP1_listsA = [second_recompress_largeQP1_list1, second_recompress_largeQP1_list2,\n",
    "                           second_recompress_largeQP1_list3, second_recompress_largeQP1_list4,\n",
    "                           second_recompress_largeQP1_list5, second_recompress_largeQP1_list6,\n",
    "                           second_recompress_largeQP1_list7, second_recompress_largeQP1_list8,\n",
    "                           second_recompress_largeQP1_list9, second_recompress_largeQP1_list10]\n",
    "\n",
    "\n",
    "second_sameQP_listsA = [second_sameQP_list1, second_sameQP_list2,\n",
    "                        second_sameQP_list3, second_sameQP_list4,\n",
    "                        second_sameQP_list5, second_sameQP_list6,\n",
    "                        second_sameQP_list7, second_sameQP_list8,\n",
    "                        second_sameQP_list9, second_sameQP_list10]\n",
    "\n",
    "second_recompress_sameQP_listsA = [second_recompress_sameQP_list1, second_recompress_sameQP_list2,\n",
    "                           second_recompress_sameQP_list3, second_recompress_sameQP_list4,\n",
    "                           second_recompress_sameQP_list5, second_recompress_sameQP_list6,\n",
    "                           second_recompress_sameQP_list7, second_recompress_sameQP_list8,\n",
    "                           second_recompress_sameQP_list9, second_recompress_sameQP_list10]\n",
    "\n",
    "\n",
    "second_largeQP2_listsA = [second_largeQP2_list1, second_largeQP2_list2,\n",
    "                        second_largeQP2_list3, second_largeQP2_list4,\n",
    "                        second_largeQP2_list5, second_largeQP2_list6,\n",
    "                        second_largeQP2_list7, second_largeQP2_list8,\n",
    "                        second_largeQP2_list9, second_largeQP2_list10]\n",
    "\n",
    "second_recompress_largeQP2_listsA = [second_recompress_largeQP2_list1, second_recompress_largeQP2_list2,\n",
    "                           second_recompress_largeQP2_list3, second_recompress_largeQP2_list4,\n",
    "                           second_recompress_largeQP2_list5, second_recompress_largeQP2_list6,\n",
    "                           second_recompress_largeQP2_list7, second_recompress_largeQP2_list8,\n",
    "                           second_recompress_largeQP2_list9, second_recompress_largeQP2_list10]\n",
    "\n",
    "\n",
    "\n",
    "for train_list, single_list, single_recompress_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                           single_listsA,\n",
    "                                                                                                                                                                                                                           single_recompress_listsA,\n",
    "                                                                                                                                                                                                                           second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                           second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                           second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP2_listsA\n",
    "                                                                                                                                                                                                                          ):\n",
    "    process_train_lists2(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "1710\n",
      "1710\n",
      "300\n",
      "300\n",
      "1170\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(single_listsA[6]))\n",
    "print(len(single_recompress_listsA[6]))\n",
    "print(len(second_largeQP1_listsA[6]))\n",
    "print(len(second_recompress_largeQP1_listsA[6]))\n",
    "print(len(second_sameQP_listsA[6]))\n",
    "print(len(second_recompress_sameQP_listsA[6]))\n",
    "print(len(second_largeQP2_listsA[6]))\n",
    "print(len(second_recompress_largeQP2_listsA[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "100\n",
      "100\n",
      "100\n",
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "single_csv1 = list(zip(single_lists[0], single_listsA[0], single_recompress_lists[0], single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(single_lists[1], single_listsA[1], single_recompress_lists[1], single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(single_lists[2], single_listsA[2], single_recompress_lists[2], single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(single_lists[3], single_listsA[3], single_recompress_lists[3], single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(single_lists[4], single_listsA[4], single_recompress_lists[4], single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(single_lists[5], single_listsA[5], single_recompress_lists[5], single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(single_lists[6], single_listsA[6], single_recompress_lists[6], single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(single_lists[7], single_listsA[7], single_recompress_lists[7], single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(single_lists[8], single_listsA[8], single_recompress_lists[8], single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(single_lists[9], single_listsA[9], single_recompress_lists[9], single_recompress_listsA[9]))\n",
    "print(len(single_csv7))\n",
    "\n",
    "second_largeQP1_csv1 = list(zip(second_largeQP1_lists[0], second_largeQP1_listsA[0], second_recompress_largeQP1_lists[0], second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(second_largeQP1_lists[1], second_largeQP1_listsA[1], second_recompress_largeQP1_lists[1], second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(second_largeQP1_lists[2], second_largeQP1_listsA[2], second_recompress_largeQP1_lists[2], second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(second_largeQP1_lists[3], second_largeQP1_listsA[3], second_recompress_largeQP1_lists[3], second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(second_largeQP1_lists[4], second_largeQP1_listsA[4], second_recompress_largeQP1_lists[4], second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(second_largeQP1_lists[5], second_largeQP1_listsA[5], second_recompress_largeQP1_lists[5], second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(second_largeQP1_lists[6], second_largeQP1_listsA[6], second_recompress_largeQP1_lists[6], second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(second_largeQP1_lists[7], second_largeQP1_listsA[7], second_recompress_largeQP1_lists[7], second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(second_largeQP1_lists[8], second_largeQP1_listsA[8], second_recompress_largeQP1_lists[8], second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(second_largeQP1_lists[9], second_largeQP1_listsA[9], second_recompress_largeQP1_lists[9], second_recompress_largeQP1_listsA[9]))\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 100)\n",
    "print(len(second_largeQP1_csv7))\n",
    "\n",
    "second_sameQP_csv1 = list(zip(second_sameQP_lists[0], second_sameQP_listsA[0], second_recompress_sameQP_lists[0], second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(second_sameQP_lists[1], second_sameQP_listsA[1], second_recompress_sameQP_lists[1], second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(second_sameQP_lists[2], second_sameQP_listsA[2], second_recompress_sameQP_lists[2], second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(second_sameQP_lists[3], second_sameQP_listsA[3], second_recompress_sameQP_lists[3], second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(second_sameQP_lists[4], second_sameQP_listsA[4], second_recompress_sameQP_lists[4], second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(second_sameQP_lists[5], second_sameQP_listsA[5], second_recompress_sameQP_lists[5], second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(second_sameQP_lists[6], second_sameQP_listsA[6], second_recompress_sameQP_lists[6], second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(second_sameQP_lists[7], second_sameQP_listsA[7], second_recompress_sameQP_lists[7], second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(second_sameQP_lists[8], second_sameQP_listsA[8], second_recompress_sameQP_lists[8], second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(second_sameQP_lists[9], second_sameQP_listsA[9], second_recompress_sameQP_lists[9], second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 100)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 100)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 100)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 100)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 100)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 100)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 100)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 100)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 100)\n",
    "second_sameQP_csv10 = random.sample(second_sameQP_csv10, 100)\n",
    "print(len(second_sameQP_csv7))\n",
    "\n",
    "second_largeQP2_csv1 = list(zip(second_largeQP2_lists[0], second_largeQP2_listsA[0], second_recompress_largeQP2_lists[0], second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(second_largeQP2_lists[1], second_largeQP2_listsA[1], second_recompress_largeQP2_lists[1], second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(second_largeQP2_lists[2], second_largeQP2_listsA[2], second_recompress_largeQP2_lists[2], second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(second_largeQP2_lists[3], second_largeQP2_listsA[3], second_recompress_largeQP2_lists[3], second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(second_largeQP2_lists[4], second_largeQP2_listsA[4], second_recompress_largeQP2_lists[4], second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(second_largeQP2_lists[5], second_largeQP2_listsA[5], second_recompress_largeQP2_lists[5], second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(second_largeQP2_lists[6], second_largeQP2_listsA[6], second_recompress_largeQP2_lists[6], second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(second_largeQP2_lists[7], second_largeQP2_listsA[7], second_recompress_largeQP2_lists[7], second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(second_largeQP2_lists[8], second_largeQP2_listsA[8], second_recompress_largeQP2_lists[8], second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(second_largeQP2_lists[9], second_largeQP2_listsA[9], second_recompress_largeQP2_lists[9], second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 100)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 100)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 100)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 100)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 100)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 100)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 100)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 100)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 100)\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 100)\n",
    "print(len(second_largeQP2_csv7))\n",
    "\n",
    "\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv10 + second_largeQP2_csv10\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10  0.000457  0.000857   0.001671  0.117086  0.073309\n",
      "1         16   0.00221  0.000518   0.001766  0.040061  0.022211\n",
      "2         20  0.001292  0.000632   0.000502  0.027687  0.019464\n",
      "3         24  0.000438  0.001074     0.0006  0.015096  0.008605\n",
      "4         27    0.0001  0.000633   0.001434  0.238281  0.190728\n",
      "..       ...       ...       ...        ...       ...       ...\n",
      "595       32  0.000009  0.000547   0.012768  0.221121   0.21157\n",
      "596       20  0.000245  0.020565   0.002304  0.310407  0.317506\n",
      "597       32  0.000049  0.001894   0.000899   0.21278  0.202102\n",
      "598       16  0.000355  0.000734   0.000192  0.016854  0.011868\n",
      "599       24  0.000034  0.006753   0.000265  0.347295  0.348914\n",
      "\n",
      "[600 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データフレームを結合\n",
    "combined_train_df = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], ignore_index=True)\n",
    "combined_train_df_onlyGhost = pd.concat([train_df_onlyGhost1, train_df_onlyGhost2, train_df_onlyGhost3, train_df_onlyGhost4, train_df_onlyGhost5, train_df_onlyGhost6, train_df_onlyGhost7, train_df_onlyGhost8, train_df_onlyGhost9, train_df_onlyGhost10], ignore_index=True)\n",
    "combined_LABEL = pd.concat([LABEL1, LABEL2, LABEL3, LABEL4, LABEL5, LABEL6, LABEL7, LABEL8, LABEL9, LABEL10], ignore_index=True)\n",
    "combined_MAE = pd.concat([MAE1, MAE2, MAE3, MAE4, MAE5, MAE6, MAE7, MAE8, MAE9, MAE10], ignore_index=True)\n",
    "combined_FINAL_QP = pd.concat([FINAL_QP1, FINAL_QP2, FINAL_QP3, FINAL_QP4, FINAL_QP5, FINAL_QP6, FINAL_QP7, FINAL_QP8, FINAL_QP9, FINAL_QP10], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 6)\n",
      "(6000, 44)\n",
      "(6000, 1)\n",
      "(6000, 1)\n",
      "(6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df.shape)\n",
    "print(combined_train_df_onlyGhost.shape)\n",
    "print(combined_LABEL.shape)\n",
    "print(combined_MAE.shape)\n",
    "print(combined_FINAL_QP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "Combined Train DF:\n",
      "  FINAL_QP    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10  0.000457  0.000857   0.001671  0.117086  0.073309\n",
      "1       16   0.00221  0.000518   0.001766  0.040061  0.022211\n",
      "2       20  0.001292  0.000632   0.000502  0.027687  0.019464\n",
      "3       24  0.000438  0.001074     0.0006  0.015096  0.008605\n",
      "4       27    0.0001  0.000633   0.001434  0.238281  0.190728\n",
      "Combined Train DF Only Ghost:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4 LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27 LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10      0  14848   9760   7592  27800      0  14720   9744   7068  28468  6109  10992  2540   4598   1669   1943   2451   1757  6089  11990  2542   4709   1652   1998   2396   1774  14824  11408   8728   8124   1896  15020  15816  11000   9176   8308   1836  13864  0.000457  0.000857   0.001671  0.117086  0.073309\n",
      "1       16      0  19584   6368   9056  24992      0  18560   7376   8412  25652  5629  10101  2104   3674   1578   1974   2986   1724  5798  10555  2205   3607   1587   2003   2824   1725  12484   5056   7448   4724   1996  28292  11536   5472   8116   4964   1744  28168   0.00221  0.000518   0.001766  0.040061  0.022211\n",
      "2       20      0  20288   6512  10500  22700      0  19712   7072   9700  23516  6366   8825  2148   3384   1625   1816   2922   1739  6803   8741  2194   3344   1588   1752   3051   1699  10628   2876   6096   3276   2540  34584  10324   3036   6440   3528   2524  34148  0.001292  0.000632   0.000502  0.027687  0.019464\n",
      "3       24      0  23744   3968  11148  21140      0  23424   4272  10720  21584  6721   7639  1899   3448   1390   1852   2666   1788  7257   7834  2112   3448   1308   1814   2558   1870   7976   1788   5336   1884   1020  41996   7348   1744   5520   1792   1024  42572  0.000438  0.001074     0.0006  0.015096  0.008605\n",
      "4       27      0  24384   4928  12012  18676      0  24384   5024  11708  18884  8015   5357  1765   3225   1387   1613   2661   1625  8381   5770  1770   3385   1363   1767   2586   1628   5444   1208   3952   1436    596  47364   5344   1060   3540   1208    452  48396    0.0001  0.000633   0.001434  0.238281  0.190728\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\")\n",
    "print(\"Combined Train DF:\")\n",
    "print(combined_train_df.head())\n",
    "print(\"Combined Train DF Only Ghost:\")\n",
    "print(combined_train_df_onlyGhost.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling:\n",
      "X_train:\n",
      "       0         1         2         3         4         5\n",
      "0  0.125  0.012172  0.004898  0.017093  0.115655  0.072245\n",
      "1  0.275  0.058844  0.002940  0.018062  0.038501  0.021082\n",
      "2  0.375  0.034419  0.003596  0.005108  0.026107  0.018332\n",
      "3  0.475  0.011666  0.006148  0.006112  0.013495  0.007460\n",
      "4  0.550  0.002656  0.003601  0.014656  0.237053  0.189815\n",
      "X_train_onlyGhost:\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.125  0.0  0.251900  0.290200  0.263172  0.484422  0.0  0.249729  0.319015  0.241262  0.494751  0.094179  0.270190  0.194845  0.197464  0.138358  0.135523  0.085602  0.183883  0.098508  0.292663  0.196051  0.200310  0.137448  0.134700  0.075194  0.180928  0.364836  0.409300  0.516939  0.496456  0.205908  0.209212  0.387495  0.383008  0.478515  0.469166  0.170315  0.191391  0.012172  0.004898  0.017093  0.115655  0.072245\n",
      "1  0.275  0.0  0.332248  0.189343  0.313921  0.435492  0.0  0.314875  0.241488  0.287138  0.445812  0.078267  0.243480  0.161399  0.157748  0.130796  0.137686  0.106863  0.180429  0.088144  0.249761  0.170060  0.152859  0.132027  0.135037  0.090462  0.175931  0.307246  0.181401  0.441128  0.288682  0.216768  0.442546  0.282634  0.190529  0.423237  0.280325  0.161781  0.442092  0.058844  0.002940  0.018062  0.038501  0.021082\n",
      "2  0.375  0.0  0.344191  0.193625  0.363977  0.395553  0.0  0.334419  0.231535  0.331103  0.408690  0.102698  0.205228  0.164774  0.145283  0.134702  0.126665  0.104320  0.181999  0.123936  0.195527  0.169212  0.141535  0.132110  0.118115  0.098559  0.173279  0.261567  0.103186  0.361052  0.200196  0.275847  0.553165  0.252940  0.105710  0.335836  0.199232  0.234137  0.546901  0.034419  0.003596  0.005108  0.026107  0.018332\n",
      "3  0.475  0.0  0.402823  0.117983  0.386439  0.368370  0.0  0.397394  0.139864  0.365920  0.375113  0.114467  0.169674  0.145674  0.148034  0.115174  0.129176  0.094146  0.187127  0.140105  0.168411  0.162888  0.146013  0.108757  0.122295  0.080973  0.190719  0.196298  0.064150  0.316039  0.115131  0.110773  0.683474  0.180027  0.060724  0.287860  0.101197  0.094991  0.694546  0.011666  0.006148  0.006112  0.013495  0.007460\n",
      "4  0.550  0.0  0.413681  0.146527  0.416389  0.325434  0.0  0.413681  0.164484  0.399645  0.328189  0.157363  0.101265  0.135394  0.138448  0.114924  0.112506  0.093947  0.170068  0.180135  0.106703  0.136511  0.143300  0.113344  0.119126  0.081972  0.166038  0.133983  0.043341  0.234068  0.087754  0.064726  0.777848  0.130929  0.036908  0.184606  0.068218  0.041929  0.796621  0.002656  0.003601  0.014656  0.237053  0.189815\n",
      "Length of X_train_restored[0]: 600\n",
      "Length of X_train_onlyGhost_restored[0]: 600\n",
      "Length of MAE_restored[0]: 600\n",
      "Length of FINAL_QP_restored[0]: 600\n",
      "Length of Y_train_restored[0]: 600\n",
      "Length of X_train_restored[1]: 600\n",
      "Length of X_train_onlyGhost_restored[1]: 600\n",
      "Length of MAE_restored[1]: 600\n",
      "Length of FINAL_QP_restored[1]: 600\n",
      "Length of Y_train_restored[1]: 600\n",
      "Length of X_train_restored[2]: 600\n",
      "Length of X_train_onlyGhost_restored[2]: 600\n",
      "Length of MAE_restored[2]: 600\n",
      "Length of FINAL_QP_restored[2]: 600\n",
      "Length of Y_train_restored[2]: 600\n",
      "Length of X_train_restored[3]: 600\n",
      "Length of X_train_onlyGhost_restored[3]: 600\n",
      "Length of MAE_restored[3]: 600\n",
      "Length of FINAL_QP_restored[3]: 600\n",
      "Length of Y_train_restored[3]: 600\n",
      "Length of X_train_restored[4]: 600\n",
      "Length of X_train_onlyGhost_restored[4]: 600\n",
      "Length of MAE_restored[4]: 600\n",
      "Length of FINAL_QP_restored[4]: 600\n",
      "Length of Y_train_restored[4]: 600\n",
      "Length of X_train_restored[5]: 600\n",
      "Length of X_train_onlyGhost_restored[5]: 600\n",
      "Length of MAE_restored[5]: 600\n",
      "Length of FINAL_QP_restored[5]: 600\n",
      "Length of Y_train_restored[5]: 600\n",
      "Length of X_train_restored[6]: 600\n",
      "Length of X_train_onlyGhost_restored[6]: 600\n",
      "Length of MAE_restored[6]: 600\n",
      "Length of FINAL_QP_restored[6]: 600\n",
      "Length of Y_train_restored[6]: 600\n",
      "Length of X_train_restored[7]: 600\n",
      "Length of X_train_onlyGhost_restored[7]: 600\n",
      "Length of MAE_restored[7]: 600\n",
      "Length of FINAL_QP_restored[7]: 600\n",
      "Length of Y_train_restored[7]: 600\n",
      "Length of X_train_restored[8]: 600\n",
      "Length of X_train_onlyGhost_restored[8]: 600\n",
      "Length of MAE_restored[8]: 600\n",
      "Length of FINAL_QP_restored[8]: 600\n",
      "Length of Y_train_restored[8]: 600\n",
      "Length of X_train_restored[9]: 600\n",
      "Length of X_train_onlyGhost_restored[9]: 600\n",
      "Length of MAE_restored[9]: 600\n",
      "Length of FINAL_QP_restored[9]: 600\n",
      "Length of Y_train_restored[9]: 600\n"
     ]
    }
   ],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP, scaler_main=None, scaler_ghost=None, fit_scaler=True):\n",
    "\n",
    "    scaler_main = MinMaxScaler()\n",
    "    scaler_ghost = MinMaxScaler()\n",
    "    X_train = scaler_main.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler_ghost.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train, scaler_main, scaler_ghost\n",
    "\n",
    "# スケーリングの処理\n",
    "X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train, scaler_main, scaler_ghost = process_results_to_lists(\n",
    "    combined_train_df, combined_train_df_onlyGhost, combined_LABEL, combined_MAE, combined_FINAL_QP, fit_scaler=True\n",
    ")\n",
    "\n",
    "# スケーリング後のデータを表示\n",
    "print(\"After scaling:\")\n",
    "print(\"X_train:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "print(\"X_train_onlyGhost:\")\n",
    "print(pd.DataFrame(X_train_onlyGhost).head())\n",
    "\n",
    "# データを元に戻すための関数\n",
    "def restore_data_to_original_order(data, num_splits, original_lengths):\n",
    "    restored_data = []\n",
    "    start_index = 0\n",
    "    for length in original_lengths:\n",
    "        restored_data.append(data[start_index:start_index + length])\n",
    "        start_index += length\n",
    "    return restored_data\n",
    "\n",
    "# 元のデータフレームの長さ\n",
    "original_lengths = [len(train_df1), len(train_df2), len(train_df3), len(train_df4), len(train_df5), \n",
    "                    len(train_df6), len(train_df7), len(train_df8), len(train_df9), len(train_df10)]\n",
    "\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "# データを元の順序に戻す\n",
    "X_train_list = restore_data_to_original_order(X_train, 10, original_lengths)\n",
    "X_train_onlyGhost_list = restore_data_to_original_order(X_train_onlyGhost, 10, original_lengths)\n",
    "MAE_list = restore_data_to_original_order(MAE_array, 10, original_lengths)\n",
    "FINAL_QP_list = restore_data_to_original_order(FINAL_QP_array, 10, original_lengths)\n",
    "Y_train_list = restore_data_to_original_order(Y_train, 10, original_lengths)\n",
    "\n",
    "# 確認用の出力\n",
    "for i in range(10):\n",
    "    print(f\"Length of X_train_restored[{i}]: {len(X_train_list[i])}\")\n",
    "    print(f\"Length of X_train_onlyGhost_restored[{i}]: {len(X_train_onlyGhost_list[i])}\")\n",
    "    print(f\"Length of MAE_restored[{i}]: {len(MAE_list[i])}\")\n",
    "    print(f\"Length of FINAL_QP_restored[{i}]: {len(FINAL_QP_list[i])}\")\n",
    "    print(f\"Length of Y_train_restored[{i}]: {len(Y_train_list[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7544    0.8500    0.7994       300\n",
      "           1     0.8282    0.7233    0.7722       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7913    0.7867    0.7858       600\n",
      "weighted avg     0.7913    0.7867    0.7858       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6303    0.8467    0.7226       300\n",
      "           1     0.7665    0.5033    0.6076       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6984    0.6750    0.6651       600\n",
      "weighted avg     0.6984    0.6750    0.6651       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8133    0.8133    0.8133       300\n",
      "           1     0.8133    0.8133    0.8133       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8133    0.8133    0.8133       600\n",
      "weighted avg     0.8133    0.8133    0.8133       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7975    0.8667    0.8307       300\n",
      "           1     0.8540    0.7800    0.8153       300\n",
      "\n",
      "    accuracy                         0.8233       600\n",
      "   macro avg     0.8258    0.8233    0.8230       600\n",
      "weighted avg     0.8258    0.8233    0.8230       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5663    0.9533    0.7106       300\n",
      "           1     0.8526    0.2700    0.4101       300\n",
      "\n",
      "    accuracy                         0.6117       600\n",
      "   macro avg     0.7095    0.6117    0.5603       600\n",
      "weighted avg     0.7095    0.6117    0.5603       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7350    0.8600    0.7926       300\n",
      "           1     0.8313    0.6900    0.7541       300\n",
      "\n",
      "    accuracy                         0.7750       600\n",
      "   macro avg     0.7832    0.7750    0.7734       600\n",
      "weighted avg     0.7832    0.7750    0.7734       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6406    0.8200    0.7193       300\n",
      "           1     0.7500    0.5400    0.6279       300\n",
      "\n",
      "    accuracy                         0.6800       600\n",
      "   macro avg     0.6953    0.6800    0.6736       600\n",
      "weighted avg     0.6953    0.6800    0.6736       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8159    0.8567    0.8358       300\n",
      "           1     0.8491    0.8067    0.8274       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8325    0.8317    0.8316       600\n",
      "weighted avg     0.8325    0.8317    0.8316       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8127    0.8533    0.8325       300\n",
      "           1     0.8456    0.8033    0.8239       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8292    0.8283    0.8282       600\n",
      "weighted avg     0.8292    0.8283    0.8282       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5601    0.9167    0.6953       300\n",
      "           1     0.7706    0.2800    0.4108       300\n",
      "\n",
      "    accuracy                         0.5983       600\n",
      "   macro avg     0.6654    0.5983    0.5530       600\n",
      "weighted avg     0.6654    0.5983    0.5530       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7399    0.7967    0.7673       300\n",
      "           1     0.7798    0.7200    0.7487       300\n",
      "\n",
      "    accuracy                         0.7583       600\n",
      "   macro avg     0.7599    0.7583    0.7580       600\n",
      "weighted avg     0.7599    0.7583    0.7580       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6450    0.7933    0.7115       300\n",
      "           1     0.7316    0.5633    0.6365       300\n",
      "\n",
      "    accuracy                         0.6783       600\n",
      "   macro avg     0.6883    0.6783    0.6740       600\n",
      "weighted avg     0.6883    0.6783    0.6740       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8167    0.8467    0.8314       300\n",
      "           1     0.8408    0.8100    0.8251       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8288    0.8283    0.8283       600\n",
      "weighted avg     0.8288    0.8283    0.8283       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8267    0.8267    0.8267       300\n",
      "           1     0.8267    0.8267    0.8267       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8267    0.8267    0.8267       600\n",
      "weighted avg     0.8267    0.8267    0.8267       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5909    0.6933    0.6380       300\n",
      "           1     0.6290    0.5200    0.5693       300\n",
      "\n",
      "    accuracy                         0.6067       600\n",
      "   macro avg     0.6100    0.6067    0.6037       600\n",
      "weighted avg     0.6100    0.6067    0.6037       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7175    0.8633    0.7837       300\n",
      "           1     0.8285    0.6600    0.7347       300\n",
      "\n",
      "    accuracy                         0.7617       600\n",
      "   macro avg     0.7730    0.7617    0.7592       600\n",
      "weighted avg     0.7730    0.7617    0.7592       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6154    0.7733    0.6854       300\n",
      "           1     0.6951    0.5167    0.5927       300\n",
      "\n",
      "    accuracy                         0.6450       600\n",
      "   macro avg     0.6552    0.6450    0.6391       600\n",
      "weighted avg     0.6552    0.6450    0.6391       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7661    0.8733    0.8162       300\n",
      "           1     0.8527    0.7333    0.7885       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8094    0.8033    0.8024       600\n",
      "weighted avg     0.8094    0.8033    0.8024       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7806    0.8300    0.8045       300\n",
      "           1     0.8185    0.7667    0.7917       300\n",
      "\n",
      "    accuracy                         0.7983       600\n",
      "   macro avg     0.7995    0.7983    0.7981       600\n",
      "weighted avg     0.7995    0.7983    0.7981       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5622    0.9033    0.6931       300\n",
      "           1     0.7542    0.2967    0.4258       300\n",
      "\n",
      "    accuracy                         0.6000       600\n",
      "   macro avg     0.6582    0.6000    0.5595       600\n",
      "weighted avg     0.6582    0.6000    0.5595       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7647    0.9100    0.8311       300\n",
      "           1     0.8889    0.7200    0.7956       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8268    0.8150    0.8133       600\n",
      "weighted avg     0.8268    0.8150    0.8133       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6577    0.8967    0.7588       300\n",
      "           1     0.8377    0.5333    0.6517       300\n",
      "\n",
      "    accuracy                         0.7150       600\n",
      "   macro avg     0.7477    0.7150    0.7053       600\n",
      "weighted avg     0.7477    0.7150    0.7053       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7969    0.8500    0.8226       300\n",
      "           1     0.8393    0.7833    0.8103       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8181    0.8167    0.8165       600\n",
      "weighted avg     0.8181    0.8167    0.8165       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7981    0.8433    0.8201       300\n",
      "           1     0.8339    0.7867    0.8096       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8160    0.8150    0.8149       600\n",
      "weighted avg     0.8160    0.8150    0.8149       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5625    0.9600    0.7094       300\n",
      "           1     0.8636    0.2533    0.3918       300\n",
      "\n",
      "    accuracy                         0.6067       600\n",
      "   macro avg     0.7131    0.6067    0.5506       600\n",
      "weighted avg     0.7131    0.6067    0.5506       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7252    0.8533    0.7841       300\n",
      "           1     0.8219    0.6767    0.7422       300\n",
      "\n",
      "    accuracy                         0.7650       600\n",
      "   macro avg     0.7735    0.7650    0.7632       600\n",
      "weighted avg     0.7735    0.7650    0.7632       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6400    0.8533    0.7314       300\n",
      "           1     0.7800    0.5200    0.6240       300\n",
      "\n",
      "    accuracy                         0.6867       600\n",
      "   macro avg     0.7100    0.6867    0.6777       600\n",
      "weighted avg     0.7100    0.6867    0.6777       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8104    0.8833    0.8453       300\n",
      "           1     0.8718    0.7933    0.8307       300\n",
      "\n",
      "    accuracy                         0.8383       600\n",
      "   macro avg     0.8411    0.8383    0.8380       600\n",
      "weighted avg     0.8411    0.8383    0.8380       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8071    0.8367    0.8216       300\n",
      "           1     0.8304    0.8000    0.8149       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8188    0.8183    0.8183       600\n",
      "weighted avg     0.8188    0.8183    0.8183       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5848    0.7933    0.6733       300\n",
      "           1     0.6788    0.4367    0.5314       300\n",
      "\n",
      "    accuracy                         0.6150       600\n",
      "   macro avg     0.6318    0.6150    0.6024       600\n",
      "weighted avg     0.6318    0.6150    0.6024       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7425    0.8267    0.7823       300\n",
      "           1     0.8045    0.7133    0.7562       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7735    0.7700    0.7693       600\n",
      "weighted avg     0.7735    0.7700    0.7693       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6425    0.8267    0.7230       300\n",
      "           1     0.7570    0.5400    0.6304       300\n",
      "\n",
      "    accuracy                         0.6833       600\n",
      "   macro avg     0.6997    0.6833    0.6767       600\n",
      "weighted avg     0.6997    0.6833    0.6767       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8235    0.8400    0.8317       300\n",
      "           1     0.8367    0.8200    0.8283       300\n",
      "\n",
      "    accuracy                         0.8300       600\n",
      "   macro avg     0.8301    0.8300    0.8300       600\n",
      "weighted avg     0.8301    0.8300    0.8300       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8000    0.8000       300\n",
      "           1     0.8000    0.8000    0.8000       300\n",
      "\n",
      "    accuracy                         0.8000       600\n",
      "   macro avg     0.8000    0.8000    0.8000       600\n",
      "weighted avg     0.8000    0.8000    0.8000       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5583    0.9733    0.7096       300\n",
      "           1     0.8961    0.2300    0.3660       300\n",
      "\n",
      "    accuracy                         0.6017       600\n",
      "   macro avg     0.7272    0.6017    0.5378       600\n",
      "weighted avg     0.7272    0.6017    0.5378       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7422    0.8733    0.8025       300\n",
      "           1     0.8462    0.6967    0.7642       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7942    0.7850    0.7833       600\n",
      "weighted avg     0.7942    0.7850    0.7833       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6373    0.8667    0.7345       300\n",
      "           1     0.7917    0.5067    0.6179       300\n",
      "\n",
      "    accuracy                         0.6867       600\n",
      "   macro avg     0.7145    0.6867    0.6762       600\n",
      "weighted avg     0.7145    0.6867    0.6762       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8129    0.8400    0.8262       300\n",
      "           1     0.8345    0.8067    0.8203       300\n",
      "\n",
      "    accuracy                         0.8233       600\n",
      "   macro avg     0.8237    0.8233    0.8233       600\n",
      "weighted avg     0.8237    0.8233    0.8233       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7987    0.8067    0.8027       300\n",
      "           1     0.8047    0.7967    0.8007       300\n",
      "\n",
      "    accuracy                         0.8017       600\n",
      "   macro avg     0.8017    0.8017    0.8017       600\n",
      "weighted avg     0.8017    0.8017    0.8017       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5671    0.9300    0.7045       300\n",
      "           1     0.8056    0.2900    0.4265       300\n",
      "\n",
      "    accuracy                         0.6100       600\n",
      "   macro avg     0.6863    0.6100    0.5655       600\n",
      "weighted avg     0.6863    0.6100    0.5655       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7610    0.8067    0.7832       300\n",
      "           1     0.7943    0.7467    0.7698       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7777    0.7767    0.7765       600\n",
      "weighted avg     0.7777    0.7767    0.7765       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6547    0.7900    0.7160       300\n",
      "           1     0.7353    0.5833    0.6506       300\n",
      "\n",
      "    accuracy                         0.6867       600\n",
      "   macro avg     0.6950    0.6867    0.6833       600\n",
      "weighted avg     0.6950    0.6867    0.6833       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7853    0.8167    0.8007       300\n",
      "           1     0.8090    0.7767    0.7925       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.7971    0.7967    0.7966       600\n",
      "weighted avg     0.7971    0.7967    0.7966       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7967    0.7967    0.7967       300\n",
      "           1     0.7967    0.7967    0.7967       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.7967    0.7967    0.7967       600\n",
      "weighted avg     0.7967    0.7967    0.7967       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5703    0.9467    0.7118       300\n",
      "           1     0.8431    0.2867    0.4279       300\n",
      "\n",
      "    accuracy                         0.6167       600\n",
      "   macro avg     0.7067    0.6167    0.5698       600\n",
      "weighted avg     0.7067    0.6167    0.5698       600\n",
      "\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7416    0.8133    0.7758       300\n",
      "           1     0.7934    0.7167    0.7531       300\n",
      "\n",
      "    accuracy                         0.7650       600\n",
      "   macro avg     0.7675    0.7650    0.7644       600\n",
      "weighted avg     0.7675    0.7650    0.7644       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6442    0.8267    0.7241       300\n",
      "           1     0.7581    0.5433    0.6330       300\n",
      "\n",
      "    accuracy                         0.6850       600\n",
      "   macro avg     0.7011    0.6850    0.6785       600\n",
      "weighted avg     0.7011    0.6850    0.6785       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8095    0.7933    0.8013       300\n",
      "           1     0.7974    0.8133    0.8053       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8035    0.8033    0.8033       600\n",
      "weighted avg     0.8035    0.8033    0.8033       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8054    0.8000    0.8027       300\n",
      "           1     0.8013    0.8067    0.8040       300\n",
      "\n",
      "    accuracy                         0.8033       600\n",
      "   macro avg     0.8033    0.8033    0.8033       600\n",
      "weighted avg     0.8033    0.8033    0.8033       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5760    0.9600    0.7200       300\n",
      "           1     0.8800    0.2933    0.4400       300\n",
      "\n",
      "    accuracy                         0.6267       600\n",
      "   macro avg     0.7280    0.6267    0.5800       600\n",
      "weighted avg     0.7280    0.6267    0.5800       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Score_RBF', 'tnr_rbf', 'tpr_rbf',\n",
    "                                'C_LINEAR', 'Score_LINEAR', 'tnr_linear', 'tpr_linear',\n",
    "                                'C_OG_RBF', 'Score_OG_RBF', 'tnr_og_rbf', 'tpr_og_rbf',\n",
    "                                'C_OG_LINEAR', 'Score_OG_LINEAR', 'tnr_og_linear', 'tpr_og_linear',\n",
    "                                'Threshold', 'Score_old', 'tnr_old', 'tpr_old'])\n",
    "\n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "        \n",
    "    test_data = [X_train_list[i] for i in test_ids]\n",
    "    test_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    test_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    train_data = [item for data in train_data for item in data]\n",
    "    train_data_OG = [item for data in train_data_OG for item in data]\n",
    "    train_label = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(train_data, train_label, test_size=600, random_state=42)\n",
    "    X_train_OG, X_val_OG, _, _ = train_test_split(train_data_OG, train_label, test_size=600, random_state=42)\n",
    "    \n",
    "    test_data = [item for data in test_data for item in data]\n",
    "    test_data_OG = [item for data in test_data_OG for item in data]\n",
    "    test_label = [item for data in test_label for item in data]\n",
    "\n",
    "    MAE_data = [MAE_list[i] for i in test_ids]\n",
    "    MAE_data = [item for data in MAE_data for item in data]\n",
    "    \n",
    "    FINAL_QP_data = [FINAL_QP_list[i] for i in test_ids]\n",
    "    FINAL_QP_data = [item for data in FINAL_QP_data for item in data]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "                \n",
    "    for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "        results_old = np.array([is_double_compressed(MAE_data[i], FINAL_QP_data[i], threshold) for i in range(600)])\n",
    "        predicted_labels = results_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(test_data)\n",
    "    test_accuracy_RBF = accuracy_score(test_label, test_predictions_RBF)\n",
    "    report_RBF = classification_report(test_label, test_predictions_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_RBF)\n",
    "    tnr_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data)\n",
    "    test_accuracy_LINEAR = accuracy_score(test_label, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label, test_predictions_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_LINEAR)\n",
    "    tnr_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(test_data_OG)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(test_label, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(test_label, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_RBF)\n",
    "    tnr_og_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary2_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(test_data_OG)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(test_label, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    tnr_og_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary2_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Score_RBF': test_accuracy_RBF, 'tnr_rbf': tnr_rbf, 'tpr_rbf': tpr_rbf,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Score_LINEAR': test_accuracy_LINEAR, 'tnr_linear': tnr_linear, 'tpr_linear': tpr_linear,\n",
    "              'C_OG_RBF': best_c_value_onlyGhost_RBF, 'Score_OG_RBF': test_accuracy_onlyGhost_RBF, 'tnr_og_rbf': tnr_og_rbf, 'tpr_og_rbf': tpr_og_rbf,\n",
    "              'C_OG_LINEAR': best_c_value_onlyGhost_LINEAR, 'Score_OG_LINEAR': test_accuracy_onlyGhost_LINEAR, 'tnr_og_linear': tnr_og_linear, 'tpr_og_linear': tpr_og_linear,\n",
    "              'Threshold': best_threshold, 'Score_old': best_accuracy, 'tnr_old': tnr_old, 'tpr_old': tpr_old}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0      RBF        84.53        70.63               77.58                1.67           81.50           75.83\n",
      "1   LINEAR        82.93        53.50               68.22                1.70           71.50           64.50\n",
      "2     RBF2        84.13        79.57               81.85                1.41           83.83           79.67\n",
      "3  LINEAR2        82.60        79.63               81.12                1.25           82.83           79.67\n",
      "4      OLD        90.30        31.57               60.93                0.86           62.67           59.83\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF', 'LINEAR', 'RBF2', 'LINEAR2', 'OLD'],\n",
    "    'Average TNR': [round(results['tnr_rbf'].mean() * 100, 2), round(results['tnr_linear'].mean() * 100, 2), round(results['tnr_og_rbf'].mean() * 100, 2), round(results['tnr_og_linear'].mean() * 100, 2), round(results['tnr_old'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf'].mean() * 100, 2), round(results['tpr_linear'].mean() * 100, 2), round(results['tpr_og_rbf'].mean() * 100, 2), round(results['tpr_og_linear'].mean() * 100, 2), round(results['tpr_old'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF'].mean() * 100, 2), round(results['Score_LINEAR'].mean() * 100, 2), round(results['Score_OG_RBF'].mean() * 100, 2), round(results['Score_OG_LINEAR'].mean() * 100, 2), round(results['Score_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF'].std() * 100, 2), round(results['Score_LINEAR'].std() * 100, 2), round(results['Score_OG_RBF'].std() * 100, 2), round(results['Score_OG_LINEAR'].std() * 100, 2), round(results['Score_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF'].max() * 100, 2), round(results['Score_LINEAR'].max() * 100, 2), round(results['Score_OG_RBF'].max() * 100, 2), round(results['Score_OG_LINEAR'].max() * 100, 2), round(results['Score_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF'].min() * 100, 2), round(results['Score_LINEAR'].min() * 100, 2), round(results['Score_OG_RBF'].min() * 100, 2), round(results['Score_OG_LINEAR'].min() * 100, 2), round(results['Score_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3000\n",
      "1    3000\n",
      "2    2000\n",
      "3    3000\n",
      "4    5000\n",
      "5    2000\n",
      "6    5000\n",
      "7    1000\n",
      "8    1000\n",
      "9    5000\n",
      "Name: C_RBF, dtype: object\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: C_LINEAR, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF'])\n",
    "print(results['C_LINEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
