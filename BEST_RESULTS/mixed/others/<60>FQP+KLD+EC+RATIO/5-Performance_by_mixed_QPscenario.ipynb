{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['223', '228', '132', '154', '189', '210', '209', '162', '18', '172', '241', '278', '229', '144', '83', '195', '16', '272', '165', '268', '255', '70', '95', '101', '196', '41', '295', '12', '14', '92'], ['214', '161', '49', '200', '69', '104', '198', '150', '231', '276', '28', '54', '284', '256', '78', '249', '171', '265', '182', '179', '66', '158', '76', '199', '107', '159', '184', '176', '39', '287'], ['102', '147', '112', '233', '31', '257', '82', '47', '291', '135', '67', '120', '2', '17', '3', '124', '59', '1', '238', '299', '30', '109', '296', '235', '213', '55', '234', '88', '225', '245'], ['29', '98', '10', '48', '64', '170', '220', '52', '24', '20', '232', '152', '286', '292', '103', '71', '23', '139', '119', '290', '84', '270', '38', '204', '134', '35', '126', '221', '116', '43'], ['206', '275', '193', '75', '145', '246', '181', '280', '79', '146', '163', '37', '186', '130', '156', '131', '166', '100', '187', '261', '215', '169', '62', '93', '167', '129', '201', '42', '22', '244'], ['191', '153', '11', '114', '266', '289', '282', '253', '192', '50', '149', '208', '56', '203', '60', '25', '72', '177', '230', '136', '157', '108', '178', '32', '58', '115', '216', '264', '105', '262'], ['21', '217', '33', '263', '254', '110', '89', '113', '173', '197', '125', '127', '151', '26', '44', '13', '224', '90', '61', '281', '226', '279', '247', '205', '138', '258', '80', '133', '4', '168'], ['267', '250', '34', '175', '180', '293', '288', '194', '222', '294', '252', '185', '143', '274', '121', '219', '73', '6', '91', '237', '183', '260', '85', '123', '239', '212', '99', '94', '128', '298'], ['251', '243', '46', '283', '97', '77', '202', '277', '164', '155', '51', '211', '57', '227', '19', '74', '190', '45', '240', '248', '81', '148', '7', '118', '242', '15', '141', '269', '218', '96'], ['65', '271', '300', '122', '53', '160', '188', '273', '68', '9', '86', '40', '174', '5', '27', '8', '117', '111', '207', '297', '140', '259', '87', '36', '106', '142', '63', '137', '285', '236']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sameQP = 1\n",
    "QPD0 = [\"_1stQP5_2ndQP5\", \"_1stQP10_2ndQP10\", \"_1stQP16_2ndQP16\", \"_1stQP20_2ndQP20\", \"_1stQP24_2ndQP24\", \n",
    "        \"_1stQP27_2ndQP27\", \"_1stQP32_2ndQP32\", \"_1stQP39_2ndQP39\", \"_1stQP42_2ndQP42\", \"_1stQP45_2ndQP45\"]\n",
    "\n",
    "# largeQP1 = 30\n",
    "QPD1 = [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"]\n",
    "QPD3 = [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"]\n",
    "QPD4 = [\"_1stQP20_2ndQP16_\"]\n",
    "QPD5 = [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"]\n",
    "QPD6 = [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"]\n",
    "QPD8 = [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"]\n",
    "QPD9 = [\"_1stQP25_2ndQP16_\"]\n",
    "QPD10 = [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"]\n",
    "QPD11 = [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"]\n",
    "QPD12 = [\"_1stQP32_2ndQP20_\"]\n",
    "QPD13 = [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"]\n",
    "QPD14 = [\"_1stQP30_2ndQP16_\"]\n",
    "QPD15 = [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"]\n",
    "QPD16 = [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"]\n",
    "QPD18 = [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"]\n",
    "QPD19 = [\"_1stQP35_2ndQP16_\"]\n",
    "QPD20 = [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"]\n",
    "QPD21 = [\"_1stQP45_2ndQP24_\"]\n",
    "QPD22 = [\"_1stQP32_2ndQP10_\"]\n",
    "QPD23 = [\"_1stQP50_2ndQP27_\"]\n",
    "QPD24 = [\"_1stQP40_2ndQP16_\"]\n",
    "QPD25 = [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"]\n",
    "QPD26 = [\"_1stQP50_2ndQP24_\"]\n",
    "QPD27 = [\"_1stQP32_2ndQP5_\"]\n",
    "QPD29 = [\"_1stQP45_2ndQP16_\"]\n",
    "QPD30 = [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"]\n",
    "QPD34 = [\"_1stQP50_2ndQP16_\"]\n",
    "QPD35 = [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"]\n",
    "QPD40 = [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"]\n",
    "QPD45 = [\"_1stQP50_2ndQP5_\"]\n",
    "\n",
    "# largeQP2 = 23\n",
    "QPD1M = [\"_1stQP15_2ndQP16\"]\n",
    "QPD2M = [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"]\n",
    "QPD4M = [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"]\n",
    "QPD5M = [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"]\n",
    "QPD6M = [\"_1stQP10_2ndQP16\"]\n",
    "QPD7M = [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"]\n",
    "QPD9M = [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"]\n",
    "QPD10M = [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"]\n",
    "QPD12M = [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"]\n",
    "QPD13M = [\"_1stQP32_2ndQP45\"]\n",
    "QPD14M = [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"]\n",
    "QPD15M = [\"_1stQP30_2ndQP45\"]\n",
    "QPD17M = [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"]\n",
    "QPD19M = [\"_1stQP20_2ndQP39\"]\n",
    "QPD20M = [\"_1stQP25_2ndQP45\"]\n",
    "QPD22M = [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"]\n",
    "QPD24M = [\"_1stQP15_2ndQP39\"]\n",
    "QPD25M = [\"_1stQP20_2ndQP45\"]\n",
    "QPD27M = [\"_1stQP15_2ndQP42\"]\n",
    "QPD29M = [\"_1stQP10_2ndQP39\"]\n",
    "QPD30M = [\"_1stQP15_2ndQP45\"]\n",
    "QPD32M = [\"_1stQP10_2ndQP42\"]\n",
    "QPD35M = [\"_1stQP10_2ndQP45\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single images:  300\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print('single images: ', len(single_csv9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD6': 60, 'QPD8': 60, 'QPD9': 60, 'QPD10': 60, 'QPD3': 60, 'QPD1': 60, 'QPD4': 60, 'QPD5': 60, 'QPD12': 30, 'QPD23': 30, 'QPD34': 30, 'QPD45': 30, 'QPD22': 30, 'QPD26': 30, 'QPD35': 30, 'QPD21': 30, 'QPD30': 20, 'QPD15': 20, 'QPD11': 20, 'QPD29': 20, 'QPD19': 20, 'QPD20': 20, 'QPD25': 20, 'QPD16': 20, 'QPD40': 20, 'QPD27': 20, 'QPD14': 20, 'QPD13': 20, 'QPD18': 20, 'QPD24': 20})\n",
      "\n",
      "double images largeQP1:  100\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "# print((second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP1_csv1, second_largeQP1_csv2, second_largeQP1_csv3,\n",
    "    second_largeQP1_csv4, second_largeQP1_csv5, second_largeQP1_csv6,\n",
    "    second_largeQP1_csv7, second_largeQP1_csv8, second_largeQP1_csv9,\n",
    "    second_largeQP1_csv10\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1\": [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"],\n",
    "    \"QPD3\": [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"],\n",
    "    \"QPD4\": [\"_1stQP20_2ndQP16_\"],\n",
    "    \"QPD5\": [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"],\n",
    "    \"QPD6\": [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"],\n",
    "    \"QPD8\": [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"],\n",
    "    \"QPD9\": [\"_1stQP25_2ndQP16_\"],\n",
    "    \"QPD10\": [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"],\n",
    "    \"QPD11\": [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"],\n",
    "    \"QPD12\": [\"_1stQP32_2ndQP20_\"],\n",
    "    \"QPD13\": [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"],\n",
    "    \"QPD14\": [\"_1stQP30_2ndQP16_\"],\n",
    "    \"QPD15\": [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"],\n",
    "    \"QPD16\": [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"],\n",
    "    \"QPD18\": [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"],\n",
    "    \"QPD19\": [\"_1stQP35_2ndQP16_\"],\n",
    "    \"QPD20\": [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"],\n",
    "    \"QPD21\": [\"_1stQP45_2ndQP24_\"],\n",
    "    \"QPD22\": [\"_1stQP32_2ndQP10_\"],\n",
    "    \"QPD23\": [\"_1stQP50_2ndQP27_\"],\n",
    "    \"QPD24\": [\"_1stQP40_2ndQP16_\"],\n",
    "    \"QPD25\": [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"],\n",
    "    \"QPD26\": [\"_1stQP50_2ndQP24_\"],\n",
    "    \"QPD27\": [\"_1stQP32_2ndQP5_\"],\n",
    "    \"QPD29\": [\"_1stQP45_2ndQP16_\"],\n",
    "    \"QPD30\": [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"],\n",
    "    \"QPD34\": [\"_1stQP50_2ndQP16_\"],\n",
    "    \"QPD35\": [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"],\n",
    "    \"QPD40\": [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"],\n",
    "    \"QPD45\": [\"_1stQP50_2ndQP5_\"]\n",
    "}\n",
    "\n",
    "# Priority QPD lists\n",
    "priority_qpd = {\"QPD1\", \"QPD3\", \"QPD4\", \"QPD5\", \"QPD6\", \"QPD8\", \"QPD9\", \"QPD10\"}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Initialize the selected data list\n",
    "selected_data = [[] for _ in range(len(datasets))]\n",
    "\n",
    "# Step 1: Select 6 items from each priority QPD from each dataset\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    selected_counts = defaultdict(int)\n",
    "    \n",
    "    for item in dataset:\n",
    "        item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        priority_check = any(qpd in priority_qpd for qpd in qpd_lists)\n",
    "        \n",
    "        if priority_check:\n",
    "            if any(selected_counts[qpd] < 6 for qpd in qpd_lists if qpd in priority_qpd):\n",
    "                selected_data[dataset_idx].append(item)\n",
    "                for qpd in qpd_lists:\n",
    "                    if qpd in priority_qpd:\n",
    "                        selected_counts[qpd] += 1\n",
    "                        \n",
    "        if all(selected_counts[qpd] >= 6 for qpd in priority_qpd):\n",
    "            break\n",
    "\n",
    "# Step 2: Select exactly 2 items from each non-priority QPD\n",
    "non_priority_qpd = set(QPD.keys()) - priority_qpd\n",
    "\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    current_selected = selected_data[dataset_idx]\n",
    "    non_priority_counts = defaultdict(int)\n",
    "    remaining_qpd_items = {qpd: [] for qpd in non_priority_qpd}\n",
    "    \n",
    "    for item in dataset:\n",
    "        item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        non_priority_check = any(qpd in non_priority_qpd for qpd in qpd_lists)\n",
    "        \n",
    "        if non_priority_check:\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in non_priority_qpd and non_priority_counts[qpd] < 2:\n",
    "                    remaining_qpd_items[qpd].append(item)\n",
    "                    non_priority_counts[qpd] += 1\n",
    "    \n",
    "    for qpd, items in remaining_qpd_items.items():\n",
    "        current_selected.extend(items[:2])\n",
    "    \n",
    "    selected_data[dataset_idx] = current_selected\n",
    "\n",
    "# Step 3: Select 8 more items from non-priority QPD to make 100 items\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    current_selected = selected_data[dataset_idx]\n",
    "    extra_items_needed = 100 - len(current_selected)\n",
    "    \n",
    "    if extra_items_needed > 0:\n",
    "        remaining_qpd_items = {qpd: [] for qpd in non_priority_qpd}\n",
    "        non_priority_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            non_priority_check = any(qpd in non_priority_qpd for qpd in qpd_lists)\n",
    "            \n",
    "            if non_priority_check:\n",
    "                for qpd in qpd_lists:\n",
    "                    if qpd in non_priority_qpd and non_priority_counts[qpd] < 3:\n",
    "                        remaining_qpd_items[qpd].append(item)\n",
    "                        non_priority_counts[qpd] += 1\n",
    "        \n",
    "        extra_items_selected = 0\n",
    "        while extra_items_selected < extra_items_needed:\n",
    "            for qpd, items in remaining_qpd_items.items():\n",
    "                if items and extra_items_selected < extra_items_needed:\n",
    "                    current_selected.append(items.pop(0))\n",
    "                    extra_items_selected += 1\n",
    "    \n",
    "    selected_data[dataset_idx] = current_selected[:100]\n",
    "    \n",
    "# Check the distribution of QPDs\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "second_largeQP1_csv1 = selected_data[0]\n",
    "second_largeQP1_csv2 = selected_data[1]\n",
    "second_largeQP1_csv3 = selected_data[2]\n",
    "second_largeQP1_csv4 = selected_data[3]\n",
    "second_largeQP1_csv5 = selected_data[4]\n",
    "second_largeQP1_csv6 = selected_data[5]\n",
    "second_largeQP1_csv7 = selected_data[6]\n",
    "second_largeQP1_csv8 = selected_data[7]\n",
    "second_largeQP1_csv9 = selected_data[8]\n",
    "second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images largeQP1: ', len(second_largeQP1_csv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "List 10: 100 items\n",
      "\n",
      "QPD Distribution: Counter({'QPD32S': 100, 'QPD10S': 100, 'QPD39S': 100, 'QPD27S': 100, 'QPD16S': 100, 'QPD24S': 100, 'QPD5S': 100, 'QPD42S': 100, 'QPD45S': 100, 'QPD20S': 100})\n",
      "\n",
      "double images sameQP:  100\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_sameQP_csv1, second_sameQP_csv2, second_sameQP_csv3,\n",
    "    second_sameQP_csv4, second_sameQP_csv5, second_sameQP_csv6,\n",
    "    second_sameQP_csv7, second_sameQP_csv8, second_sameQP_csv9,\n",
    "    second_sameQP_csv10\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD5S\": [\"_1stQP5_2ndQP5\"],\n",
    "    \"QPD10S\": [\"_1stQP10_2ndQP10\"],\n",
    "    \"QPD16S\": [\"_1stQP16_2ndQP16\"],\n",
    "    \"QPD20S\": [\"_1stQP20_2ndQP20\"],\n",
    "    \"QPD24S\": [\"_1stQP24_2ndQP24\"],\n",
    "    \"QPD27S\": [\"_1stQP27_2ndQP27\"],\n",
    "    \"QPD32S\": [\"_1stQP32_2ndQP32\"],\n",
    "    \"QPD39S\": [\"_1stQP39_2ndQP39\"],\n",
    "    \"QPD42S\": [\"_1stQP42_2ndQP42\"],\n",
    "    \"QPD45S\": [\"_1stQP45_2ndQP45\"],\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        while len(selected_from_dataset) < 100 and indices:\n",
    "            idx = indices.pop()\n",
    "            item = dataset[idx]\n",
    "            item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            if qpd_lists:\n",
    "                if all(selected_counts[qpd] < 100 / len(QPD) for qpd in qpd_lists):\n",
    "                    selected_from_dataset.append(item)\n",
    "                    for qpd in qpd_lists:\n",
    "                        selected_counts[qpd] += 1\n",
    "        \n",
    "        selected_data.append(selected_from_dataset)\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "\n",
    "# Print the distribution of QPD lists in the selected data (optional)\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_sameQP_csv1 = selected_data[0]\n",
    "second_sameQP_csv2 = selected_data[1]\n",
    "second_sameQP_csv3 = selected_data[2]\n",
    "second_sameQP_csv4 = selected_data[3]\n",
    "second_sameQP_csv5 = selected_data[4]\n",
    "second_sameQP_csv6 = selected_data[5]\n",
    "second_sameQP_csv7 = selected_data[6]\n",
    "second_sameQP_csv8 = selected_data[7]\n",
    "second_sameQP_csv9 = selected_data[8]\n",
    "second_sameQP_csv10 = selected_data[9]\n",
    "print('\\ndouble images sameQP: ', len(second_sameQP_csv1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "List 10: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD7M': 50, 'QPD10M': 50, 'QPD12M': 49, 'QPD17M': 47, 'QPD2M': 46, 'QPD5M': 46, 'QPD22M': 46, 'QPD14M': 44, 'QPD24M': 44, 'QPD9M': 43, 'QPD15M': 43, 'QPD4M': 42, 'QPD1M': 41, 'QPD6M': 41, 'QPD13M': 41, 'QPD19M': 41, 'QPD25M': 41, 'QPD27M': 41, 'QPD29M': 41, 'QPD30M': 41, 'QPD32M': 41, 'QPD35M': 41, 'QPD20M': 40})\n",
      "\n",
      "double images largeQP2:  100\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP2_csv1, second_largeQP2_csv2, second_largeQP2_csv3,\n",
    "    second_largeQP2_csv4, second_largeQP2_csv5, second_largeQP2_csv6,\n",
    "    second_largeQP2_csv7, second_largeQP2_csv8, second_largeQP2_csv9,\n",
    "    second_largeQP2_csv10\n",
    "]\n",
    "\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1M\": [\"_1stQP15_2ndQP16\"],\n",
    "    \"QPD2M\": [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"],\n",
    "    \"QPD4M\": [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"],\n",
    "    \"QPD5M\": [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"],\n",
    "    \"QPD6M\": [\"_1stQP10_2ndQP16\"],\n",
    "    \"QPD7M\": [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"],\n",
    "    \"QPD9M\": [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"],\n",
    "    \"QPD10M\": [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"],\n",
    "    \"QPD12M\": [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"],\n",
    "    \"QPD13M\": [\"_1stQP32_2ndQP45\"],\n",
    "    \"QPD14M\": [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"],\n",
    "    \"QPD15M\": [\"_1stQP30_2ndQP45\"],\n",
    "    \"QPD17M\": [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"],\n",
    "    \"QPD19M\": [\"_1stQP20_2ndQP39\"],\n",
    "    \"QPD20M\": [\"_1stQP25_2ndQP45\"],\n",
    "    \"QPD22M\": [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"],\n",
    "    \"QPD24M\": [\"_1stQP15_2ndQP39\"],\n",
    "    \"QPD25M\": [\"_1stQP20_2ndQP45\"],\n",
    "    \"QPD27M\": [\"_1stQP15_2ndQP42\"],\n",
    "    \"QPD29M\": [\"_1stQP10_2ndQP39\"],\n",
    "    \"QPD30M\": [\"_1stQP15_2ndQP45\"],\n",
    "    \"QPD32M\": [\"_1stQP10_2ndQP42\"],\n",
    "    \"QPD35M\": [\"_1stQP10_2ndQP45\"]\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 4 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 4:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 4:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_largeQP2_csv1 = selected_data[0]\n",
    "second_largeQP2_csv2 = selected_data[1]\n",
    "second_largeQP2_csv3 = selected_data[2]\n",
    "second_largeQP2_csv4 = selected_data[3]\n",
    "second_largeQP2_csv5 = selected_data[4]\n",
    "second_largeQP2_csv6 = selected_data[5]\n",
    "second_largeQP2_csv7 = selected_data[6]\n",
    "second_largeQP2_csv8 = selected_data[7]\n",
    "second_largeQP2_csv9 = selected_data[8]\n",
    "second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images largeQP2: ', len(second_largeQP2_csv1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv9 + second_largeQP2_csv10\n",
    "print(\"train_csv_list: \", len(train_csv_list10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10      0   8896   6976  12056  32072      0   8896   6960  12020  32124   9044  10963  2307   3407   1861    614   1067    586   8862  11741  2326   3547   1837    612    954    604  13660  14396   9440   4740   2092  15672  13452  15232   9828   4792   1884  14812  0.000002  0.001049   0.001143  0.128237  0.081754\n",
      "1         16      0  19968   8240   7228  24564      0  20032   8288   6668  25012   7692   9358  2700   2257   2474    665    700    598   7350  10197  2426   2480   2655    680    714    610  15028  16092   5612   2736   1180  19352  13916  17724   5804   2704    932  18920  0.000453  0.002534   0.002658  0.054909  0.033101\n",
      "2         20      0  20864  10592   9952  18592      0  20800  10592   9552  19056   9586   8165  1945   2187   2036    420    827    408   9319   8813  2082   2423   2175    423    745    422  15936  10620   5928   3472   1420  22624  16460  10848   5800   3500   1280  22112  0.000234  0.001592   0.000427  0.016134  0.012266\n",
      "3         24      0  23488   9088  11204  16220      0  22976   9744  10652  16628  10178   7726  2143   2156   1786    348   1202    361  10357   7929  2269   2259   1681    409   1200    414  14772   6144   5680   1960   1088  30356  14836   6240   5840   1992    920  30172  0.000789  0.000556   0.000306  0.012341  0.009129\n",
      "4         27      0  26304   8016  11448  14232      0  26304   8208  10972  14516  10686   6400  2278   2426   1651    810   1049    409  10911   7052  2046   2571   1614    585   1165    491  13228   5340   3428   1740   1188  35076  13364   5604   3520   1504   1136  34872  0.000254  0.003593    0.00046  0.166804  0.129979\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "595       42      0  24640  24272   9340   1748      0  26368  23552   8564   1516  20383  12657  1173    986    292   1017   7973    654  20883  13309  1112   1092    338   1204   8384    591   2548    892   1176   1492   1300  52592   1972    960    884    976    820  54388  0.001997  0.000602   0.006483   0.12682  0.158835\n",
      "596       20      0   3648   7152  21708  27492      0   3584   7168  21204  28044  11134  10945  1114   1726    791   1493   1930   1537  11109  11476  1132   1692    668   1447   1977   1518  16388  11084   5476   4536   1860  20656  16392  11496   5292   4288   1688  20844    0.0002  0.000753    0.00045  0.005418  0.004763\n",
      "597       24      0  21440   7328  10108  21124      0  22016   6880  10004  21100   8863   4569  4186   4707    990    942   5540   1393   7610   5400  4209   4816    783   1151   6699   1364   7992   4824   4812   3940   2084  36348   8160   4852   4948   4072   2060  35908  0.000374  0.009867   0.000145  0.239448  0.011394\n",
      "598       42      0  46592  11488   1820    100      0  46848  11872   1216     64  16538   9640  2283   7061   4898   3185   8006    686  17338   9538  2400   5616   5258   2169   8936    128    244     64    272      0    128  59292     52     64    192    256      0  59436  0.002422   0.02002   0.015787  0.335867  0.285514\n",
      "599       45      0  21824  20224  15168   2784      0  22912  21088  13808   2192  18645  12048   740    447    271    413   4180    162  19375  12045   696    590    197    513   5091    141   1336    572    352    340    516  56884    840    380    304    248    112  58116  0.003043  0.002857   0.009662  0.100474  0.120006\n",
      "\n",
      "[600 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP,\n",
    "                            X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6919    0.9433    0.7983       300\n",
      "           1     0.9110    0.5800    0.7088       300\n",
      "\n",
      "    accuracy                         0.7617       600\n",
      "   macro avg     0.8015    0.7617    0.7535       600\n",
      "weighted avg     0.8015    0.7617    0.7535       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6433    0.9800    0.7768       300\n",
      "           1     0.9580    0.4567    0.6185       300\n",
      "\n",
      "    accuracy                         0.7183       600\n",
      "   macro avg     0.8007    0.7183    0.6976       600\n",
      "weighted avg     0.8007    0.7183    0.6976       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6787    0.9367    0.7871       300\n",
      "           1     0.8978    0.5567    0.6872       300\n",
      "\n",
      "    accuracy                         0.7467       600\n",
      "   macro avg     0.7883    0.7467    0.7372       600\n",
      "weighted avg     0.7883    0.7467    0.7372       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6208    0.9767    0.7591       300\n",
      "           1     0.9453    0.4033    0.5654       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.7830    0.6900    0.6622       600\n",
      "weighted avg     0.7830    0.6900    0.6622       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5506    0.9800    0.7050       300\n",
      "           1     0.9091    0.2000    0.3279       300\n",
      "\n",
      "    accuracy                         0.5900       600\n",
      "   macro avg     0.7298    0.5900    0.5165       600\n",
      "weighted avg     0.7298    0.5900    0.5165       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7825    0.8033    0.7928       300\n",
      "           1     0.7979    0.7767    0.7872       300\n",
      "\n",
      "    accuracy                         0.7900       600\n",
      "   macro avg     0.7902    0.7900    0.7900       600\n",
      "weighted avg     0.7902    0.7900    0.7900       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7252    0.8533    0.7841       300\n",
      "           1     0.8219    0.6767    0.7422       300\n",
      "\n",
      "    accuracy                         0.7650       600\n",
      "   macro avg     0.7735    0.7650    0.7632       600\n",
      "weighted avg     0.7735    0.7650    0.7632       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7695    0.7900    0.7796       300\n",
      "           1     0.7842    0.7633    0.7736       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7769    0.7767    0.7766       600\n",
      "weighted avg     0.7769    0.7767    0.7766       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7151    0.8367    0.7711       300\n",
      "           1     0.8032    0.6667    0.7286       300\n",
      "\n",
      "    accuracy                         0.7517       600\n",
      "   macro avg     0.7592    0.7517    0.7499       600\n",
      "weighted avg     0.7592    0.7517    0.7499       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5526    0.8933    0.6828       300\n",
      "           1     0.7217    0.2767    0.4000       300\n",
      "\n",
      "    accuracy                         0.5850       600\n",
      "   macro avg     0.6372    0.5850    0.5414       600\n",
      "weighted avg     0.6372    0.5850    0.5414       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8259    0.6167    0.7061       300\n",
      "           1     0.6941    0.8700    0.7722       300\n",
      "\n",
      "    accuracy                         0.7433       600\n",
      "   macro avg     0.7600    0.7433    0.7391       600\n",
      "weighted avg     0.7600    0.7433    0.7391       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8586    0.5667    0.6827       300\n",
      "           1     0.6766    0.9067    0.7749       300\n",
      "\n",
      "    accuracy                         0.7367       600\n",
      "   macro avg     0.7676    0.7367    0.7288       600\n",
      "weighted avg     0.7676    0.7367    0.7288       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8203    0.5933    0.6886       300\n",
      "           1     0.6815    0.8700    0.7643       300\n",
      "\n",
      "    accuracy                         0.7317       600\n",
      "   macro avg     0.7509    0.7317    0.7264       600\n",
      "weighted avg     0.7509    0.7317    0.7264       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8708    0.5167    0.6485       300\n",
      "           1     0.6564    0.9233    0.7673       300\n",
      "\n",
      "    accuracy                         0.7200       600\n",
      "   macro avg     0.7636    0.7200    0.7079       600\n",
      "weighted avg     0.7636    0.7200    0.7079       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5491    0.9500    0.6960       300\n",
      "           1     0.8148    0.2200    0.3465       300\n",
      "\n",
      "    accuracy                         0.5850       600\n",
      "   macro avg     0.6820    0.5850    0.5212       600\n",
      "weighted avg     0.6820    0.5850    0.5212       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8048    0.6733    0.7332       300\n",
      "           1     0.7192    0.8367    0.7735       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.7620    0.7550    0.7534       600\n",
      "weighted avg     0.7620    0.7550    0.7534       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8274    0.6233    0.7110       300\n",
      "           1     0.6979    0.8700    0.7745       300\n",
      "\n",
      "    accuracy                         0.7467       600\n",
      "   macro avg     0.7626    0.7467    0.7428       600\n",
      "weighted avg     0.7626    0.7467    0.7428       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7893    0.6867    0.7344       300\n",
      "           1     0.7227    0.8167    0.7668       300\n",
      "\n",
      "    accuracy                         0.7517       600\n",
      "   macro avg     0.7560    0.7517    0.7506       600\n",
      "weighted avg     0.7560    0.7517    0.7506       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8008    0.6433    0.7135       300\n",
      "           1     0.7019    0.8400    0.7648       300\n",
      "\n",
      "    accuracy                         0.7417       600\n",
      "   macro avg     0.7514    0.7417    0.7391       600\n",
      "weighted avg     0.7514    0.7417    0.7391       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5541    0.9567    0.7017       300\n",
      "           1     0.8415    0.2300    0.3613       300\n",
      "\n",
      "    accuracy                         0.5933       600\n",
      "   macro avg     0.6978    0.5933    0.5315       600\n",
      "weighted avg     0.6978    0.5933    0.5315       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8062    0.7767    0.7912       300\n",
      "           1     0.7846    0.8133    0.7987       300\n",
      "\n",
      "    accuracy                         0.7950       600\n",
      "   macro avg     0.7954    0.7950    0.7949       600\n",
      "weighted avg     0.7954    0.7950    0.7949       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.7000    0.7706       300\n",
      "           1     0.7465    0.8833    0.8092       300\n",
      "\n",
      "    accuracy                         0.7917       600\n",
      "   macro avg     0.8018    0.7917    0.7899       600\n",
      "weighted avg     0.8018    0.7917    0.7899       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8160    0.6800    0.7418       300\n",
      "           1     0.7257    0.8467    0.7815       300\n",
      "\n",
      "    accuracy                         0.7633       600\n",
      "   macro avg     0.7709    0.7633    0.7617       600\n",
      "weighted avg     0.7709    0.7633    0.7617       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8548    0.6867    0.7616       300\n",
      "           1     0.7382    0.8833    0.8042       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7965    0.7850    0.7829       600\n",
      "weighted avg     0.7965    0.7850    0.7829       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5575    0.9700    0.7080       300\n",
      "           1     0.8846    0.2300    0.3651       300\n",
      "\n",
      "    accuracy                         0.6000       600\n",
      "   macro avg     0.7210    0.6000    0.5366       600\n",
      "weighted avg     0.7210    0.6000    0.5366       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7348    0.8867    0.8036       300\n",
      "           1     0.8571    0.6800    0.7584       300\n",
      "\n",
      "    accuracy                         0.7833       600\n",
      "   macro avg     0.7960    0.7833    0.7810       600\n",
      "weighted avg     0.7960    0.7833    0.7810       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6565    0.9300    0.7697       300\n",
      "           1     0.8800    0.5133    0.6484       300\n",
      "\n",
      "    accuracy                         0.7217       600\n",
      "   macro avg     0.7682    0.7217    0.7090       600\n",
      "weighted avg     0.7682    0.7217    0.7090       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6891    0.8867    0.7755       300\n",
      "           1     0.8411    0.6000    0.7004       300\n",
      "\n",
      "    accuracy                         0.7433       600\n",
      "   macro avg     0.7651    0.7433    0.7379       600\n",
      "weighted avg     0.7651    0.7433    0.7379       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6343    0.9367    0.7564       300\n",
      "           1     0.8790    0.4600    0.6039       300\n",
      "\n",
      "    accuracy                         0.6983       600\n",
      "   macro avg     0.7566    0.6983    0.6802       600\n",
      "weighted avg     0.7566    0.6983    0.6802       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5515    0.9467    0.6969       300\n",
      "           1     0.8118    0.2300    0.3584       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6816    0.5883    0.5277       600\n",
      "weighted avg     0.6816    0.5883    0.5277       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8651    0.7267    0.7899       300\n",
      "           1     0.7644    0.8867    0.8210       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8147    0.8067    0.8054       600\n",
      "weighted avg     0.8147    0.8067    0.8054       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7394    0.7567    0.7479       300\n",
      "           1     0.7509    0.7333    0.7420       300\n",
      "\n",
      "    accuracy                         0.7450       600\n",
      "   macro avg     0.7451    0.7450    0.7450       600\n",
      "weighted avg     0.7451    0.7450    0.7450       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8145    0.7467    0.7791       300\n",
      "           1     0.7662    0.8300    0.7968       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7903    0.7883    0.7880       600\n",
      "weighted avg     0.7903    0.7883    0.7880       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.8167    0.7621       300\n",
      "           1     0.7860    0.6733    0.7253       300\n",
      "\n",
      "    accuracy                         0.7450       600\n",
      "   macro avg     0.7501    0.7450    0.7437       600\n",
      "weighted avg     0.7501    0.7450    0.7437       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5549    0.9267    0.6941       300\n",
      "           1     0.7778    0.2567    0.3860       300\n",
      "\n",
      "    accuracy                         0.5917       600\n",
      "   macro avg     0.6663    0.5917    0.5400       600\n",
      "weighted avg     0.6663    0.5917    0.5400       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8033    0.6400    0.7124       300\n",
      "           1     0.7008    0.8433    0.7655       300\n",
      "\n",
      "    accuracy                         0.7417       600\n",
      "   macro avg     0.7521    0.7417    0.7390       600\n",
      "weighted avg     0.7521    0.7417    0.7390       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8194    0.5900    0.6860       300\n",
      "           1     0.6797    0.8700    0.7632       300\n",
      "\n",
      "    accuracy                         0.7300       600\n",
      "   macro avg     0.7496    0.7300    0.7246       600\n",
      "weighted avg     0.7496    0.7300    0.7246       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7640    0.6367    0.6945       300\n",
      "           1     0.6886    0.8033    0.7415       300\n",
      "\n",
      "    accuracy                         0.7200       600\n",
      "   macro avg     0.7263    0.7200    0.7180       600\n",
      "weighted avg     0.7263    0.7200    0.7180       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7695    0.6567    0.7086       300\n",
      "           1     0.7006    0.8033    0.7484       300\n",
      "\n",
      "    accuracy                         0.7300       600\n",
      "   macro avg     0.7351    0.7300    0.7285       600\n",
      "weighted avg     0.7351    0.7300    0.7285       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5528    0.9600    0.7016       300\n",
      "           1     0.8481    0.2233    0.3536       300\n",
      "\n",
      "    accuracy                         0.5917       600\n",
      "   macro avg     0.7004    0.5917    0.5276       600\n",
      "weighted avg     0.7004    0.5917    0.5276       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8167    0.8167    0.8167       300\n",
      "           1     0.8167    0.8167    0.8167       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8167    0.8167    0.8167       600\n",
      "weighted avg     0.8167    0.8167    0.8167       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7909    0.5800    0.6692       300\n",
      "           1     0.6684    0.8467    0.7471       300\n",
      "\n",
      "    accuracy                         0.7133       600\n",
      "   macro avg     0.7297    0.7133    0.7081       600\n",
      "weighted avg     0.7297    0.7133    0.7081       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8048    0.7833    0.7939       300\n",
      "           1     0.7890    0.8100    0.7993       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.7969    0.7967    0.7966       600\n",
      "weighted avg     0.7969    0.7967    0.7966       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7882    0.5333    0.6362       300\n",
      "           1     0.6474    0.8567    0.7374       300\n",
      "\n",
      "    accuracy                         0.6950       600\n",
      "   macro avg     0.7178    0.6950    0.6868       600\n",
      "weighted avg     0.7178    0.6950    0.6868       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5543    0.9533    0.7010       300\n",
      "           1     0.8333    0.2333    0.3646       300\n",
      "\n",
      "    accuracy                         0.5933       600\n",
      "   macro avg     0.6938    0.5933    0.5328       600\n",
      "weighted avg     0.6938    0.5933    0.5328       600\n",
      "\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6949    0.8200    0.7523       300\n",
      "           1     0.7805    0.6400    0.7033       300\n",
      "\n",
      "    accuracy                         0.7300       600\n",
      "   macro avg     0.7377    0.7300    0.7278       600\n",
      "weighted avg     0.7377    0.7300    0.7278       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6496    0.8900    0.7511       300\n",
      "           1     0.8254    0.5200    0.6380       300\n",
      "\n",
      "    accuracy                         0.7050       600\n",
      "   macro avg     0.7375    0.7050    0.6945       600\n",
      "weighted avg     0.7375    0.7050    0.6945       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7033    0.7900    0.7441       300\n",
      "           1     0.7605    0.6667    0.7105       300\n",
      "\n",
      "    accuracy                         0.7283       600\n",
      "   macro avg     0.7319    0.7283    0.7273       600\n",
      "weighted avg     0.7319    0.7283    0.7273       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6580    0.8467    0.7405       300\n",
      "           1     0.7850    0.5600    0.6537       300\n",
      "\n",
      "    accuracy                         0.7033       600\n",
      "   macro avg     0.7215    0.7033    0.6971       600\n",
      "weighted avg     0.7215    0.7033    0.6971       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5551    0.9233    0.6934       300\n",
      "           1     0.7723    0.2600    0.3890       300\n",
      "\n",
      "    accuracy                         0.5917       600\n",
      "   macro avg     0.6637    0.5917    0.5412       600\n",
      "weighted avg     0.6637    0.5917    0.5412       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500]}\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Score_RBF', 'tnr_rbf', 'tpr_rbf',\n",
    "                                'C_LINEAR', 'Score_LINEAR', 'tnr_linear', 'tpr_linear',\n",
    "                                'C_OG_RBF', 'Score_OG_RBF', 'tnr_og_rbf', 'tpr_og_rbf',\n",
    "                                'C_OG_LINEAR', 'Score_OG_LINEAR', 'tnr_og_linear', 'tpr_og_linear',\n",
    "                                'Threshold', 'Score_old', 'tnr_old', 'tpr_old'])\n",
    "    \n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "        \n",
    "    test_data = [X_train_list[i] for i in test_ids]\n",
    "    test_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    test_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    \n",
    "    train_data = [item for data in train_data for item in data]\n",
    "    train_data_OG = [item for data in train_data_OG for item in data]\n",
    "    train_label = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(train_data, train_label, test_size=600, random_state=42)\n",
    "    X_train_OG, X_val_OG, _, _ = train_test_split(train_data_OG, train_label, test_size=600, random_state=42)\n",
    "    \n",
    "    test_data = [item for data in test_data for item in data]\n",
    "    test_data_OG = [item for data in test_data_OG for item in data]\n",
    "    test_label = [item for data in test_label for item in data]\n",
    "\n",
    "    MAE_data = [MAE_list[i] for i in test_ids]\n",
    "    MAE_data = [item for data in MAE_data for item in data]\n",
    "    \n",
    "    FINAL_QP_data = [FINAL_QP_list[i] for i in test_ids]\n",
    "    FINAL_QP_data = [item for data in FINAL_QP_data for item in data]\n",
    "                                                                                                 \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "                \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        results_old = np.array([is_double_compressed(MAE_data[i], FINAL_QP_data[i], threshold) for i in range(600)])\n",
    "        predicted_labels = results_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(test_data)\n",
    "    # test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(test_label, test_predictions_RBF)\n",
    "    report_RBF = classification_report(test_label, test_predictions_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_RBF)\n",
    "    tnr_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data)\n",
    "    # test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(test_label, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label, test_predictions_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_LINEAR)\n",
    "    tnr_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(test_data_OG)\n",
    "    # test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(test_label, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(test_label, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_RBF)\n",
    "    tnr_og_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(test_data_OG)\n",
    "    # test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(test_label, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    tnr_og_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Score_RBF': test_accuracy_RBF, 'tnr_rbf': tnr_rbf, 'tpr_rbf': tpr_rbf,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Score_LINEAR': test_accuracy_LINEAR, 'tnr_linear': tnr_linear, 'tpr_linear': tpr_linear,\n",
    "              'C_OG_RBF': best_c_value_onlyGhost_RBF, 'Score_OG_RBF': test_accuracy_onlyGhost_RBF, 'tnr_og_rbf': tnr_og_rbf, 'tpr_og_rbf': tpr_og_rbf,\n",
    "              'C_OG_LINEAR': best_c_value_onlyGhost_LINEAR, 'Score_OG_LINEAR': test_accuracy_onlyGhost_LINEAR, 'tnr_og_linear': tnr_og_linear, 'tpr_og_linear': tpr_og_linear,\n",
    "              'Threshold': best_threshold, 'Score_old': best_accuracy, 'tnr_old': tnr_old, 'tpr_old': tpr_old,}\n",
    "\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0                    RBF        77.03        77.43               77.23                3.00           81.67            73.0\n",
      "1                 LINEAR        74.70        72.77               73.73                2.61           79.17            70.5\n",
      "2     Only Ghost and RBF        75.30        75.63               75.47                2.60           79.67            72.0\n",
      "3  Only Ghost and LINEAR        74.50        70.70               72.60                3.04           78.50            69.0\n",
      "4                    OLD        94.60        23.60               59.10                0.44           60.00            58.5\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF', 'LINEAR', 'Only Ghost and RBF', 'Only Ghost and LINEAR', 'OLD'],\n",
    "    'Average TNR': [round(results['tnr_rbf'].mean() * 100, 2), round(results['tnr_linear'].mean() * 100, 2), round(results['tnr_og_rbf'].mean() * 100, 2), round(results['tnr_og_linear'].mean() * 100, 2), round(results['tnr_old'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf'].mean() * 100, 2), round(results['tpr_linear'].mean() * 100, 2), round(results['tpr_og_rbf'].mean() * 100, 2), round(results['tpr_og_linear'].mean() * 100, 2), round(results['tpr_old'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF'].mean() * 100, 2), round(results['Score_LINEAR'].mean() * 100, 2), round(results['Score_OG_RBF'].mean() * 100, 2), round(results['Score_OG_LINEAR'].mean() * 100, 2), round(results['Score_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF'].std() * 100, 2), round(results['Score_LINEAR'].std() * 100, 2), round(results['Score_OG_RBF'].std() * 100, 2), round(results['Score_OG_LINEAR'].std() * 100, 2), round(results['Score_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF'].max() * 100, 2), round(results['Score_LINEAR'].max() * 100, 2), round(results['Score_OG_RBF'].max() * 100, 2), round(results['Score_OG_LINEAR'].max() * 100, 2), round(results['Score_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF'].min() * 100, 2), round(results['Score_LINEAR'].min() * 100, 2), round(results['Score_OG_RBF'].min() * 100, 2), round(results['Score_OG_LINEAR'].min() * 100, 2), round(results['Score_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: C_RBF, dtype: object\n",
      "0    1000\n",
      "1    1000\n",
      "2     100\n",
      "3     100\n",
      "4    5000\n",
      "5    1000\n",
      "6    4000\n",
      "7    1000\n",
      "8     100\n",
      "9    4000\n",
      "Name: C_LINEAR, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF'])\n",
    "print(results['C_LINEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
