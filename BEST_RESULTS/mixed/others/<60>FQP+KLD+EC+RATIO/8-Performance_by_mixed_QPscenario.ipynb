{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['168', '171', '254', '181', '203', '86', '60', '176', '259', '196', '270', '85', '3', '35', '278', '295', '55', '163', '102', '113', '84', '105', '263', '129', '42', '247', '10', '108', '279', '159'], ['27', '83', '205', '224', '106', '97', '44', '269', '132', '191', '157', '198', '155', '109', '67', '69', '115', '7', '199', '189', '220', '63', '59', '250', '41', '294', '212', '124', '13', '149'], ['232', '20', '213', '65', '274', '78', '39', '36', '116', '282', '267', '236', '264', '230', '215', '221', '284', '139', '166', '8', '258', '30', '260', '178', '70', '152', '187', '238', '131', '158'], ['19', '32', '135', '17', '222', '104', '265', '194', '291', '177', '228', '201', '34', '206', '172', '21', '202', '99', '170', '173', '148', '50', '103', '128', '118', '268', '241', '87', '77', '223'], ['58', '244', '15', '218', '227', '71', '79', '164', '231', '125', '23', '89', '167', '200', '193', '161', '138', '160', '22', '237', '117', '12', '114', '186', '288', '121', '154', '110', '180', '145'], ['174', '122', '246', '204', '175', '74', '235', '150', '130', '197', '240', '96', '126', '287', '253', '88', '11', '68', '25', '66', '91', '169', '153', '256', '272', '162', '289', '24', '296', '81'], ['229', '136', '52', '49', '64', '14', '137', '2', '285', '57', '4', '112', '216', '214', '242', '134', '47', '146', '233', '38', '141', '72', '179', '9', '243', '252', '43', '280', '5', '48'], ['73', '185', '276', '226', '95', '225', '31', '293', '111', '90', '1', '46', '45', '183', '283', '80', '101', '217', '245', '290', '277', '271', '255', '209', '107', '188', '92', '56', '165', '190'], ['156', '151', '249', '142', '75', '54', '18', '16', '147', '40', '195', '208', '6', '37', '120', '211', '53', '26', '210', '251', '123', '94', '298', '219', '182', '273', '93', '143', '144', '239'], ['100', '76', '281', '184', '119', '192', '51', '292', '29', '297', '61', '266', '248', '140', '300', '207', '127', '234', '257', '275', '299', '62', '28', '262', '286', '33', '98', '261', '133', '82']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sameQP = 1\n",
    "QPD0 = [\"_1stQP5_2ndQP5\", \"_1stQP10_2ndQP10\", \"_1stQP16_2ndQP16\", \"_1stQP20_2ndQP20\", \"_1stQP24_2ndQP24\", \n",
    "        \"_1stQP27_2ndQP27\", \"_1stQP32_2ndQP32\", \"_1stQP39_2ndQP39\", \"_1stQP42_2ndQP42\", \"_1stQP45_2ndQP45\"]\n",
    "\n",
    "# largeQP1 = 30\n",
    "QPD1 = [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"]\n",
    "QPD3 = [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"]\n",
    "QPD4 = [\"_1stQP20_2ndQP16_\"]\n",
    "QPD5 = [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"]\n",
    "QPD6 = [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"]\n",
    "QPD8 = [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"]\n",
    "QPD9 = [\"_1stQP25_2ndQP16_\"]\n",
    "QPD10 = [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"]\n",
    "QPD11 = [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"]\n",
    "QPD12 = [\"_1stQP32_2ndQP20_\"]\n",
    "QPD13 = [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"]\n",
    "QPD14 = [\"_1stQP30_2ndQP16_\"]\n",
    "QPD15 = [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"]\n",
    "QPD16 = [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"]\n",
    "QPD18 = [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"]\n",
    "QPD19 = [\"_1stQP35_2ndQP16_\"]\n",
    "QPD20 = [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"]\n",
    "QPD21 = [\"_1stQP45_2ndQP24_\"]\n",
    "QPD22 = [\"_1stQP32_2ndQP10_\"]\n",
    "QPD23 = [\"_1stQP50_2ndQP27_\"]\n",
    "QPD24 = [\"_1stQP40_2ndQP16_\"]\n",
    "QPD25 = [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"]\n",
    "QPD26 = [\"_1stQP50_2ndQP24_\"]\n",
    "QPD27 = [\"_1stQP32_2ndQP5_\"]\n",
    "QPD29 = [\"_1stQP45_2ndQP16_\"]\n",
    "QPD30 = [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"]\n",
    "QPD34 = [\"_1stQP50_2ndQP16_\"]\n",
    "QPD35 = [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"]\n",
    "QPD40 = [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"]\n",
    "QPD45 = [\"_1stQP50_2ndQP5_\"]\n",
    "\n",
    "# largeQP2 = 23\n",
    "QPD1M = [\"_1stQP15_2ndQP16\"]\n",
    "QPD2M = [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"]\n",
    "QPD4M = [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"]\n",
    "QPD5M = [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"]\n",
    "QPD6M = [\"_1stQP10_2ndQP16\"]\n",
    "QPD7M = [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"]\n",
    "QPD9M = [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"]\n",
    "QPD10M = [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"]\n",
    "QPD12M = [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"]\n",
    "QPD13M = [\"_1stQP32_2ndQP45\"]\n",
    "QPD14M = [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"]\n",
    "QPD15M = [\"_1stQP30_2ndQP45\"]\n",
    "QPD17M = [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"]\n",
    "QPD19M = [\"_1stQP20_2ndQP39\"]\n",
    "QPD20M = [\"_1stQP25_2ndQP45\"]\n",
    "QPD22M = [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"]\n",
    "QPD24M = [\"_1stQP15_2ndQP39\"]\n",
    "QPD25M = [\"_1stQP20_2ndQP45\"]\n",
    "QPD27M = [\"_1stQP15_2ndQP42\"]\n",
    "QPD29M = [\"_1stQP10_2ndQP39\"]\n",
    "QPD30M = [\"_1stQP15_2ndQP45\"]\n",
    "QPD32M = [\"_1stQP10_2ndQP42\"]\n",
    "QPD35M = [\"_1stQP10_2ndQP45\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single images:  300\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print('single images: ', len(single_csv9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD5': 60, 'QPD6': 60, 'QPD1': 60, 'QPD3': 60, 'QPD8': 60, 'QPD10': 60, 'QPD9': 60, 'QPD4': 60, 'QPD35': 30, 'QPD20': 30, 'QPD15': 30, 'QPD25': 30, 'QPD18': 30, 'QPD30': 30, 'QPD40': 30, 'QPD23': 30, 'QPD45': 20, 'QPD11': 20, 'QPD34': 20, 'QPD21': 20, 'QPD29': 20, 'QPD24': 20, 'QPD14': 20, 'QPD19': 20, 'QPD12': 20, 'QPD16': 20, 'QPD22': 20, 'QPD27': 20, 'QPD13': 20, 'QPD26': 20})\n",
      "\n",
      "double images largeQP1:  100\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "# print((second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP1_csv1, second_largeQP1_csv2, second_largeQP1_csv3,\n",
    "    second_largeQP1_csv4, second_largeQP1_csv5, second_largeQP1_csv6,\n",
    "    second_largeQP1_csv7, second_largeQP1_csv8, second_largeQP1_csv9,\n",
    "    second_largeQP1_csv10\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1\": [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"],\n",
    "    \"QPD3\": [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"],\n",
    "    \"QPD4\": [\"_1stQP20_2ndQP16_\"],\n",
    "    \"QPD5\": [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"],\n",
    "    \"QPD6\": [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"],\n",
    "    \"QPD8\": [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"],\n",
    "    \"QPD9\": [\"_1stQP25_2ndQP16_\"],\n",
    "    \"QPD10\": [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"],\n",
    "    \"QPD11\": [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"],\n",
    "    \"QPD12\": [\"_1stQP32_2ndQP20_\"],\n",
    "    \"QPD13\": [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"],\n",
    "    \"QPD14\": [\"_1stQP30_2ndQP16_\"],\n",
    "    \"QPD15\": [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"],\n",
    "    \"QPD16\": [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"],\n",
    "    \"QPD18\": [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"],\n",
    "    \"QPD19\": [\"_1stQP35_2ndQP16_\"],\n",
    "    \"QPD20\": [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"],\n",
    "    \"QPD21\": [\"_1stQP45_2ndQP24_\"],\n",
    "    \"QPD22\": [\"_1stQP32_2ndQP10_\"],\n",
    "    \"QPD23\": [\"_1stQP50_2ndQP27_\"],\n",
    "    \"QPD24\": [\"_1stQP40_2ndQP16_\"],\n",
    "    \"QPD25\": [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"],\n",
    "    \"QPD26\": [\"_1stQP50_2ndQP24_\"],\n",
    "    \"QPD27\": [\"_1stQP32_2ndQP5_\"],\n",
    "    \"QPD29\": [\"_1stQP45_2ndQP16_\"],\n",
    "    \"QPD30\": [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"],\n",
    "    \"QPD34\": [\"_1stQP50_2ndQP16_\"],\n",
    "    \"QPD35\": [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"],\n",
    "    \"QPD40\": [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"],\n",
    "    \"QPD45\": [\"_1stQP50_2ndQP5_\"]\n",
    "}\n",
    "\n",
    "# Priority QPD lists\n",
    "priority_qpd = {\"QPD1\", \"QPD3\", \"QPD4\", \"QPD5\", \"QPD6\", \"QPD8\", \"QPD9\", \"QPD10\"}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Initialize the selected data list\n",
    "selected_data = [[] for _ in range(len(datasets))]\n",
    "\n",
    "# Step 1: Select 6 items from each priority QPD from each dataset\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    selected_counts = defaultdict(int)\n",
    "    \n",
    "    for item in dataset:\n",
    "        item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        priority_check = any(qpd in priority_qpd for qpd in qpd_lists)\n",
    "        \n",
    "        if priority_check:\n",
    "            if any(selected_counts[qpd] < 6 for qpd in qpd_lists if qpd in priority_qpd):\n",
    "                selected_data[dataset_idx].append(item)\n",
    "                for qpd in qpd_lists:\n",
    "                    if qpd in priority_qpd:\n",
    "                        selected_counts[qpd] += 1\n",
    "                        \n",
    "        if all(selected_counts[qpd] >= 6 for qpd in priority_qpd):\n",
    "            break\n",
    "\n",
    "# Step 2: Select exactly 2 items from each non-priority QPD\n",
    "non_priority_qpd = set(QPD.keys()) - priority_qpd\n",
    "\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    current_selected = selected_data[dataset_idx]\n",
    "    non_priority_counts = defaultdict(int)\n",
    "    remaining_qpd_items = {qpd: [] for qpd in non_priority_qpd}\n",
    "    \n",
    "    for item in dataset:\n",
    "        item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        non_priority_check = any(qpd in non_priority_qpd for qpd in qpd_lists)\n",
    "        \n",
    "        if non_priority_check:\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in non_priority_qpd and non_priority_counts[qpd] < 2:\n",
    "                    remaining_qpd_items[qpd].append(item)\n",
    "                    non_priority_counts[qpd] += 1\n",
    "    \n",
    "    for qpd, items in remaining_qpd_items.items():\n",
    "        current_selected.extend(items[:2])\n",
    "    \n",
    "    selected_data[dataset_idx] = current_selected\n",
    "\n",
    "# Step 3: Select 8 more items from non-priority QPD to make 100 items\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    current_selected = selected_data[dataset_idx]\n",
    "    extra_items_needed = 100 - len(current_selected)\n",
    "    \n",
    "    if extra_items_needed > 0:\n",
    "        remaining_qpd_items = {qpd: [] for qpd in non_priority_qpd}\n",
    "        non_priority_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            non_priority_check = any(qpd in non_priority_qpd for qpd in qpd_lists)\n",
    "            \n",
    "            if non_priority_check:\n",
    "                for qpd in qpd_lists:\n",
    "                    if qpd in non_priority_qpd and non_priority_counts[qpd] < 3:\n",
    "                        remaining_qpd_items[qpd].append(item)\n",
    "                        non_priority_counts[qpd] += 1\n",
    "        \n",
    "        extra_items_selected = 0\n",
    "        while extra_items_selected < extra_items_needed:\n",
    "            for qpd, items in remaining_qpd_items.items():\n",
    "                if items and extra_items_selected < extra_items_needed:\n",
    "                    current_selected.append(items.pop(0))\n",
    "                    extra_items_selected += 1\n",
    "    \n",
    "    selected_data[dataset_idx] = current_selected[:100]\n",
    "    \n",
    "# Check the distribution of QPDs\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "second_largeQP1_csv1 = selected_data[0]\n",
    "second_largeQP1_csv2 = selected_data[1]\n",
    "second_largeQP1_csv3 = selected_data[2]\n",
    "second_largeQP1_csv4 = selected_data[3]\n",
    "second_largeQP1_csv5 = selected_data[4]\n",
    "second_largeQP1_csv6 = selected_data[5]\n",
    "second_largeQP1_csv7 = selected_data[6]\n",
    "second_largeQP1_csv8 = selected_data[7]\n",
    "second_largeQP1_csv9 = selected_data[8]\n",
    "second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images largeQP1: ', len(second_largeQP1_csv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "List 10: 100 items\n",
      "\n",
      "QPD Distribution: Counter({'QPD45S': 100, 'QPD32S': 100, 'QPD24S': 100, 'QPD42S': 100, 'QPD5S': 100, 'QPD20S': 100, 'QPD39S': 100, 'QPD10S': 100, 'QPD16S': 100, 'QPD27S': 100})\n",
      "\n",
      "double images sameQP:  100\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_sameQP_csv1, second_sameQP_csv2, second_sameQP_csv3,\n",
    "    second_sameQP_csv4, second_sameQP_csv5, second_sameQP_csv6,\n",
    "    second_sameQP_csv7, second_sameQP_csv8, second_sameQP_csv9,\n",
    "    second_sameQP_csv10\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD5S\": [\"_1stQP5_2ndQP5\"],\n",
    "    \"QPD10S\": [\"_1stQP10_2ndQP10\"],\n",
    "    \"QPD16S\": [\"_1stQP16_2ndQP16\"],\n",
    "    \"QPD20S\": [\"_1stQP20_2ndQP20\"],\n",
    "    \"QPD24S\": [\"_1stQP24_2ndQP24\"],\n",
    "    \"QPD27S\": [\"_1stQP27_2ndQP27\"],\n",
    "    \"QPD32S\": [\"_1stQP32_2ndQP32\"],\n",
    "    \"QPD39S\": [\"_1stQP39_2ndQP39\"],\n",
    "    \"QPD42S\": [\"_1stQP42_2ndQP42\"],\n",
    "    \"QPD45S\": [\"_1stQP45_2ndQP45\"],\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        while len(selected_from_dataset) < 100 and indices:\n",
    "            idx = indices.pop()\n",
    "            item = dataset[idx]\n",
    "            item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            if qpd_lists:\n",
    "                if all(selected_counts[qpd] < 100 / len(QPD) for qpd in qpd_lists):\n",
    "                    selected_from_dataset.append(item)\n",
    "                    for qpd in qpd_lists:\n",
    "                        selected_counts[qpd] += 1\n",
    "        \n",
    "        selected_data.append(selected_from_dataset)\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "\n",
    "# Print the distribution of QPD lists in the selected data (optional)\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_sameQP_csv1 = selected_data[0]\n",
    "second_sameQP_csv2 = selected_data[1]\n",
    "second_sameQP_csv3 = selected_data[2]\n",
    "second_sameQP_csv4 = selected_data[3]\n",
    "second_sameQP_csv5 = selected_data[4]\n",
    "second_sameQP_csv6 = selected_data[5]\n",
    "second_sameQP_csv7 = selected_data[6]\n",
    "second_sameQP_csv8 = selected_data[7]\n",
    "second_sameQP_csv9 = selected_data[8]\n",
    "second_sameQP_csv10 = selected_data[9]\n",
    "print('\\ndouble images sameQP: ', len(second_sameQP_csv1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "List 10: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD17M': 50, 'QPD7M': 49, 'QPD9M': 48, 'QPD10M': 48, 'QPD12M': 48, 'QPD2M': 47, 'QPD14M': 45, 'QPD5M': 44, 'QPD22M': 44, 'QPD35M': 44, 'QPD4M': 43, 'QPD15M': 42, 'QPD29M': 42, 'QPD32M': 42, 'QPD13M': 41, 'QPD19M': 41, 'QPD24M': 41, 'QPD27M': 41, 'QPD1M': 40, 'QPD6M': 40, 'QPD20M': 40, 'QPD25M': 40, 'QPD30M': 40})\n",
      "\n",
      "double images largeQP2:  100\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP2_csv1, second_largeQP2_csv2, second_largeQP2_csv3,\n",
    "    second_largeQP2_csv4, second_largeQP2_csv5, second_largeQP2_csv6,\n",
    "    second_largeQP2_csv7, second_largeQP2_csv8, second_largeQP2_csv9,\n",
    "    second_largeQP2_csv10\n",
    "]\n",
    "\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1M\": [\"_1stQP15_2ndQP16\"],\n",
    "    \"QPD2M\": [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"],\n",
    "    \"QPD4M\": [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"],\n",
    "    \"QPD5M\": [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"],\n",
    "    \"QPD6M\": [\"_1stQP10_2ndQP16\"],\n",
    "    \"QPD7M\": [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"],\n",
    "    \"QPD9M\": [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"],\n",
    "    \"QPD10M\": [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"],\n",
    "    \"QPD12M\": [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"],\n",
    "    \"QPD13M\": [\"_1stQP32_2ndQP45\"],\n",
    "    \"QPD14M\": [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"],\n",
    "    \"QPD15M\": [\"_1stQP30_2ndQP45\"],\n",
    "    \"QPD17M\": [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"],\n",
    "    \"QPD19M\": [\"_1stQP20_2ndQP39\"],\n",
    "    \"QPD20M\": [\"_1stQP25_2ndQP45\"],\n",
    "    \"QPD22M\": [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"],\n",
    "    \"QPD24M\": [\"_1stQP15_2ndQP39\"],\n",
    "    \"QPD25M\": [\"_1stQP20_2ndQP45\"],\n",
    "    \"QPD27M\": [\"_1stQP15_2ndQP42\"],\n",
    "    \"QPD29M\": [\"_1stQP10_2ndQP39\"],\n",
    "    \"QPD30M\": [\"_1stQP15_2ndQP45\"],\n",
    "    \"QPD32M\": [\"_1stQP10_2ndQP42\"],\n",
    "    \"QPD35M\": [\"_1stQP10_2ndQP45\"]\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 4 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 4:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 4:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_largeQP2_csv1 = selected_data[0]\n",
    "second_largeQP2_csv2 = selected_data[1]\n",
    "second_largeQP2_csv3 = selected_data[2]\n",
    "second_largeQP2_csv4 = selected_data[3]\n",
    "second_largeQP2_csv5 = selected_data[4]\n",
    "second_largeQP2_csv6 = selected_data[5]\n",
    "second_largeQP2_csv7 = selected_data[6]\n",
    "second_largeQP2_csv8 = selected_data[7]\n",
    "second_largeQP2_csv9 = selected_data[8]\n",
    "second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images largeQP2: ', len(second_largeQP2_csv1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv9 + second_largeQP2_csv10\n",
    "print(\"train_csv_list: \", len(train_csv_list10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10      0  11136   7328  11444  30092      0  11072   7328  11228  30372   7491   8539  2984   3615   2079   1860   2967   1688   7187   9115  3178   4467   1837   1886   2835   1666  11800  15996   8388   7240   1872  14704  11788  16300   8880   7508   1700  13824  0.000059  0.003921   0.000957  0.132963   0.07724\n",
      "1         16      0  13312   8832  14516  23340      0  12544   9488  14056  23912   7549   6398  2989   2266   1848   1684   3045   1517   7306   6812  3210   2833   2071   1666   3021   1622  11260   7064   8016   4340   1488  27832  10744   7224   8132   5068   1248  27584   0.00101  0.002715   0.001543  0.053895  0.027892\n",
      "2         20      0  12992  13136  14512  19360      0  12800  13216  14324  19660   8607   6806  2501   1786   1169   1817   3044   1635   8811   7376  2575   2054   1176   1812   3137   1529   8912   3808   6924   3744   1376  35236   8564   3688   7204   3760   1208  35576  0.000087   0.00101   0.000454  0.020671  0.013854\n",
      "3         24      0  19712   9648  14684  15956      0  19904   9408  14572  16116   9706   6686  1820   3043   1120   1553   3213   1666   9622   7575  1944   3216   1174   1504   3360   1604   8276   1792   5592   2084   1332  40924   7644   2032   5208   1952   1036  42128  0.000086  0.001595   0.001907  0.077032  0.071636\n",
      "4         27      0  21632  10624  14236  13508      0  21440  11104  14020  13436   9690   7583  1741   3260    990   1580   3522   1435   9989   8654  1640   3886    792   1616   3701   1413   5956   1452   3192   1844    716  46840   5348   1268   3260   1564    836  47724  0.000221   0.00337   0.001462  0.217541  0.193399\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "595       42      0  22848  18080  15600   3472      0  23680  18752  14532   3036  12389   4745  5113   3739    607   1835  10079    835  11924   5689  5061   3553    704   1575  10019    945    884    660    456    652    224  57124    756    608    324    496    212  57604  0.001586   0.00333   0.001013  0.135657  0.210909\n",
      "596       45      0  26048  23088   9932    932      0  27584  23232   8496    688  13500   6257  1020    446    942   2665   8894   3251  13566   6928  1117    485    970   2898   9538   2825   1284    752    628    728    656  55952    992    440    516    424    360  57268  0.003298  0.002121   0.005724  0.129964  0.156126\n",
      "597       39      0  39744  13312   5716   1228      0  41024  12848   5020   1108  19171  12067   794   2161   1465    253   4070    148  18887  13001  1173   2674   1212    601   5747     80   1152    304    696    416    416  57016    820    144    464    192    192  58188  0.001348  0.012026    0.00638  0.138245  0.167951\n",
      "598       32      0  39104  14048   5412   1436      0  39424  13952   5232   1392  15791   5944  1931   3743   1438   1257   3832    694  14494   6194  1911   4060   1538   1180   4365    973   2544    824    824   1020    260  54528   2224    836    932    836    240  54932   0.00009  0.004572   0.000829   0.24981  0.202846\n",
      "599       45      0  46016  11664   2192    128      0  48896   9328   1680     96  27987  14524   967    736    740    308   3361    188  28845  13944  1515    896    888    244   4917    301    544    144    232    288    320  58472     80     64    160    208    192  59296  0.007218  0.008236   0.011335  0.153793  0.129248\n",
      "\n",
      "[600 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP,\n",
    "                            X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8129    0.7967    0.8047       300\n",
      "           1     0.8007    0.8167    0.8086       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8068    0.8067    0.8066       600\n",
      "weighted avg     0.8068    0.8067    0.8066       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8340    0.6700    0.7431       300\n",
      "           1     0.7242    0.8667    0.7891       300\n",
      "\n",
      "    accuracy                         0.7683       600\n",
      "   macro avg     0.7791    0.7683    0.7661       600\n",
      "weighted avg     0.7791    0.7683    0.7661       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8293    0.6800    0.7473       300\n",
      "           1     0.7288    0.8600    0.7890       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7790    0.7700    0.7681       600\n",
      "weighted avg     0.7790    0.7700    0.7681       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8363    0.6300    0.7186       300\n",
      "           1     0.7032    0.8767    0.7804       300\n",
      "\n",
      "    accuracy                         0.7533       600\n",
      "   macro avg     0.7697    0.7533    0.7495       600\n",
      "weighted avg     0.7697    0.7533    0.7495       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5558    0.9300    0.6958       300\n",
      "           1     0.7857    0.2567    0.3869       300\n",
      "\n",
      "    accuracy                         0.5933       600\n",
      "   macro avg     0.6707    0.5933    0.5413       600\n",
      "weighted avg     0.6707    0.5933    0.5413       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7757    0.6800    0.7247       300\n",
      "           1     0.7151    0.8033    0.7567       300\n",
      "\n",
      "    accuracy                         0.7417       600\n",
      "   macro avg     0.7454    0.7417    0.7407       600\n",
      "weighted avg     0.7454    0.7417    0.7407       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7632    0.8167    0.7890       300\n",
      "           1     0.8029    0.7467    0.7737       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7831    0.7817    0.7814       600\n",
      "weighted avg     0.7831    0.7817    0.7814       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7293    0.6467    0.6855       300\n",
      "           1     0.6826    0.7600    0.7192       300\n",
      "\n",
      "    accuracy                         0.7033       600\n",
      "   macro avg     0.7060    0.7033    0.7024       600\n",
      "weighted avg     0.7060    0.7033    0.7024       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7417    0.7467    0.7442       300\n",
      "           1     0.7450    0.7400    0.7425       300\n",
      "\n",
      "    accuracy                         0.7433       600\n",
      "   macro avg     0.7433    0.7433    0.7433       600\n",
      "weighted avg     0.7433    0.7433    0.7433       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5600    0.9333    0.7000       300\n",
      "           1     0.8000    0.2667    0.4000       300\n",
      "\n",
      "    accuracy                         0.6000       600\n",
      "   macro avg     0.6800    0.6000    0.5500       600\n",
      "weighted avg     0.6800    0.6000    0.5500       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8194    0.6200    0.7059       300\n",
      "           1     0.6944    0.8633    0.7697       300\n",
      "\n",
      "    accuracy                         0.7417       600\n",
      "   macro avg     0.7569    0.7417    0.7378       600\n",
      "weighted avg     0.7569    0.7417    0.7378       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8291    0.6467    0.7266       300\n",
      "           1     0.7104    0.8667    0.7808       300\n",
      "\n",
      "    accuracy                         0.7567       600\n",
      "   macro avg     0.7697    0.7567    0.7537       600\n",
      "weighted avg     0.7697    0.7567    0.7537       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8304    0.6200    0.7099       300\n",
      "           1     0.6968    0.8733    0.7751       300\n",
      "\n",
      "    accuracy                         0.7467       600\n",
      "   macro avg     0.7636    0.7467    0.7425       600\n",
      "weighted avg     0.7636    0.7467    0.7425       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8097    0.6667    0.7313       300\n",
      "           1     0.7167    0.8433    0.7749       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.7632    0.7550    0.7531       600\n",
      "weighted avg     0.7632    0.7550    0.7531       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5602    0.7600    0.6450       300\n",
      "           1     0.6269    0.4033    0.4909       300\n",
      "\n",
      "    accuracy                         0.5817       600\n",
      "   macro avg     0.5936    0.5817    0.5679       600\n",
      "weighted avg     0.5936    0.5817    0.5679       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7852    0.7067    0.7439       300\n",
      "           1     0.7333    0.8067    0.7683       300\n",
      "\n",
      "    accuracy                         0.7567       600\n",
      "   macro avg     0.7593    0.7567    0.7561       600\n",
      "weighted avg     0.7593    0.7567    0.7561       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7810    0.7967    0.7888       300\n",
      "           1     0.7925    0.7767    0.7845       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7868    0.7867    0.7866       600\n",
      "weighted avg     0.7868    0.7867    0.7866       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7829    0.7933    0.7881       300\n",
      "           1     0.7905    0.7800    0.7852       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7867    0.7867    0.7867       600\n",
      "weighted avg     0.7867    0.7867    0.7867       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7692    0.8000    0.7843       300\n",
      "           1     0.7917    0.7600    0.7755       300\n",
      "\n",
      "    accuracy                         0.7800       600\n",
      "   macro avg     0.7804    0.7800    0.7799       600\n",
      "weighted avg     0.7804    0.7800    0.7799       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5510    0.9367    0.6938       300\n",
      "           1     0.7889    0.2367    0.3641       300\n",
      "\n",
      "    accuracy                         0.5867       600\n",
      "   macro avg     0.6699    0.5867    0.5290       600\n",
      "weighted avg     0.6699    0.5867    0.5290       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7866    0.8600    0.8217       300\n",
      "           1     0.8456    0.7667    0.8042       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8161    0.8133    0.8129       600\n",
      "weighted avg     0.8161    0.8133    0.8129       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7020    0.9500    0.8074       300\n",
      "           1     0.9227    0.5967    0.7247       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.8123    0.7733    0.7660       600\n",
      "weighted avg     0.8123    0.7733    0.7660       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7819    0.7767    0.7793       300\n",
      "           1     0.7781    0.7833    0.7807       300\n",
      "\n",
      "    accuracy                         0.7800       600\n",
      "   macro avg     0.7800    0.7800    0.7800       600\n",
      "weighted avg     0.7800    0.7800    0.7800       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6951    0.9500    0.8028       300\n",
      "           1     0.9211    0.5833    0.7143       300\n",
      "\n",
      "    accuracy                         0.7667       600\n",
      "   macro avg     0.8081    0.7667    0.7586       600\n",
      "weighted avg     0.8081    0.7667    0.7586       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5567    0.9167    0.6927       300\n",
      "           1     0.7642    0.2700    0.3990       300\n",
      "\n",
      "    accuracy                         0.5933       600\n",
      "   macro avg     0.6604    0.5933    0.5459       600\n",
      "weighted avg     0.6604    0.5933    0.5459       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6532    0.9667    0.7796       300\n",
      "           1     0.9359    0.4867    0.6404       300\n",
      "\n",
      "    accuracy                         0.7267       600\n",
      "   macro avg     0.7945    0.7267    0.7100       600\n",
      "weighted avg     0.7945    0.7267    0.7100       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6372    0.9600    0.7660       300\n",
      "           1     0.9189    0.4533    0.6071       300\n",
      "\n",
      "    accuracy                         0.7067       600\n",
      "   macro avg     0.7780    0.7067    0.6866       600\n",
      "weighted avg     0.7780    0.7067    0.6866       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6394    0.9633    0.7686       300\n",
      "           1     0.9257    0.4567    0.6116       300\n",
      "\n",
      "    accuracy                         0.7100       600\n",
      "   macro avg     0.7825    0.7100    0.6901       600\n",
      "weighted avg     0.7825    0.7100    0.6901       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6392    0.9567    0.7664       300\n",
      "           1     0.9139    0.4600    0.6120       300\n",
      "\n",
      "    accuracy                         0.7083       600\n",
      "   macro avg     0.7766    0.7083    0.6892       600\n",
      "weighted avg     0.7766    0.7083    0.6892       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5517    0.9600    0.7007       300\n",
      "           1     0.8462    0.2200    0.3492       300\n",
      "\n",
      "    accuracy                         0.5900       600\n",
      "   macro avg     0.6989    0.5900    0.5250       600\n",
      "weighted avg     0.6989    0.5900    0.5250       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7768    0.9167    0.8410       300\n",
      "           1     0.8984    0.7367    0.8095       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8376    0.8267    0.8253       600\n",
      "weighted avg     0.8376    0.8267    0.8253       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7768    0.8467    0.8102       300\n",
      "           1     0.8315    0.7567    0.7923       300\n",
      "\n",
      "    accuracy                         0.8017       600\n",
      "   macro avg     0.8041    0.8017    0.8013       600\n",
      "weighted avg     0.8041    0.8017    0.8013       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7928    0.8800    0.8341       300\n",
      "           1     0.8652    0.7700    0.8148       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8290    0.8250    0.8245       600\n",
      "weighted avg     0.8290    0.8250    0.8245       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7902    0.8033    0.7967       300\n",
      "           1     0.8000    0.7867    0.7933       300\n",
      "\n",
      "    accuracy                         0.7950       600\n",
      "   macro avg     0.7951    0.7950    0.7950       600\n",
      "weighted avg     0.7951    0.7950    0.7950       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5516    0.9267    0.6915       300\n",
      "           1     0.7708    0.2467    0.3737       300\n",
      "\n",
      "    accuracy                         0.5867       600\n",
      "   macro avg     0.6612    0.5867    0.5326       600\n",
      "weighted avg     0.6612    0.5867    0.5326       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7679    0.7167    0.7414       300\n",
      "           1     0.7344    0.7833    0.7581       300\n",
      "\n",
      "    accuracy                         0.7500       600\n",
      "   macro avg     0.7511    0.7500    0.7497       600\n",
      "weighted avg     0.7511    0.7500    0.7497       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.5167    0.6379       300\n",
      "           1     0.6498    0.8967    0.7535       300\n",
      "\n",
      "    accuracy                         0.7067       600\n",
      "   macro avg     0.7415    0.7067    0.6957       600\n",
      "weighted avg     0.7415    0.7067    0.6957       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7732    0.6933    0.7311       300\n",
      "           1     0.7221    0.7967    0.7575       300\n",
      "\n",
      "    accuracy                         0.7450       600\n",
      "   macro avg     0.7476    0.7450    0.7443       600\n",
      "weighted avg     0.7476    0.7450    0.7443       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8448    0.4900    0.6203       300\n",
      "           1     0.6408    0.9100    0.7521       300\n",
      "\n",
      "    accuracy                         0.7000       600\n",
      "   macro avg     0.7428    0.7000    0.6862       600\n",
      "weighted avg     0.7428    0.7000    0.6862       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5573    0.9567    0.7043       300\n",
      "           1     0.8471    0.2400    0.3740       300\n",
      "\n",
      "    accuracy                         0.5983       600\n",
      "   macro avg     0.7022    0.5983    0.5392       600\n",
      "weighted avg     0.7022    0.5983    0.5392       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7467    0.7567    0.7517       300\n",
      "           1     0.7534    0.7433    0.7483       300\n",
      "\n",
      "    accuracy                         0.7500       600\n",
      "   macro avg     0.7500    0.7500    0.7500       600\n",
      "weighted avg     0.7500    0.7500    0.7500       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8038    0.7100    0.7540       300\n",
      "           1     0.7403    0.8267    0.7811       300\n",
      "\n",
      "    accuracy                         0.7683       600\n",
      "   macro avg     0.7720    0.7683    0.7675       600\n",
      "weighted avg     0.7720    0.7683    0.7675       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8040    0.8067    0.8053       300\n",
      "           1     0.8060    0.8033    0.8047       300\n",
      "\n",
      "    accuracy                         0.8050       600\n",
      "   macro avg     0.8050    0.8050    0.8050       600\n",
      "weighted avg     0.8050    0.8050    0.8050       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8347    0.6733    0.7454       300\n",
      "           1     0.7263    0.8667    0.7903       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7805    0.7700    0.7678       600\n",
      "weighted avg     0.7805    0.7700    0.7678       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5599    0.9500    0.7046       300\n",
      "           1     0.8352    0.2533    0.3887       300\n",
      "\n",
      "    accuracy                         0.6017       600\n",
      "   macro avg     0.6975    0.6017    0.5467       600\n",
      "weighted avg     0.6975    0.6017    0.5467       600\n",
      "\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8314    0.7233    0.7736       300\n",
      "           1     0.7552    0.8533    0.8013       300\n",
      "\n",
      "    accuracy                         0.7883       600\n",
      "   macro avg     0.7933    0.7883    0.7874       600\n",
      "weighted avg     0.7933    0.7883    0.7874       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7217    0.7867    0.7528       300\n",
      "           1     0.7656    0.6967    0.7295       300\n",
      "\n",
      "    accuracy                         0.7417       600\n",
      "   macro avg     0.7436    0.7417    0.7411       600\n",
      "weighted avg     0.7436    0.7417    0.7411       600\n",
      "\n",
      "Summary_onlyGhost_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7287    0.9133    0.8107       300\n",
      "           1     0.8839    0.6600    0.7557       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.8063    0.7867    0.7832       600\n",
      "weighted avg     0.8063    0.7867    0.7832       600\n",
      "\n",
      "Summary_onlyGhost_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7704    0.8167    0.7929       300\n",
      "           1     0.8050    0.7567    0.7801       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7877    0.7867    0.7865       600\n",
      "weighted avg     0.7877    0.7867    0.7865       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5525    0.9467    0.6978       300\n",
      "           1     0.8140    0.2333    0.3627       300\n",
      "\n",
      "    accuracy                         0.5900       600\n",
      "   macro avg     0.6832    0.5900    0.5302       600\n",
      "weighted avg     0.6832    0.5900    0.5302       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500]}\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Score_RBF', 'tnr_rbf', 'tpr_rbf',\n",
    "                                'C_LINEAR', 'Score_LINEAR', 'tnr_linear', 'tpr_linear',\n",
    "                                'C_OG_RBF', 'Score_OG_RBF', 'tnr_og_rbf', 'tpr_og_rbf',\n",
    "                                'C_OG_LINEAR', 'Score_OG_LINEAR', 'tnr_og_linear', 'tpr_og_linear',\n",
    "                                'Threshold', 'Score_old', 'tnr_old', 'tpr_old'])\n",
    "    \n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "        \n",
    "    test_data = [X_train_list[i] for i in test_ids]\n",
    "    test_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    test_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    \n",
    "    train_data = [item for data in train_data for item in data]\n",
    "    train_data_OG = [item for data in train_data_OG for item in data]\n",
    "    train_label = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(train_data, train_label, test_size=600, random_state=42)\n",
    "    X_train_OG, X_val_OG, _, _ = train_test_split(train_data_OG, train_label, test_size=600, random_state=42)\n",
    "    \n",
    "    test_data = [item for data in test_data for item in data]\n",
    "    test_data_OG = [item for data in test_data_OG for item in data]\n",
    "    test_label = [item for data in test_label for item in data]\n",
    "\n",
    "    MAE_data = [MAE_list[i] for i in test_ids]\n",
    "    MAE_data = [item for data in MAE_data for item in data]\n",
    "    \n",
    "    FINAL_QP_data = [FINAL_QP_list[i] for i in test_ids]\n",
    "    FINAL_QP_data = [item for data in FINAL_QP_data for item in data]\n",
    "                                                                                                 \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "                \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        results_old = np.array([is_double_compressed(MAE_data[i], FINAL_QP_data[i], threshold) for i in range(600)])\n",
    "        predicted_labels = results_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "            \n",
    "    # テストデータで評価\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(test_data)\n",
    "    # test_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    test_accuracy_RBF = accuracy_score(test_label, test_predictions_RBF)\n",
    "    report_RBF = classification_report(test_label, test_predictions_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_RBF)\n",
    "    tnr_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data)\n",
    "    # test_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    test_accuracy_LINEAR = accuracy_score(test_label, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label, test_predictions_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_LINEAR)\n",
    "    tnr_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(test_data_OG)\n",
    "    # test_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(test_label, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(test_label, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_RBF)\n",
    "    tnr_og_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(test_data_OG)\n",
    "    # test_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(test_label, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    tnr_og_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "\n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Score_RBF': test_accuracy_RBF, 'tnr_rbf': tnr_rbf, 'tpr_rbf': tpr_rbf,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Score_LINEAR': test_accuracy_LINEAR, 'tnr_linear': tnr_linear, 'tpr_linear': tpr_linear,\n",
    "              'C_OG_RBF': best_c_value_onlyGhost_RBF, 'Score_OG_RBF': test_accuracy_onlyGhost_RBF, 'tnr_og_rbf': tnr_og_rbf, 'tpr_og_rbf': tpr_og_rbf,\n",
    "              'C_OG_LINEAR': best_c_value_onlyGhost_LINEAR, 'Score_OG_LINEAR': test_accuracy_onlyGhost_LINEAR, 'tnr_og_linear': tnr_og_linear, 'tpr_og_linear': tpr_og_linear,\n",
    "              'Threshold': best_threshold, 'Score_old': best_accuracy, 'tnr_old': tnr_old, 'tpr_old': tpr_old,}\n",
    "\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0                    RBF        77.43        76.60               77.02                3.53           82.67           72.67\n",
      "1                 LINEAR        77.00        74.83               75.92                3.21           80.17           70.67\n",
      "2     Only Ghost and RBF        77.73        75.43               76.58                3.94           82.50           70.33\n",
      "3  Only Ghost and LINEAR        75.33        75.83               75.58                3.15           79.50           70.00\n",
      "4                    OLD        92.17        26.27               59.22                0.64           60.17           58.17\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF', 'LINEAR', 'Only Ghost and RBF', 'Only Ghost and LINEAR', 'OLD'],\n",
    "    'Average TNR': [round(results['tnr_rbf'].mean() * 100, 2), round(results['tnr_linear'].mean() * 100, 2), round(results['tnr_og_rbf'].mean() * 100, 2), round(results['tnr_og_linear'].mean() * 100, 2), round(results['tnr_old'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf'].mean() * 100, 2), round(results['tpr_linear'].mean() * 100, 2), round(results['tpr_og_rbf'].mean() * 100, 2), round(results['tpr_og_linear'].mean() * 100, 2), round(results['tpr_old'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF'].mean() * 100, 2), round(results['Score_LINEAR'].mean() * 100, 2), round(results['Score_OG_RBF'].mean() * 100, 2), round(results['Score_OG_LINEAR'].mean() * 100, 2), round(results['Score_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF'].std() * 100, 2), round(results['Score_LINEAR'].std() * 100, 2), round(results['Score_OG_RBF'].std() * 100, 2), round(results['Score_OG_LINEAR'].std() * 100, 2), round(results['Score_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF'].max() * 100, 2), round(results['Score_LINEAR'].max() * 100, 2), round(results['Score_OG_RBF'].max() * 100, 2), round(results['Score_OG_LINEAR'].max() * 100, 2), round(results['Score_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF'].min() * 100, 2), round(results['Score_LINEAR'].min() * 100, 2), round(results['Score_OG_RBF'].min() * 100, 2), round(results['Score_OG_LINEAR'].min() * 100, 2), round(results['Score_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      10\n",
      "1     100\n",
      "2      10\n",
      "3     100\n",
      "4      10\n",
      "5     100\n",
      "6     100\n",
      "7      10\n",
      "8    2000\n",
      "9      10\n",
      "Name: C_RBF, dtype: object\n",
      "0    1000\n",
      "1      10\n",
      "2    4000\n",
      "3      10\n",
      "4    2000\n",
      "5     100\n",
      "6    1000\n",
      "7    1000\n",
      "8    3000\n",
      "9      10\n",
      "Name: C_LINEAR, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF'])\n",
    "print(results['C_LINEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
