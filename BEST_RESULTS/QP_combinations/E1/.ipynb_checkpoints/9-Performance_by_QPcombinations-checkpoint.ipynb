{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['58', '232', '29', '274', '207', '62', '145', '233', '298', '126', '65', '12', '106', '129', '272', '206', '216', '254', '74', '122', '57', '264', '23', '102', '8', '95'], ['93', '196', '139', '213', '70', '300', '135', '256', '24', '114', '5', '268', '229', '104', '279', '142', '244', '13', '32', '201', '107', '59', '248', '226', '66', '147'], ['210', '221', '203', '198', '223', '67', '247', '86', '49', '262', '214', '132', '117', '54', '80', '123', '72', '60', '17', '288', '252', '103', '7', '269', '90', '130'], ['255', '289', '14', '287', '133', '28', '239', '39', '249', '291', '285', '236', '97', '1', '220', '33', '69', '118', '215', '19', '115', '120', '194', '205', '81', '193'], ['3', '94', '11', '218', '276', '84', '149', '42', '204', '41', '209', '191', '250', '15', '230', '278', '9', '92', '237', '296', '127', '25', '225', '263', '258', '297'], ['124', '228', '110', '141', '21', '146', '211', '245', '240', '18', '253', '79', '200', '134', '281', '48', '51', '10', '261', '284', '222', '293', '246', '34', '35', '31'], ['273', '259', '231', '55', '277', '96', '282', '56', '295', '52', '197', '235', '68', '234', '241', '265', '30', '267', '199', '83', '251', '91', '20', '260', '2', '219'], ['16', '257', '227', '116', '40', '299', '38', '148', '286', '47', '243', '53', '63', '109', '212', '100', '85', '271', '238', '131', '125', '217', '280', '242', '105', '138'], ['77', '292', '73', '270', '224', '144', '50', '136', '140', '88', '46', '61', '82', '137', '119', '111', '26', '195', '78', '150', '64', '192', '45', '266', '143', '37'], ['76', '121', '112', '108', '275', '43', '4', '71', '75', '6', '208', '89', '22', '44', '98', '101', '87', '202', '113', '99', '290', '283', '294', '36', '27', '128']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1482\n",
      "CSV List 2A: 1482\n",
      "CSV List 3A: 1482\n",
      "CSV List 4A: 1482\n",
      "CSV List 5A: 1482\n",
      "CSV List 6A: 1482\n",
      "CSV List 7A: 1482\n",
      "CSV List 8A: 1482\n",
      "CSV List 9A: 1482\n",
      "CSV List 10A: 1482\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1482\n",
      "CSV List 2A: 1482\n",
      "CSV List 3A: 1482\n",
      "CSV List 4A: 1482\n",
      "CSV List 5A: 1482\n",
      "CSV List 6A: 1482\n",
      "CSV List 7A: 1482\n",
      "CSV List 8A: 1482\n",
      "CSV List 9A: 1482\n",
      "CSV List 10A: 1482\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1014\n",
      "CSV List 2A: 1014\n",
      "CSV List 3A: 1014\n",
      "CSV List 4A: 1014\n",
      "CSV List 5A: 1014\n",
      "CSV List 6A: 1014\n",
      "CSV List 7A: 1014\n",
      "CSV List 8A: 1014\n",
      "CSV List 9A: 1014\n",
      "CSV List 10A: 1014\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1014\n",
      "CSV List 2A: 1014\n",
      "CSV List 3A: 1014\n",
      "CSV List 4A: 1014\n",
      "CSV List 5A: 1014\n",
      "CSV List 6A: 1014\n",
      "CSV List 7A: 1014\n",
      "CSV List 8A: 1014\n",
      "CSV List 9A: 1014\n",
      "CSV List 10A: 1014\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1482\n",
      "PKL List 2A: 1482\n",
      "PKL List 3A: 1482\n",
      "PKL List 4A: 1482\n",
      "PKL List 5A: 1482\n",
      "PKL List 6A: 1482\n",
      "PKL List 7A: 1482\n",
      "PKL List 8A: 1482\n",
      "PKL List 9A: 1482\n",
      "PKL List 10A: 1482\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1482\n",
      "PKL List 2A: 1482\n",
      "PKL List 3A: 1482\n",
      "PKL List 4A: 1482\n",
      "PKL List 5A: 1482\n",
      "PKL List 6A: 1482\n",
      "PKL List 7A: 1482\n",
      "PKL List 8A: 1482\n",
      "PKL List 9A: 1482\n",
      "PKL List 10A: 1482\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1014\n",
      "PKL List 2A: 1014\n",
      "PKL List 3A: 1014\n",
      "PKL List 4A: 1014\n",
      "PKL List 5A: 1014\n",
      "PKL List 6A: 1014\n",
      "PKL List 7A: 1014\n",
      "PKL List 8A: 1014\n",
      "PKL List 9A: 1014\n",
      "PKL List 10A: 1014\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1014\n",
      "PKL List 2A: 1014\n",
      "PKL List 3A: 1014\n",
      "PKL List 4A: 1014\n",
      "PKL List 5A: 1014\n",
      "PKL List 6A: 1014\n",
      "PKL List 7A: 1014\n",
      "PKL List 8A: 1014\n",
      "PKL List 9A: 1014\n",
      "PKL List 10A: 1014\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "\n",
    "train_list6 = [\"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\"]\n",
    "\n",
    "train_list7 = [\"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\", \"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\"]\n",
    "\n",
    "train_list8 = [\"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\", \"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\"]\n",
    "\n",
    "train_list9 = [\"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\", \"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\"]\n",
    "\n",
    "train_list10 = [\"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+26] for i in range(0, len(combined_train_list), 26)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "\n",
    "single_csv1 = random.sample(single_csv1, 240)\n",
    "single_csv2 = random.sample(single_csv2, 240)\n",
    "single_csv3 = random.sample(single_csv3, 240)\n",
    "single_csv4 = random.sample(single_csv4, 240)\n",
    "single_csv5 = random.sample(single_csv5, 240)\n",
    "single_csv6 = random.sample(single_csv6, 240)\n",
    "single_csv7 = random.sample(single_csv7, 240)\n",
    "single_csv8 = random.sample(single_csv8, 240)\n",
    "single_csv9 = random.sample(single_csv9, 240)\n",
    "single_csv10 = random.sample(single_csv10, 240)\n",
    "print(len(single_csv1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482\n",
      "\n",
      "double images train by QP1>QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 80)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 80)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 80)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 80)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 80)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 80)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 80)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 80)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 80)\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 80)\n",
    "# second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1>QP2: ', len(second_largeQP1_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "\n",
      "double images train by QP1=QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "print(len(second_sameQP_csv10))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 80)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 80)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 80)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 80)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 80)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 80)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 80)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 80)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 80)\n",
    "second_sameQP_csv10 = random.sample(second_sameQP_csv10, 80)\n",
    "print('\\ndouble images train by QP1=QP2: ',len(second_sameQP_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "\n",
      "double images train by QP1<QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "print(len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 80)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 80)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 80)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 80)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 80)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 80)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 80)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 80)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 80)\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 80)\n",
    "# second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1<QP2: ', len(second_largeQP2_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2 = [\"_1stQP2_\"]\n",
    "QP4 = [\"_1stQP4_\"]\n",
    "QP12 = [\"_1stQP12_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP4_QP2 = [\"_1stQP4_2ndQP2_\"]\n",
    "QP12_QP2 = [\"_1stQP12_2ndQP2_\"]\n",
    "QP12_QP4 = [\"_1stQP12_2ndQP4_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2_QP2 = [\"_1stQP2_2ndQP2\"]\n",
    "QP4_QP4 = [\"_1stQP4_2ndQP4\"]\n",
    "QP12_QP12 = [\"_1stQP12_2ndQP12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2_QP4 = [\"_1stQP2_2ndQP4\"]\n",
    "QP2_QP12 = [\"_1stQP2_2ndQP12\"]\n",
    "QP4_QP12 = [\"_1stQP4_2ndQP12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath2 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath2, 'GIMP_csv')\n",
    "GIMP_path2 = os.path.join(rootpath2, 'GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_path1_csv = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_csv = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath2, 'LIBHEIF_csv')\n",
    "LIBHEIF_path2 = os.path.join(rootpath2, 'LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_path1_csv = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_csv = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath2, 'GIMP_GIMP_csv')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath2, 'GIMP_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_GIMP_path1_csv = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_csv = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath2, 'LIBHEIF_GIMP_csv')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath2, 'LIBHEIF_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_GIMP_path1_csv = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_csv = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath2, 'GIMP_LIBHEIF_csv')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath2, 'GIMP_LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_LIBHEIF_path1_csv = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_csv = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath2 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath2, 'GIMP_csv')\n",
    "GIMP_path2 = os.path.join(rootpath2, 'GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_path1_csv = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_csv = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath2, 'LIBHEIF_csv')\n",
    "LIBHEIF_path2 = os.path.join(rootpath2, 'LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_path1_csv = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_csv = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath2, 'GIMP_GIMP_csv')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath2, 'GIMP_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_GIMP_path1_csv = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_csv = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath2, 'LIBHEIF_GIMP_csv')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath2, 'LIBHEIF_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_GIMP_path1_csv = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_csv = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath2, 'GIMP_LIBHEIF_csv')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath2, 'GIMP_LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_LIBHEIF_path1_csv = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_csv = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath3 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/PKL/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath3, 'pkl_GIMP')\n",
    "GIMP_path2 = os.path.join(rootpath3, 'pkl_GIMP_RECOMPRESSED')\n",
    "\n",
    "GIMP_path1_pkl = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_pkl = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath3, 'pkl_LIBHEIF')\n",
    "LIBHEIF_path2 = os.path.join(rootpath3, 'pkl_LIBHEIF_RECOMPRESSED')\n",
    "\n",
    "LIBHEIF_path1_pkl = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_pkl = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath3, 'pkl_GIMP_GIMP')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath3, 'pkl_GIMP_GIMP_RECOMPRESSED')\n",
    "\n",
    "GIMP_GIMP_path1_pkl = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_pkl = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath3, 'pkl_LIBHEIF_GIMP')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath3, 'pkl_LIBHEIF_GIMP_RECOMPRESSED')\n",
    "\n",
    "LIBHEIF_GIMP_path1_pkl = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_pkl = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath3, 'pkl_GIMP_LIBHEIF')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath3, 'pkl_GIMP_LIBHEIF_RECOMPRESSED')\n",
    "\n",
    "GIMP_LIBHEIF_path1_pkl = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_pkl = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIMP_csv = list(zip(GIMP_path1_csv, GIMP_path1_pkl, GIMP_path2_csv, GIMP_path2_pkl))\n",
    "\n",
    "\n",
    "single_QP2_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP2)]\n",
    "single_QP4_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP4)]\n",
    "single_QP12_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP12)]\n",
    "\n",
    "single_QP2_GIMP = random.sample(single_QP2_GIMP, 10)\n",
    "single_QP4_GIMP = random.sample(single_QP4_GIMP, 10)\n",
    "single_QP12_GIMP = random.sample(single_QP12_GIMP, 10)\n",
    "\n",
    "LIBHEIF_csv = list(zip(LIBHEIF_path1_csv, LIBHEIF_path1_pkl, LIBHEIF_path2_csv, LIBHEIF_path2_pkl))\n",
    "# LIBHEIF_csv1 = random.sample(LIBHEIF_csv, 10)\n",
    "\n",
    "single_QP2_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP2)]\n",
    "single_QP4_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP4)]\n",
    "single_QP12_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP12)]\n",
    "\n",
    "single_QP2_LIBHEIF = random.sample(single_QP2_LIBHEIF, 10)\n",
    "single_QP4_LIBHEIF = random.sample(single_QP4_LIBHEIF, 10)\n",
    "single_QP12_LIBHEIF = random.sample(single_QP12_LIBHEIF, 10)\n",
    "\n",
    "\n",
    "GIMP_GIMP_csv = list(zip(GIMP_GIMP_path1_csv, GIMP_GIMP_path1_pkl, GIMP_GIMP_path2_csv, GIMP_GIMP_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "LIBHEIF_GIMP_csv = list(zip(LIBHEIF_GIMP_path1_csv, LIBHEIF_GIMP_path1_pkl, LIBHEIF_GIMP_path2_csv, LIBHEIF_GIMP_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "GIMP_LIBHEIF_csv = list(zip(GIMP_LIBHEIF_path1_csv, GIMP_LIBHEIF_path1_pkl, GIMP_LIBHEIF_path2_csv, GIMP_LIBHEIF_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "# print(len(single_QP2_GIMP))\n",
    "# print(len(single_QP2_LIBHEIF))\n",
    "# print(len(second_QP4_QP2_GG))\n",
    "# print(len(second_QP4_QP2_LG))\n",
    "# print(len(second_QP4_QP2_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  480\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv10 + second_largeQP2_csv10\n",
    "print(\"train_csv_list: \", len(train_csv_list10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP12_QP4_GG:  20\n",
      "test_QP12_QP4_LG:  20\n",
      "test_QP12_QP4_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP4_QP2_GG = second_QP4_QP2_GG + single_QP2_GIMP\n",
    "test_QP12_QP2_GG = second_QP12_QP2_GG + single_QP2_GIMP\n",
    "test_QP12_QP4_GG = second_QP12_QP4_GG + single_QP4_GIMP\n",
    "\n",
    "test_QP4_QP2_LG = second_QP4_QP2_LG + single_QP2_GIMP\n",
    "test_QP12_QP2_LG = second_QP12_QP2_LG + single_QP2_GIMP\n",
    "test_QP12_QP4_LG = second_QP12_QP4_LG + single_QP4_GIMP\n",
    "\n",
    "test_QP4_QP2_GL = second_QP4_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP12_QP2_GL = second_QP12_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP12_QP4_GL = second_QP12_QP4_GL + single_QP4_LIBHEIF\n",
    "\n",
    "print('test_QP12_QP4_GG: ', len(test_QP12_QP4_GG))\n",
    "print('test_QP12_QP4_LG: ', len(test_QP12_QP4_LG))\n",
    "print('test_QP12_QP4_GL: ', len(test_QP12_QP4_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP12_QP12_GG:  20\n",
      "test_QP12_QP12_LG:  20\n",
      "test_QP12_QP12_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP2_QP2_GG = second_QP2_QP2_GG + single_QP2_GIMP\n",
    "test_QP4_QP4_GG = second_QP4_QP4_GG + single_QP4_GIMP\n",
    "test_QP12_QP12_GG = second_QP12_QP12_GG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP2_LG = second_QP2_QP2_LG + single_QP2_GIMP\n",
    "test_QP4_QP4_LG = second_QP4_QP4_LG + single_QP4_GIMP\n",
    "test_QP12_QP12_LG = second_QP12_QP12_LG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP2_GL = second_QP2_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP4_QP4_GL = second_QP4_QP4_GL + single_QP4_LIBHEIF\n",
    "test_QP12_QP12_GL = second_QP12_QP12_GL + single_QP12_LIBHEIF\n",
    "\n",
    "print('test_QP12_QP12_GG: ', len(test_QP12_QP12_GG))\n",
    "print('test_QP12_QP12_LG: ', len(test_QP12_QP12_LG))\n",
    "print('test_QP12_QP12_GL: ', len(test_QP12_QP12_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP4_QP12_GG:  20\n",
      "test_QP4_QP12_LG:  20\n",
      "test_QP4_QP12_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP2_QP4_GG = second_QP2_QP4_GG + single_QP4_GIMP\n",
    "test_QP2_QP12_GG = second_QP2_QP12_GG + single_QP12_GIMP\n",
    "test_QP4_QP12_GG = second_QP4_QP12_GG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP4_LG = second_QP2_QP4_LG + single_QP4_GIMP\n",
    "test_QP2_QP12_LG = second_QP2_QP12_LG + single_QP12_GIMP\n",
    "test_QP4_QP12_LG = second_QP4_QP12_LG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP4_GL = second_QP2_QP4_GL + single_QP4_LIBHEIF\n",
    "test_QP2_QP12_GL = second_QP2_QP12_GL + single_QP12_LIBHEIF\n",
    "test_QP4_QP12_GL = second_QP4_QP12_GL + single_QP12_LIBHEIF\n",
    "\n",
    "print('test_QP4_QP12_GG: ', len(test_QP4_QP12_GG))\n",
    "print('test_QP4_QP12_LG: ', len(test_QP4_QP12_LG))\n",
    "print('test_QP4_QP12_GL: ', len(test_QP4_QP12_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_OG = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1番目のCSVファイルを処理する\n",
    "test_df1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_QP4_QP2_GG)\n",
    "\n",
    "# 2番目のCSVファイルを処理する\n",
    "test_df2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_QP12_QP2_GG)\n",
    "\n",
    "# 3番目のCSVファイルを処理する\n",
    "test_df3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_QP12_QP4_GG)\n",
    "\n",
    "# 4番目のCSVファイルを処理する\n",
    "test_df4, LABEL_t4, MAE_t4, FINAL_QP_t4 = process_train_csv_lists(test_QP4_QP2_LG)\n",
    "\n",
    "# 5番目のCSVファイルを処理する\n",
    "test_df5, LABEL_t5, MAE_t5, FINAL_QP_t5 = process_train_csv_lists(test_QP12_QP2_LG)\n",
    "\n",
    "# 6番目のCSVファイルを処理する\n",
    "test_df6, LABEL_t6, MAE_t6, FINAL_QP_t6 = process_train_csv_lists(test_QP12_QP4_LG)\n",
    "\n",
    "# 7番目のCSVファイルを処理する\n",
    "test_df7, LABEL_t7, MAE_t7, FINAL_QP_t7 = process_train_csv_lists(test_QP4_QP2_GL)\n",
    "\n",
    "# 8番目のCSVファイルを処理する\n",
    "test_df8, LABEL_t8, MAE_t8, FINAL_QP_t8 = process_train_csv_lists(test_QP12_QP2_GL)\n",
    "\n",
    "# 9番目のCSVファイルを処理する\n",
    "test_df9, LABEL_t9, MAE_t9, FINAL_QP_t9 = process_train_csv_lists(test_QP12_QP4_GL)\n",
    "\n",
    "\n",
    "# 10番目のCSVファイルを処理する\n",
    "test_df10, LABEL_t10, MAE_t10, FINAL_QP_t10 = process_train_csv_lists(test_QP2_QP2_GG)\n",
    "\n",
    "# 11番目のCSVファイルを処理する\n",
    "test_df11, LABEL_t11, MAE_t11, FINAL_QP_t11 = process_train_csv_lists(test_QP4_QP4_GG)\n",
    "\n",
    "# 12番目のCSVファイルを処理する\n",
    "test_df12, LABEL_t12, MAE_t12, FINAL_QP_t12 = process_train_csv_lists(test_QP12_QP12_GG)\n",
    "\n",
    "# 13番目のCSVファイルを処理する\n",
    "test_df13, LABEL_t13, MAE_t13, FINAL_QP_t13 = process_train_csv_lists(test_QP2_QP2_LG)\n",
    "\n",
    "# 14番目のCSVファイルを処理する\n",
    "test_df14, LABEL_t14, MAE_t14, FINAL_QP_t14 = process_train_csv_lists(test_QP4_QP4_LG)\n",
    "\n",
    "# 15番目のCSVファイルを処理する\n",
    "test_df15, LABEL_t15, MAE_t15, FINAL_QP_t15 = process_train_csv_lists(test_QP12_QP12_LG)\n",
    "\n",
    "# 16番目のCSVファイルを処理する\n",
    "test_df16, LABEL_t16, MAE_t16, FINAL_QP_t16 = process_train_csv_lists(test_QP2_QP2_GL)\n",
    "\n",
    "# 17番目のCSVファイルを処理する\n",
    "test_df17, LABEL_t17, MAE_t17, FINAL_QP_t17 = process_train_csv_lists(test_QP4_QP4_GL)\n",
    "\n",
    "# 18番目のCSVファイルを処理する\n",
    "test_df18, LABEL_t18, MAE_t18, FINAL_QP_t18 = process_train_csv_lists(test_QP12_QP12_GL)\n",
    "\n",
    "\n",
    "# 19番目のCSVファイルを処理する\n",
    "test_df19, LABEL_t19, MAE_t19, FINAL_QP_t19 = process_train_csv_lists(test_QP2_QP4_GG)\n",
    "\n",
    "# 20番目のCSVファイルを処理する\n",
    "test_df20, LABEL_t20, MAE_t20, FINAL_QP_t20 = process_train_csv_lists(test_QP2_QP12_GG)\n",
    "\n",
    "# 21番目のCSVファイルを処理する\n",
    "test_df21, LABEL_t21, MAE_t21, FINAL_QP_t21 = process_train_csv_lists(test_QP4_QP12_GG)\n",
    "\n",
    "# 22番目のCSVファイルを処理する\n",
    "test_df22, LABEL_t22, MAE_t22, FINAL_QP_t22 = process_train_csv_lists(test_QP2_QP4_LG)\n",
    "\n",
    "# 23番目のCSVファイルを処理する\n",
    "test_df23, LABEL_t23, MAE_t23, FINAL_QP_t23 = process_train_csv_lists(test_QP2_QP12_LG)\n",
    "\n",
    "# 24番目のCSVファイルを処理する\n",
    "test_df24, LABEL_t24, MAE_t24, FINAL_QP_t24 = process_train_csv_lists(test_QP4_QP12_LG)\n",
    "\n",
    "# 25番目のCSVファイルを処理する\n",
    "test_df25, LABEL_t25, MAE_t25, FINAL_QP_t25 = process_train_csv_lists(test_QP2_QP4_GL)\n",
    "\n",
    "# 26番目のCSVファイルを処理する\n",
    "test_df26, LABEL_t26, MAE_t26, FINAL_QP_t26 = process_train_csv_lists(test_QP2_QP12_GL)\n",
    "\n",
    "# 27番目のCSVファイルを処理する\n",
    "test_df27, LABEL_t27, MAE_t27, FINAL_QP_t27 = process_train_csv_lists(test_QP4_QP12_GL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0 CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0 CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         32      0  27904   6880  12388  12828      0  28160   6704  12028  13108  11943   7080  2397   6547   1338    564   1724    636  12298   7861  2860   7373   1632    556   1674    482   7152  2728   3436   1836    872  43976   7204  2388   3436   1812    960  44200  0.000197  0.002844    0.00047  0.221359  0.200779\n",
      "1         20      0   1536   5712  18140  34612      0   1472   5792  17344  35392   9688   6915  1557   3428   3455    851   2555   1732   9950   7390  1616   3319   3424    849   2512   1769  14188  6596   7088   4356   1744  26028  13716  7340   7300   4172   1564  25908  0.000476  0.000529   0.001074  0.013491   0.00733\n",
      "2         16      0   5312   5072  18776  30840      0   5440   4800  18364  31396   7013   4852  1130   1434   1026   6074   5856   3584   7099   5425  1113   1488    981   5826   5888   3583   9600  4684   3128   7288   1168  34132   9120  4844   3224   7688   1172  33952   0.00031  0.001172    0.00046  0.048955  0.024087\n",
      "3         10      0      0    320   9044  50636      0      0    288   7636  52076   6351   3871  5236   5122   2136   2401   4084   1377   6345   4049  5215   5144   2075   2460   4056   1398  15368  8008  10176   6720   1180  18548  15508  8320  10476   6876   1136  17684  0.002405  0.000179   0.000571  0.034019  0.025504\n",
      "4         39      0  33536  21184   4588    692      0  34176  21168   4060    596  16963   6144   439   1025    797    454   2627    241  17062   6871   623    902    781    351   3163    319   1840   464    668    532    692  55804   1912   164    544    424    544  56412  0.000772  0.004139   0.003846  0.162275  0.169914\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "475       32      0  43584   6672   4940   4804      0  43648   6688   4796   4868   6474   4954  3340  14287   2239    293    883    127   6743   5423  3552  14489   1878    240    773    185   2040   532   1684    372    144  55228   1848   340   1504    368    112  55828  0.000044   0.00254   0.001224  0.188089  0.177138\n",
      "476       32      0  44928   9680   4700    692      0  45184   9600   4576    640  16736   4985   794   1034    756    635   1958    853  16793   5724   685   1390    923    648   2023    764   7748  1332   1904   2256    712  46048   7536  1168   1920   1844    720  46812   0.00008  0.003795   0.001054  0.179051  0.169702\n",
      "477       45      0  31744  22224   5632    400      0  32704  21920   5080    296  18675   7273   913    909    763    863   2564    777  20002   7011   877   1045    800    831   2898    748   6788  2516   2444   2568   1332  44352   4844  2104   1796   1812   1336  48108  0.001028  0.001516   0.012984  0.139668  0.127654\n",
      "478       42      0  28672  21536   8760   1032      0  29184  21712   8220    884  19168   6546  1076   1057   1011    851   2812    793  19380   7020  1029   1123    913    731   2705    741   6872  2696   1844   2400    708  45480   5752  2060   1564   2272    628  47724  0.000572  0.001118   0.004627  0.153567   0.14697\n",
      "479       42      0  48256   8272   2764    708      0  49088   7584   2636    692  25662  17834  1559   2610   1633    186   2252     66  25411  17538  1045   2978   2407    273   2273    127    184   208    420    144      8  59036    180     0    132    136      4  59548  0.000677  0.005985   0.018489  0.180633  0.197168\n",
      "\n",
      "[480 rows x 44 columns]\n",
      "   FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         2      0     64   2944  24068  32924      0      0   2544  23268  34188  10177   7936  3797   2715   3118   1082   1296    947  10408   7877  3775   2631   3061   1139   1298    975  16032   8248  12060   9220   3268  11172  16388   8948  12644   9420   2984   9616  0.004575  0.000207   0.003009  0.924978  0.842385\n",
      "1         2      0      0   1104  11216  47680      0      0   1168   9200  49632   7899   6087  1403   1924   1420   1474   2027   1390   7962   6410  1445   2014   1342   1531   1957   1382  18960  11312   8168  10072   1704   9784  18912  12848   8108  10164   1432   8536  0.004116  0.000511   0.003463  0.883246  0.868847\n",
      "2         2      0     64  15520  24580  19836      0      0  16176  23040  20784  14064  13897  1872   1866   1230   1413   1427    882  14514  14160  1812   1836   1132   1370   1526    851  16852   8916  10672  11052   4604   7904  16932   9632  10680  11148   4260   7348  0.004885  0.000417   0.001032  0.995059  0.995078\n",
      "3         2      0      0   8544  22528  28928      0      0   8752  21452  29796   6998   6662   764    733    429   1133   1011    683   7227   6606   746    788    408   1216    946    697  16208   9848   7428  10532   1772  14212  16180  10384   7904  10608   1764  13160  0.000697  0.000571   0.001166  0.901612  0.888913\n",
      "4         2      0     64   9792  25976  24168      0     64   9888  25060  24988  10079   7951  2317   3003   1909   4840   4515   1856  10466   8279  2278   3037   1735   4857   4486   1823  13724   8968   9400  13144   2600  12164  13252   9272   9688  13940   2264  11584   0.00051  0.000547   0.001315  0.951786  0.946901\n",
      "5         2      0      0  11312  28728  19960      0      0  11168  28216  20616   8755   3762  3844   3525   1650   2255   2539   1677   9252   3972  3846   3459   1775   2344   2504   1547  14160  10120   9576  11452   1764  12928  13816  10816   9852  11608   1676  12232  0.000268  0.000829   0.000896   0.95574  0.955028\n",
      "6         2      0   7936  19504  20592  11968      0   8128  19808  19504  12560  10886   9614   408    608    350   7257   2011    774  11257   9642   407    657    532   7194   2343   1166  21652   8408   5800  12284   3040   8816  21004   8596   6020  12600   3264   8516   0.00081  0.003692   0.000552   0.96969  0.991705\n",
      "7         2      0    128  14432  24588  20852      0    192  14480  23264  22064   9913   7935  5799   5472   5174    252    378    226  10541   7905  5530   5722   5086    235    407    248  16104   8648  14508   6604   3240  10896  15276   9292  15088   6824   3072  10448  0.001383  0.000872   0.001229  0.900866  0.886457\n",
      "8         2      0   1152  18464  26224  14160      0   1280  17920  25848  14952  13479   8664  2040   3129   1337   2020   3674   1755  13754   8805  1931   3022   1198   1883   4026   1869  17028   8396   8180  11420   3768  11208  16596   8744   8648  11564   3332  11116  0.000648  0.001073   0.000902  0.863138  0.790621\n",
      "9         2      0    128  11248  23804  24820      0     64  11168  22352  26416  12553   6541  3828   2927   2697    581    904    652  13196   6724  3575   3132   2581    699    804    559  16084   9292  13072   8668   3152   9732  15700   9532  13992   9020   3180   8576  0.002001  0.001789   0.002005  0.946705  0.947938\n",
      "10        2      0    512  22480  23848  13160      0    384  23232  22836  13548  11188   6859  1793   2076   2010   1462   2428   1220  11597   6990  1949   1948   1799   1458   2422   1213  16856  10624   9420  10372   3008   9720  16364  10752   9604  11752   2804   8724  0.000988  0.000999   0.002605  0.928712  0.934809\n",
      "11        2      0    128  18688  24260  16924      0     64  19456  23068  17412   9793   6391  3486   4290   2479   1765   2043   1248  10169   6618  3401   4374   2419   1743   1989   1157  17960   9544   9060   9544   2944  10948  17276  10204   9276  10084   2664  10496  0.001283  0.000499   0.001271  0.969706  0.963012\n",
      "12        2      0     64   6608  22648  30680      0     64   6592  21460  31884  11104   8485  2171   1915   1798   1746   1748   1569  11419   8767  2091   1916   1816   1800   1785   1475  15776   8716   9172  11284   3432  11620  15740   9216   9860  11956   3160  10068  0.000922  0.000331   0.003049  0.967025  0.951641\n",
      "13        2      0   8192   6016  21068  24724      0   8192   5632  20380  25796  14116   5422  2809   3559   1909   2163   1480   1078  13331   5290  2875   3501   1977   2145   1679   1097  18208   7328   9116   8012   8072   9264  19144   8180   9728   8112   6828   8008  0.000781  0.001009   0.004837  0.992415   0.99453\n",
      "14        2      0    128  21792  21356  16724      0    192  21264  21168  17376  12532   4840  1880   1958   1795   1237   1517   1304  13038   5322  1847   1808   1573   1216   1280   1300  16568  11084   9220   9796   2424  10908  16228  11636  10288   9976   2164   9708  0.000529  0.002688   0.002704  0.847781  0.775031\n",
      "15        2      0     64  15984  28416  15536      0     64  15776  28352  15808  12014   4576  1464   1713   1471   1613   1660   1408  12579   4823  1309   1687   1243   1517   1819   1374  15968   9328   8364  10428   2836  13076  15696  10252   8812  10820   2716  11704  0.000063  0.002123   0.002402  0.938239  0.806896\n",
      "16        2      0      0   5264  26080  28656      0      0   4896  24876  30228   6712   4309  5715   8220   4337   1570   3907   1578   6916   4312  5859   8163   4027   1516   4083   1559  14092   8760  10568   9580   2280  14720  13820   9740  11376   9848   1940  13276  0.001396   0.00059   0.003179  0.965555  0.961922\n",
      "17        2      0     64  12416  27196  20324      0      0  11376  27340  21284  11465   7865  1237   1667    800   2600   3069   2186  11821   8249  1232   1709    775   2530   3086   2099  16032   9016   7572  13276   2800  11304  15860   9364   8016  13908   2668  10184  0.004597  0.000396    0.00162   0.93812  0.927244\n",
      "18        2      0    128   5152  10656  44064      0     64   5008   9352  45576   8413   6691  1959   1872   1603   1253   1407   1146   8510   7076  1895   1981   1485   1232   1344   1175  17764  10084   8732   8984   2320  12116  17364  11324   9344   9296   2076  10596  0.002313  0.000795   0.003647  0.899797  0.866267\n",
      "19        2      0     64   7216  21452  31268      0      0   7120  21136  31744  11208   9628  1194   1147   1068    906   1015    906  11491   9231  1263   1321   1069   1074    940    886  15468   9536   9028  10228   4000  11740  14656  10772   9576  10944   3328  10724  0.003565  0.001554   0.004102  0.962458   0.90149\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)\n",
    "print(test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, LABEL, MAE, FINAL_QP, X_train_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    # X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "for i in range(1, 28):\n",
    "    globals()[f'X_test_list{i}'] = []\n",
    "    globals()[f'MAE_list_t{i}'] = []\n",
    "    globals()[f'FINAL_QP_list_t{i}'] = []\n",
    "    globals()[f'Y_test_list{i}'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, LABEL1, MAE1, FINAL_QP1, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, LABEL2, MAE2, FINAL_QP2, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, LABEL3, MAE3, FINAL_QP3, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, LABEL4, MAE4, FINAL_QP4, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, LABEL5, MAE5, FINAL_QP5, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, LABEL6, MAE6, FINAL_QP6, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, LABEL7, MAE7, FINAL_QP7, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, LABEL8, MAE8, FINAL_QP8, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, LABEL9, MAE9, FINAL_QP9, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df10, LABEL10, MAE10, FINAL_QP10, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 28):\n",
    "    eval(f'append_results_to_lists(test_df{i}, LABEL_t{i}, MAE_t{i}, FINAL_QP_t{i}, X_test_list{i}, MAE_list_t{i}, FINAL_QP_list_t{i}, Y_test_list{i})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "4320\n",
      "480\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "4320\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "# C_values = {'C': [0.01]}\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# データフレームを初期化\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# 1から106までの列名を作成し、データフレームに追加\n",
    "columns = []\n",
    "for i in range(1, 28):\n",
    "    columns.extend([\n",
    "        f'C_RBF{i}', f'Score_RBF{i}', f'tnr_rbf{i}', f'tpr_rbf{i}', f'AUC_RBF{i}',\n",
    "        f'C_LINEAR{i}', f'Score_LINEAR{i}', f'tnr_linear{i}', f'tpr_linear{i}', f'AUC_LINEAR{i}',\n",
    "        f'Threshold{i}', f'Score_old{i}', f'tnr_old{i}', f'tpr_old{i}', f'AUC_old{i}'\n",
    "    ])\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "        \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    print(len(Y_train))\n",
    "    print(len(Y_val))\n",
    "    \n",
    "    # リストの作成（1から106まで）\n",
    "    for i in range(1, 28):\n",
    "        globals()[f'test_data{i}'] = [item for data in globals()[f'X_test_list{i}'] for item in data]\n",
    "        globals()[f'test_label{i}'] = [item for data in globals()[f'Y_test_list{i}'] for item in data]\n",
    "        globals()[f'MAE_data{i}'] = [item for data in globals()[f'MAE_list_t{i}'] for item in data]\n",
    "        globals()[f'FINAL_QP_data{i}'] = [item for data in globals()[f'FINAL_QP_list_t{i}'] for item in data]\n",
    "\n",
    "        globals()[f'best_threshold{i}'] = 0\n",
    "        globals()[f'best_accuracy{i}'] = 0\n",
    "        globals()[f'best_predicted_labels{i}'] = []\n",
    "        globals()[f'best_ground_truth_labels{i}'] = []\n",
    "        globals()[f'tnr_old{i}'] = 0\n",
    "        globals()[f'tpr_old{i}'] = 0\n",
    "        \n",
    "        for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "            test_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(20)])\n",
    "            predicted_labels = test_old.astype(int)\n",
    "            ground_truth_labels = np.array(globals()[f'test_label{i}'])\n",
    "            accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "            if accuracy > globals()[f'best_accuracy{i}']:\n",
    "                globals()[f'best_accuracy{i}'] = accuracy\n",
    "                globals()[f'best_threshold{i}'] = threshold\n",
    "                globals()[f'best_predicted_labels{i}'] = predicted_labels\n",
    "                globals()[f'best_ground_truth_labels{i}'] = ground_truth_labels\n",
    "\n",
    "\n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value, probability=True)\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value, probability=True)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "    \n",
    "    fold_results = {}\n",
    "    for i in range(1, 28):\n",
    "        # RBFモデルの評価\n",
    "        predictions_RBF = best_svm_model_RBF.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_RBF = best_svm_model_RBF.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_RBF = accuracy_score(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'accuracy_RBF{i}'] = accuracy_RBF\n",
    "        report_RBF = classification_report(globals()[f'test_label{i}'], predictions_RBF, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'tnr_rbf{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_rbf{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_rbf, tpr_rbf, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_RBF)\n",
    "        auc_rbf = auc(fpr_rbf, tpr_rbf)\n",
    "        globals()[f'auc_rbf{i}'] = auc_rbf\n",
    "        # print(report_RBF)\n",
    "\n",
    "        # LINEARモデルの評価\n",
    "        predictions_LINEAR = best_svm_model_LINEAR.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_LINEAR = best_svm_model_LINEAR.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_LINEAR = accuracy_score(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'accuracy_LINEAR{i}'] = accuracy_LINEAR\n",
    "        report_LINEAR = classification_report(globals()[f'test_label{i}'], predictions_LINEAR, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'tnr_linear{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_linear{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_linear, tpr_linear, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_LINEAR)\n",
    "        auc_linear = auc(fpr_linear, tpr_linear)\n",
    "        globals()[f'auc_linear{i}'] = auc_linear\n",
    "        # print(report_LINEAR)\n",
    "\n",
    "        # Old modelの評価\n",
    "        thresholds = np.arange(0.00, 1.01, 0.01)\n",
    "        tpr_old_list = []\n",
    "        fpr_old_list = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(20)])\n",
    "            tn, fp, fn, tp = confusion_matrix(globals()[f'test_label{i}'], predicted_labels_old).ravel()\n",
    "            tpr_old = tp / (tp + fn)\n",
    "            fpr_old = fp / (fp + tn)\n",
    "            tpr_old_list.append(tpr_old)\n",
    "            fpr_old_list.append(fpr_old)\n",
    "        \n",
    "        auc_old = auc(fpr_old_list, tpr_old_list)\n",
    "        globals()[f'auc_old{i}'] = auc_old\n",
    "\n",
    "        # fold_resultsに保存\n",
    "        fold_results[f'C_RBF{i}'] = best_c_value_RBF\n",
    "        fold_results[f'Score_RBF{i}'] = globals()[f'accuracy_RBF{i}']\n",
    "        fold_results[f'tnr_rbf{i}'] = globals()[f'tnr_rbf{i}']\n",
    "        fold_results[f'tpr_rbf{i}'] = globals()[f'tpr_rbf{i}']\n",
    "        fold_results[f'AUC_RBF{i}'] = globals()[f'auc_rbf{i}']\n",
    "\n",
    "        fold_results[f'C_LINEAR{i}'] = best_c_value_LINEAR\n",
    "        fold_results[f'Score_LINEAR{i}'] = globals()[f'accuracy_LINEAR{i}']\n",
    "        fold_results[f'tnr_linear{i}'] = globals()[f'tnr_linear{i}']\n",
    "        fold_results[f'tpr_linear{i}'] = globals()[f'tpr_linear{i}']\n",
    "        fold_results[f'AUC_LINEAR{i}'] = globals()[f'auc_linear{i}']\n",
    "\n",
    "        fold_results[f'Threshold{i}'] = globals()[f'best_threshold{i}']\n",
    "        fold_results[f'Score_old{i}'] = globals()[f'best_accuracy{i}']\n",
    "        fold_results[f'tnr_old{i}'] = globals()[f'tnr_old{i}']\n",
    "        fold_results[f'tpr_old{i}'] = globals()[f'tpr_old{i}']\n",
    "        fold_results[f'AUC_old{i}'] = globals()[f'auc_old{i}']\n",
    "\n",
    "    # 結果をデータフレームに追加\n",
    "    results = pd.concat([results, pd.DataFrame(fold_results, index=[fold])], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各統計情報を100倍して小数点第2位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': [f'RBF{i}' for i in range(1, 28)] + [f'LINEAR{i}' for i in range(1, 28)] + [f'OLD{i}' for i in range(1, 28)],\n",
    "    'Average TNR': [\n",
    "        round(results[f'tnr_rbf{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tnr_linear{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tnr_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average TPR': [\n",
    "        round(results[f'tpr_rbf{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tpr_linear{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tpr_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Standard Deviation': [\n",
    "        round(results[f'Score_RBF{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Max Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Min Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'AUC STD': [\n",
    "        round(results[f'AUC_RBF{i}'].std(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].std(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].std(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Max AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].max(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].max(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].max(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Min AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].min(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].min(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].min(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)\n",
    "\n",
    "\n",
    "\n",
    "# 関数を定義して、各セグメントの統計情報を計算\n",
    "def calculate_statistics(segment, prefix):\n",
    "    # モデル番号を抽出してフラットなリストに変換\n",
    "    model_numbers = statistics_df['Model'].str.extract(r'(\\d+)').astype(int)[0]\n",
    "    is_in_segment = model_numbers.isin(segment)\n",
    "    is_correct_prefix = statistics_df['Model'].str.startswith(prefix)\n",
    "    \n",
    "    tnr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TNR'].mean(), 2)\n",
    "    tpr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TPR'].mean(), 2)\n",
    "    acc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average Test Score'].mean(), 2)\n",
    "    acc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Standard Deviation'].std(), 2)\n",
    "    \n",
    "    acc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max Test Score'].max(), 2)\n",
    "    acc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min Test Score'].min(), 2)\n",
    "\n",
    "    auc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average AUC'].mean(), 2)\n",
    "    auc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'AUC STD'].std(), 2)\n",
    "    auc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max AUC'].max(), 2)\n",
    "    auc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min AUC'].min(), 2)\n",
    "    \n",
    "    return tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min\n",
    "\n",
    "# セグメントを定義\n",
    "segments = {\n",
    "    '1_10': list(range(1, 10)),\n",
    "    '10_19': list(range(10, 19)),\n",
    "    '19_28': list(range(19, 28))\n",
    "}\n",
    "\n",
    "# 結果を保存するリスト\n",
    "results_summary = []\n",
    "\n",
    "# 統計情報を計算して表示\n",
    "for model in ['RBF', 'LINEAR', 'OLD']:\n",
    "    for segment_name, segment in segments.items():\n",
    "        tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min = calculate_statistics(segment, model)\n",
    "        results_summary.append({\n",
    "            'Model': f'{model}_{segment_name}',\n",
    "            'Average TNR': tnr_mean,\n",
    "            'Average TPR': tpr_mean,\n",
    "            'Average Test Score': acc_mean,\n",
    "            'Test Score STD': acc_std,\n",
    "            'Test Score MAX': acc_max,\n",
    "            'Test Score MIN': acc_min,\n",
    "            'Average AUC': auc_mean,\n",
    "            'AUC STD': auc_std,\n",
    "            'Max AUC': auc_max,\n",
    "            'Min AUC': auc_min\n",
    "        })\n",
    "\n",
    "# DataFrameに変換\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# 表示\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['C_RBF1'])\n",
    "print(results['C_LINEAR1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
