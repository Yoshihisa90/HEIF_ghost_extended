{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['74', '138', '109', '56', '211', '104', '264', '95', '277', '217', '39', '260', '276', '203', '210', '205', '88', '263', '247', '262', '91', '249', '197', '102', '134', '23'], ['100', '108', '28', '86', '101', '219', '297', '4', '255', '273', '284', '150', '60', '129', '110', '14', '221', '290', '234', '251', '10', '201', '114', '257', '111', '115'], ['93', '279', '229', '128', '220', '283', '103', '246', '98', '57', '51', '214', '46', '298', '294', '142', '268', '139', '8', '84', '58', '59', '67', '195', '136', '94'], ['213', '117', '208', '258', '237', '236', '218', '122', '33', '69', '80', '215', '149', '127', '270', '120', '281', '76', '89', '106', '21', '92', '16', '75', '200', '223'], ['265', '87', '291', '144', '226', '245', '6', '24', '235', '45', '44', '17', '34', '287', '97', '107', '13', '54', '140', '35', '42', '141', '71', '38', '228', '81'], ['272', '209', '222', '50', '266', '27', '64', '256', '296', '145', '259', '61', '146', '193', '41', '239', '12', '65', '231', '131', '15', '31', '130', '36', '82', '32'], ['118', '148', '53', '125', '198', '19', '202', '212', '119', '121', '252', '90', '63', '47', '224', '271', '286', '300', '1', '26', '267', '194', '105', '269', '289', '2'], ['48', '196', '147', '135', '241', '248', '206', '285', '261', '99', '227', '5', '250', '79', '143', '295', '293', '18', '116', '77', '278', '52', '282', '238', '225', '299'], ['280', '275', '49', '254', '253', '73', '126', '232', '22', '199', '243', '55', '137', '191', '37', '240', '62', '40', '204', '83', '29', '96', '68', '3', '72', '288'], ['242', '30', '25', '192', '113', '20', '11', '244', '78', '274', '133', '123', '43', '207', '112', '230', '216', '66', '85', '233', '70', '124', '7', '132', '292', '9']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1482\n",
      "CSV List 2A: 1482\n",
      "CSV List 3A: 1482\n",
      "CSV List 4A: 1482\n",
      "CSV List 5A: 1482\n",
      "CSV List 6A: 1482\n",
      "CSV List 7A: 1482\n",
      "CSV List 8A: 1482\n",
      "CSV List 9A: 1482\n",
      "CSV List 10A: 1482\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1482\n",
      "CSV List 2A: 1482\n",
      "CSV List 3A: 1482\n",
      "CSV List 4A: 1482\n",
      "CSV List 5A: 1482\n",
      "CSV List 6A: 1482\n",
      "CSV List 7A: 1482\n",
      "CSV List 8A: 1482\n",
      "CSV List 9A: 1482\n",
      "CSV List 10A: 1482\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1014\n",
      "CSV List 2A: 1014\n",
      "CSV List 3A: 1014\n",
      "CSV List 4A: 1014\n",
      "CSV List 5A: 1014\n",
      "CSV List 6A: 1014\n",
      "CSV List 7A: 1014\n",
      "CSV List 8A: 1014\n",
      "CSV List 9A: 1014\n",
      "CSV List 10A: 1014\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1014\n",
      "CSV List 2A: 1014\n",
      "CSV List 3A: 1014\n",
      "CSV List 4A: 1014\n",
      "CSV List 5A: 1014\n",
      "CSV List 6A: 1014\n",
      "CSV List 7A: 1014\n",
      "CSV List 8A: 1014\n",
      "CSV List 9A: 1014\n",
      "CSV List 10A: 1014\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1482\n",
      "PKL List 2A: 1482\n",
      "PKL List 3A: 1482\n",
      "PKL List 4A: 1482\n",
      "PKL List 5A: 1482\n",
      "PKL List 6A: 1482\n",
      "PKL List 7A: 1482\n",
      "PKL List 8A: 1482\n",
      "PKL List 9A: 1482\n",
      "PKL List 10A: 1482\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1482\n",
      "PKL List 2A: 1482\n",
      "PKL List 3A: 1482\n",
      "PKL List 4A: 1482\n",
      "PKL List 5A: 1482\n",
      "PKL List 6A: 1482\n",
      "PKL List 7A: 1482\n",
      "PKL List 8A: 1482\n",
      "PKL List 9A: 1482\n",
      "PKL List 10A: 1482\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1014\n",
      "PKL List 2A: 1014\n",
      "PKL List 3A: 1014\n",
      "PKL List 4A: 1014\n",
      "PKL List 5A: 1014\n",
      "PKL List 6A: 1014\n",
      "PKL List 7A: 1014\n",
      "PKL List 8A: 1014\n",
      "PKL List 9A: 1014\n",
      "PKL List 10A: 1014\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1014\n",
      "PKL List 2A: 1014\n",
      "PKL List 3A: 1014\n",
      "PKL List 4A: 1014\n",
      "PKL List 5A: 1014\n",
      "PKL List 6A: 1014\n",
      "PKL List 7A: 1014\n",
      "PKL List 8A: 1014\n",
      "PKL List 9A: 1014\n",
      "PKL List 10A: 1014\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "\n",
    "train_list6 = [\"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\"]\n",
    "\n",
    "train_list7 = [\"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\", \"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\"]\n",
    "\n",
    "train_list8 = [\"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\", \"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\"]\n",
    "\n",
    "train_list9 = [\"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\", \"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\"]\n",
    "\n",
    "train_list10 = [\"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+26] for i in range(0, len(combined_train_list), 26)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "\n",
    "single_csv1 = random.sample(single_csv1, 240)\n",
    "single_csv2 = random.sample(single_csv2, 240)\n",
    "single_csv3 = random.sample(single_csv3, 240)\n",
    "single_csv4 = random.sample(single_csv4, 240)\n",
    "single_csv5 = random.sample(single_csv5, 240)\n",
    "single_csv6 = random.sample(single_csv6, 240)\n",
    "single_csv7 = random.sample(single_csv7, 240)\n",
    "single_csv8 = random.sample(single_csv8, 240)\n",
    "single_csv9 = random.sample(single_csv9, 240)\n",
    "single_csv10 = random.sample(single_csv10, 240)\n",
    "print(len(single_csv1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482\n",
      "\n",
      "double images train by QP1>QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 80)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 80)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 80)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 80)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 80)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 80)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 80)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 80)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 80)\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 80)\n",
    "# second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1>QP2: ', len(second_largeQP1_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "\n",
      "double images train by QP1=QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "print(len(second_sameQP_csv10))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 80)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 80)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 80)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 80)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 80)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 80)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 80)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 80)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 80)\n",
    "second_sameQP_csv10 = random.sample(second_sameQP_csv10, 80)\n",
    "print('\\ndouble images train by QP1=QP2: ',len(second_sameQP_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "\n",
      "double images train by QP1<QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "print(len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 80)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 80)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 80)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 80)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 80)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 80)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 80)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 80)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 80)\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 80)\n",
    "# second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1<QP2: ', len(second_largeQP2_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2 = [\"_1stQP2_\"]\n",
    "QP4 = [\"_1stQP4_\"]\n",
    "QP12 = [\"_1stQP12_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP4_QP2 = [\"_1stQP4_2ndQP2_\"]\n",
    "QP12_QP2 = [\"_1stQP12_2ndQP2_\"]\n",
    "QP12_QP4 = [\"_1stQP12_2ndQP4_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2_QP2 = [\"_1stQP2_2ndQP2\"]\n",
    "QP4_QP4 = [\"_1stQP4_2ndQP4\"]\n",
    "QP12_QP12 = [\"_1stQP12_2ndQP12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2_QP4 = [\"_1stQP2_2ndQP4\"]\n",
    "QP2_QP12 = [\"_1stQP2_2ndQP12\"]\n",
    "QP4_QP12 = [\"_1stQP4_2ndQP12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath2 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath2, 'GIMP_csv')\n",
    "GIMP_path2 = os.path.join(rootpath2, 'GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_path1_csv = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_csv = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath2, 'LIBHEIF_csv')\n",
    "LIBHEIF_path2 = os.path.join(rootpath2, 'LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_path1_csv = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_csv = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath2, 'GIMP_GIMP_csv')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath2, 'GIMP_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_GIMP_path1_csv = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_csv = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath2, 'LIBHEIF_GIMP_csv')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath2, 'LIBHEIF_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_GIMP_path1_csv = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_csv = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath2, 'GIMP_LIBHEIF_csv')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath2, 'GIMP_LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_LIBHEIF_path1_csv = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_csv = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath2 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath2, 'GIMP_csv')\n",
    "GIMP_path2 = os.path.join(rootpath2, 'GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_path1_csv = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_csv = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath2, 'LIBHEIF_csv')\n",
    "LIBHEIF_path2 = os.path.join(rootpath2, 'LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_path1_csv = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_csv = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath2, 'GIMP_GIMP_csv')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath2, 'GIMP_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_GIMP_path1_csv = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_csv = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath2, 'LIBHEIF_GIMP_csv')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath2, 'LIBHEIF_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_GIMP_path1_csv = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_csv = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath2, 'GIMP_LIBHEIF_csv')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath2, 'GIMP_LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_LIBHEIF_path1_csv = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_csv = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath3 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/PKL/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath3, 'pkl_GIMP')\n",
    "GIMP_path2 = os.path.join(rootpath3, 'pkl_GIMP_RECOMPRESSED')\n",
    "\n",
    "GIMP_path1_pkl = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_pkl = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath3, 'pkl_LIBHEIF')\n",
    "LIBHEIF_path2 = os.path.join(rootpath3, 'pkl_LIBHEIF_RECOMPRESSED')\n",
    "\n",
    "LIBHEIF_path1_pkl = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_pkl = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath3, 'pkl_GIMP_GIMP')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath3, 'pkl_GIMP_GIMP_RECOMPRESSED')\n",
    "\n",
    "GIMP_GIMP_path1_pkl = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_pkl = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath3, 'pkl_LIBHEIF_GIMP')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath3, 'pkl_LIBHEIF_GIMP_RECOMPRESSED')\n",
    "\n",
    "LIBHEIF_GIMP_path1_pkl = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_pkl = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath3, 'pkl_GIMP_LIBHEIF')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath3, 'pkl_GIMP_LIBHEIF_RECOMPRESSED')\n",
    "\n",
    "GIMP_LIBHEIF_path1_pkl = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_pkl = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIMP_csv = list(zip(GIMP_path1_csv, GIMP_path1_pkl, GIMP_path2_csv, GIMP_path2_pkl))\n",
    "\n",
    "\n",
    "single_QP2_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP2)]\n",
    "single_QP4_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP4)]\n",
    "single_QP12_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP12)]\n",
    "\n",
    "single_QP2_GIMP = random.sample(single_QP2_GIMP, 10)\n",
    "single_QP4_GIMP = random.sample(single_QP4_GIMP, 10)\n",
    "single_QP12_GIMP = random.sample(single_QP12_GIMP, 10)\n",
    "\n",
    "LIBHEIF_csv = list(zip(LIBHEIF_path1_csv, LIBHEIF_path1_pkl, LIBHEIF_path2_csv, LIBHEIF_path2_pkl))\n",
    "# LIBHEIF_csv1 = random.sample(LIBHEIF_csv, 10)\n",
    "\n",
    "single_QP2_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP2)]\n",
    "single_QP4_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP4)]\n",
    "single_QP12_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP12)]\n",
    "\n",
    "single_QP2_LIBHEIF = random.sample(single_QP2_LIBHEIF, 10)\n",
    "single_QP4_LIBHEIF = random.sample(single_QP4_LIBHEIF, 10)\n",
    "single_QP12_LIBHEIF = random.sample(single_QP12_LIBHEIF, 10)\n",
    "\n",
    "\n",
    "GIMP_GIMP_csv = list(zip(GIMP_GIMP_path1_csv, GIMP_GIMP_path1_pkl, GIMP_GIMP_path2_csv, GIMP_GIMP_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "LIBHEIF_GIMP_csv = list(zip(LIBHEIF_GIMP_path1_csv, LIBHEIF_GIMP_path1_pkl, LIBHEIF_GIMP_path2_csv, LIBHEIF_GIMP_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "GIMP_LIBHEIF_csv = list(zip(GIMP_LIBHEIF_path1_csv, GIMP_LIBHEIF_path1_pkl, GIMP_LIBHEIF_path2_csv, GIMP_LIBHEIF_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "# print(len(single_QP2_GIMP))\n",
    "# print(len(single_QP2_LIBHEIF))\n",
    "# print(len(second_QP4_QP2_GG))\n",
    "# print(len(second_QP4_QP2_LG))\n",
    "# print(len(second_QP4_QP2_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  480\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv10 + second_largeQP2_csv10\n",
    "print(\"train_csv_list: \", len(train_csv_list10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP12_QP4_GG:  20\n",
      "test_QP12_QP4_LG:  20\n",
      "test_QP12_QP4_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP4_QP2_GG = second_QP4_QP2_GG + single_QP2_GIMP\n",
    "test_QP12_QP2_GG = second_QP12_QP2_GG + single_QP2_GIMP\n",
    "test_QP12_QP4_GG = second_QP12_QP4_GG + single_QP4_GIMP\n",
    "\n",
    "test_QP4_QP2_LG = second_QP4_QP2_LG + single_QP2_GIMP\n",
    "test_QP12_QP2_LG = second_QP12_QP2_LG + single_QP2_GIMP\n",
    "test_QP12_QP4_LG = second_QP12_QP4_LG + single_QP4_GIMP\n",
    "\n",
    "test_QP4_QP2_GL = second_QP4_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP12_QP2_GL = second_QP12_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP12_QP4_GL = second_QP12_QP4_GL + single_QP4_LIBHEIF\n",
    "\n",
    "print('test_QP12_QP4_GG: ', len(test_QP12_QP4_GG))\n",
    "print('test_QP12_QP4_LG: ', len(test_QP12_QP4_LG))\n",
    "print('test_QP12_QP4_GL: ', len(test_QP12_QP4_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP12_QP12_GG:  20\n",
      "test_QP12_QP12_LG:  20\n",
      "test_QP12_QP12_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP2_QP2_GG = second_QP2_QP2_GG + single_QP2_GIMP\n",
    "test_QP4_QP4_GG = second_QP4_QP4_GG + single_QP4_GIMP\n",
    "test_QP12_QP12_GG = second_QP12_QP12_GG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP2_LG = second_QP2_QP2_LG + single_QP2_GIMP\n",
    "test_QP4_QP4_LG = second_QP4_QP4_LG + single_QP4_GIMP\n",
    "test_QP12_QP12_LG = second_QP12_QP12_LG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP2_GL = second_QP2_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP4_QP4_GL = second_QP4_QP4_GL + single_QP4_LIBHEIF\n",
    "test_QP12_QP12_GL = second_QP12_QP12_GL + single_QP12_LIBHEIF\n",
    "\n",
    "print('test_QP12_QP12_GG: ', len(test_QP12_QP12_GG))\n",
    "print('test_QP12_QP12_LG: ', len(test_QP12_QP12_LG))\n",
    "print('test_QP12_QP12_GL: ', len(test_QP12_QP12_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP4_QP12_GG:  20\n",
      "test_QP4_QP12_LG:  20\n",
      "test_QP4_QP12_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP2_QP4_GG = second_QP2_QP4_GG + single_QP4_GIMP\n",
    "test_QP2_QP12_GG = second_QP2_QP12_GG + single_QP12_GIMP\n",
    "test_QP4_QP12_GG = second_QP4_QP12_GG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP4_LG = second_QP2_QP4_LG + single_QP4_GIMP\n",
    "test_QP2_QP12_LG = second_QP2_QP12_LG + single_QP12_GIMP\n",
    "test_QP4_QP12_LG = second_QP4_QP12_LG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP4_GL = second_QP2_QP4_GL + single_QP4_LIBHEIF\n",
    "test_QP2_QP12_GL = second_QP2_QP12_GL + single_QP12_LIBHEIF\n",
    "test_QP4_QP12_GL = second_QP4_QP12_GL + single_QP12_LIBHEIF\n",
    "\n",
    "print('test_QP4_QP12_GG: ', len(test_QP4_QP12_GG))\n",
    "print('test_QP4_QP12_LG: ', len(test_QP4_QP12_LG))\n",
    "print('test_QP4_QP12_GL: ', len(test_QP4_QP12_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_OG = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1番目のCSVファイルを処理する\n",
    "test_df1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_QP4_QP2_GG)\n",
    "\n",
    "# 2番目のCSVファイルを処理する\n",
    "test_df2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_QP12_QP2_GG)\n",
    "\n",
    "# 3番目のCSVファイルを処理する\n",
    "test_df3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_QP12_QP4_GG)\n",
    "\n",
    "# 4番目のCSVファイルを処理する\n",
    "test_df4, LABEL_t4, MAE_t4, FINAL_QP_t4 = process_train_csv_lists(test_QP4_QP2_LG)\n",
    "\n",
    "# 5番目のCSVファイルを処理する\n",
    "test_df5, LABEL_t5, MAE_t5, FINAL_QP_t5 = process_train_csv_lists(test_QP12_QP2_LG)\n",
    "\n",
    "# 6番目のCSVファイルを処理する\n",
    "test_df6, LABEL_t6, MAE_t6, FINAL_QP_t6 = process_train_csv_lists(test_QP12_QP4_LG)\n",
    "\n",
    "# 7番目のCSVファイルを処理する\n",
    "test_df7, LABEL_t7, MAE_t7, FINAL_QP_t7 = process_train_csv_lists(test_QP4_QP2_GL)\n",
    "\n",
    "# 8番目のCSVファイルを処理する\n",
    "test_df8, LABEL_t8, MAE_t8, FINAL_QP_t8 = process_train_csv_lists(test_QP12_QP2_GL)\n",
    "\n",
    "# 9番目のCSVファイルを処理する\n",
    "test_df9, LABEL_t9, MAE_t9, FINAL_QP_t9 = process_train_csv_lists(test_QP12_QP4_GL)\n",
    "\n",
    "\n",
    "# 10番目のCSVファイルを処理する\n",
    "test_df10, LABEL_t10, MAE_t10, FINAL_QP_t10 = process_train_csv_lists(test_QP2_QP2_GG)\n",
    "\n",
    "# 11番目のCSVファイルを処理する\n",
    "test_df11, LABEL_t11, MAE_t11, FINAL_QP_t11 = process_train_csv_lists(test_QP4_QP4_GG)\n",
    "\n",
    "# 12番目のCSVファイルを処理する\n",
    "test_df12, LABEL_t12, MAE_t12, FINAL_QP_t12 = process_train_csv_lists(test_QP12_QP12_GG)\n",
    "\n",
    "# 13番目のCSVファイルを処理する\n",
    "test_df13, LABEL_t13, MAE_t13, FINAL_QP_t13 = process_train_csv_lists(test_QP2_QP2_LG)\n",
    "\n",
    "# 14番目のCSVファイルを処理する\n",
    "test_df14, LABEL_t14, MAE_t14, FINAL_QP_t14 = process_train_csv_lists(test_QP4_QP4_LG)\n",
    "\n",
    "# 15番目のCSVファイルを処理する\n",
    "test_df15, LABEL_t15, MAE_t15, FINAL_QP_t15 = process_train_csv_lists(test_QP12_QP12_LG)\n",
    "\n",
    "# 16番目のCSVファイルを処理する\n",
    "test_df16, LABEL_t16, MAE_t16, FINAL_QP_t16 = process_train_csv_lists(test_QP2_QP2_GL)\n",
    "\n",
    "# 17番目のCSVファイルを処理する\n",
    "test_df17, LABEL_t17, MAE_t17, FINAL_QP_t17 = process_train_csv_lists(test_QP4_QP4_GL)\n",
    "\n",
    "# 18番目のCSVファイルを処理する\n",
    "test_df18, LABEL_t18, MAE_t18, FINAL_QP_t18 = process_train_csv_lists(test_QP12_QP12_GL)\n",
    "\n",
    "\n",
    "# 19番目のCSVファイルを処理する\n",
    "test_df19, LABEL_t19, MAE_t19, FINAL_QP_t19 = process_train_csv_lists(test_QP2_QP4_GG)\n",
    "\n",
    "# 20番目のCSVファイルを処理する\n",
    "test_df20, LABEL_t20, MAE_t20, FINAL_QP_t20 = process_train_csv_lists(test_QP2_QP12_GG)\n",
    "\n",
    "# 21番目のCSVファイルを処理する\n",
    "test_df21, LABEL_t21, MAE_t21, FINAL_QP_t21 = process_train_csv_lists(test_QP4_QP12_GG)\n",
    "\n",
    "# 22番目のCSVファイルを処理する\n",
    "test_df22, LABEL_t22, MAE_t22, FINAL_QP_t22 = process_train_csv_lists(test_QP2_QP4_LG)\n",
    "\n",
    "# 23番目のCSVファイルを処理する\n",
    "test_df23, LABEL_t23, MAE_t23, FINAL_QP_t23 = process_train_csv_lists(test_QP2_QP12_LG)\n",
    "\n",
    "# 24番目のCSVファイルを処理する\n",
    "test_df24, LABEL_t24, MAE_t24, FINAL_QP_t24 = process_train_csv_lists(test_QP4_QP12_LG)\n",
    "\n",
    "# 25番目のCSVファイルを処理する\n",
    "test_df25, LABEL_t25, MAE_t25, FINAL_QP_t25 = process_train_csv_lists(test_QP2_QP4_GL)\n",
    "\n",
    "# 26番目のCSVファイルを処理する\n",
    "test_df26, LABEL_t26, MAE_t26, FINAL_QP_t26 = process_train_csv_lists(test_QP2_QP12_GL)\n",
    "\n",
    "# 27番目のCSVファイルを処理する\n",
    "test_df27, LABEL_t27, MAE_t27, FINAL_QP_t27 = process_train_csv_lists(test_QP4_QP12_GL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0          5      0      0   2512  22644  34844      0      0   2416  20412  37172  10014   4177  1443   1740   1138   1577   1822   1621  10507   4537  1395   1739   1061   1482   1874   1669  16952   8692   6596   9672   2180  15908  17104   9596   6828  10632   1872  13968  0.003236  0.000986   0.004104   0.50883  0.322531\n",
      "1         20      0  33088   8480   9068   9364      0  32896   8720   8960   9424  10545  21926   739    379    423    723    669    750  10193  24426   515    366    526    599    574    759   7440  16772   2148   1880   1492  30268   6940  15080   2080   1596   1452  32852  0.000079  0.004655   0.003984  0.018182   0.01338\n",
      "2         16      0  10048   3040  12904  34008      0   9728   3344  12168  34760   6880   6402  2242   2838   1947   1949   4335   2438   6900   7566  2365   2896   1989   1949   4486   2530  14532   6900   8128   8684   1708  20048  13576   6580   8836   9396   1560  20052  0.000824  0.001994   0.001736  0.021426  0.012819\n",
      "3          5      0      0   1808  22976  35216      0      0   1344  20616  38040   8591   7610  1034   1283   1014   1250   1744   1164   8988   7810  1033   1298    923   1234   1663   1106  16308  11732   7880   7996   3052  13032  16372  13656   8096   8268   2472  11136  0.005164  0.000703   0.006101  0.584198  0.306567\n",
      "4         27      0  20160  11824  12872  15144      0  20160  11728  12768  15344  16642  17728   414    611    515    681   1076    762  16793  18070   422    593    536    694   1036    795  14216   6192   3892   3428   2376  29896  13988   5728   3548   3140   2076  31520  0.000035  0.000076   0.001873  0.269221  0.235313\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "475       39      0  33152  23984   2408    456      0  33792  23584   2172    452  23297  13886   727    371    404    433   1456    549  23701  14618   678    375    420    516   1335    446   6140   1640   1888   1464   2204  46664   5096   1092   1264    972   1784  49792  0.000364  0.000885   0.010106  0.170477  0.152214\n",
      "476       42      0  17024  20816  17284   4876      0  18688  20768  16044   4500  12194   4008   627   1034    375   5403   9185   4163  13263   4970   698    821    397   4817   8351   4216   1456   1124    624   1324    492  54980   1192    584    464   1264    560  55936  0.002307  0.006618   0.004361  0.105904   0.12936\n",
      "477       45      0  43648  10128   5452    772      0  45056  10144   4168    632  24266  13361  2434   1562    468   1190   4456    220  23201  15165  2391   1379    531   1625   4354    109   1104    436   1612    752    192  55904    588    272    844    592    164  57540  0.003613  0.005292    0.00902  0.171915  0.158525\n",
      "478       42      0  35840  16144   7060    956      0  36544  16800   5836    820  17790   5880  1277   3375    990   3354   8691    652  19234   6223  1117   3363    933   3350   8140    615   2052    508   1420    572    396  55052   1304    252    840    340    204  57060  0.002511  0.002054   0.010314  0.161606  0.163204\n",
      "479       39      0  38528  12864   5912   2696      0  38464  13328   5612   2596  16500  21749  1906   2101   2170    287   1557    342  17108  21338  1873   2335   2016    174   1718    513   7276   1620   5484    960   1372  43288   5364    920   4848    684    748  47436    0.0003  0.002172    0.01678  0.156756   0.17815\n",
      "\n",
      "[480 rows x 44 columns]\n",
      "   FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         2      0     64   2944  24068  32924      0      0   2544  23268  34188  10177   7936  3797   2715   3118   1082   1296    947  10408   7877  3775   2631   3061   1139   1298    975  16032   8248  12060   9220   3268  11172  16388   8948  12644   9420   2984   9616  0.004575  0.000207   0.003009  0.924978  0.842385\n",
      "1         2      0      0   1104  11216  47680      0      0   1168   9200  49632   7899   6087  1403   1924   1420   1474   2027   1390   7962   6410  1445   2014   1342   1531   1957   1382  18960  11312   8168  10072   1704   9784  18912  12848   8108  10164   1432   8536  0.004116  0.000511   0.003463  0.883246  0.868847\n",
      "2         2      0     64  15520  24580  19836      0      0  16176  23040  20784  14064  13897  1872   1866   1230   1413   1427    882  14514  14160  1812   1836   1132   1370   1526    851  16852   8916  10672  11052   4604   7904  16932   9632  10680  11148   4260   7348  0.004885  0.000417   0.001032  0.995059  0.995078\n",
      "3         2      0      0   8544  22528  28928      0      0   8752  21452  29796   6998   6662   764    733    429   1133   1011    683   7227   6606   746    788    408   1216    946    697  16208   9848   7428  10532   1772  14212  16180  10384   7904  10608   1764  13160  0.000697  0.000571   0.001166  0.901612  0.888913\n",
      "4         2      0     64   9792  25976  24168      0     64   9888  25060  24988  10079   7951  2317   3003   1909   4840   4515   1856  10466   8279  2278   3037   1735   4857   4486   1823  13724   8968   9400  13144   2600  12164  13252   9272   9688  13940   2264  11584   0.00051  0.000547   0.001315  0.951786  0.946901\n",
      "5         2      0      0  11312  28728  19960      0      0  11168  28216  20616   8755   3762  3844   3525   1650   2255   2539   1677   9252   3972  3846   3459   1775   2344   2504   1547  14160  10120   9576  11452   1764  12928  13816  10816   9852  11608   1676  12232  0.000268  0.000829   0.000896   0.95574  0.955028\n",
      "6         2      0   7936  19504  20592  11968      0   8128  19808  19504  12560  10886   9614   408    608    350   7257   2011    774  11257   9642   407    657    532   7194   2343   1166  21652   8408   5800  12284   3040   8816  21004   8596   6020  12600   3264   8516   0.00081  0.003692   0.000552   0.96969  0.991705\n",
      "7         2      0    128  14432  24588  20852      0    192  14480  23264  22064   9913   7935  5799   5472   5174    252    378    226  10541   7905  5530   5722   5086    235    407    248  16104   8648  14508   6604   3240  10896  15276   9292  15088   6824   3072  10448  0.001383  0.000872   0.001229  0.900866  0.886457\n",
      "8         2      0   1152  18464  26224  14160      0   1280  17920  25848  14952  13479   8664  2040   3129   1337   2020   3674   1755  13754   8805  1931   3022   1198   1883   4026   1869  17028   8396   8180  11420   3768  11208  16596   8744   8648  11564   3332  11116  0.000648  0.001073   0.000902  0.863138  0.790621\n",
      "9         2      0    128  11248  23804  24820      0     64  11168  22352  26416  12553   6541  3828   2927   2697    581    904    652  13196   6724  3575   3132   2581    699    804    559  16084   9292  13072   8668   3152   9732  15700   9532  13992   9020   3180   8576  0.002001  0.001789   0.002005  0.946705  0.947938\n",
      "10        2      0      0  15440  22880  21680      0    128  14832  22844  22196  12217   9997  2224   2799   1832   2082   2476   1519  12246  10937  2220   2449   1736   2075   2597   1487  16204   9068   9692  10984   4116   9936  16204   9064   9852  11720   3684   9476  0.002358  0.001885   0.001005   0.98821  0.953204\n",
      "11        2      0     64  15952  28236  15748      0     64  15520  28488  15928   8943   6634  2159   4050   2093   5923   6854   2395   9190   6927  2139   4083   2218   5723   7079   2352  14744   8256   9508  11348   3576  12568  14280   9036   9908  11868   3016  11892  0.000135  0.000394   0.002162  0.984405  0.979833\n",
      "12        2      0    256  15872  28596  15276      0    256  16448  28072  15224  13455   7699  2212   1210   1344    981   1580   1398  13699   8243  2160   1396   1119   1085   1409   1471  16044   8508   8680  10188   3516  13064  16228   8940   8588  10548   3588  12108  0.000253  0.002147   0.000931  0.947378  0.761254\n",
      "13        2      0    128  15648  22568  21656      0      0  15952  21452  22596  12589   7076  4194   3650   2321    791    938    757  12657   7454  4124   3684   2226    741    908    752  15648   9320  12332   8236   3424  11040  15212   9544  13380   8560   3092  10212  0.009169  0.000434   0.001785  0.928257  0.929511\n",
      "14        2      0    576  36448  18592   4384      0    384  36464  18724   4428  16280  18544   348    407    500    331    333    334  16500  18974   407    389    386    298    337    427  16716  10540   8904  11248   5376   7216  15740  11632   8940  10880   5252   7556  0.000702  0.000913   0.001634  0.977329  0.977928\n",
      "15        2      0    128  17232  25244  17396      0     64  17376  24476  18084  11376   3701  1700   1799   1508   2144   2533   1823  11791   4104  1621   2097   1353   2183   2475   1794  18044   8984   8004   9888   1688  13392  16896   9964   8136  11084   1476  12444  0.000837  0.001842    0.00343  0.911682  0.875659\n",
      "16        2      0     64  12416  27196  20324      0      0  11376  27340  21284  11465   7865  1237   1667    800   2600   3069   2186  11821   8249  1232   1709    775   2530   3086   2099  16032   9016   7572  13276   2800  11304  15860   9364   8016  13908   2668  10184  0.004597  0.000396    0.00162   0.93812  0.927244\n",
      "17        2      0    512  22480  23848  13160      0    384  23232  22836  13548  11188   6859  1793   2076   2010   1462   2428   1220  11597   6990  1949   1948   1799   1458   2422   1213  16856  10624   9420  10372   3008   9720  16364  10752   9604  11752   2804   8724  0.000988  0.000999   0.002605  0.928712  0.934809\n",
      "18        2      0      0  12288  28236  19476      0    128  11792  27784  20296  14500  10452  2279   2460   1328   1755   1908   1003  14706  10580  2345   2169   1377   1872   1962    907  17692   8032  10068  11128   5664   7416  16968   8448  11132  11276   5332   6844  0.002564  0.000879   0.001876  0.997487  0.996394\n",
      "19        2      0    128  21792  21356  16724      0    192  21264  21168  17376  12532   4840  1880   1958   1795   1237   1517   1304  13038   5322  1847   1808   1573   1216   1280   1300  16568  11084   9220   9796   2424  10908  16228  11636  10288   9976   2164   9708  0.000529  0.002688   0.002704  0.847781  0.775031\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)\n",
    "print(test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, LABEL, MAE, FINAL_QP, X_train_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    # X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "for i in range(1, 28):\n",
    "    globals()[f'X_test_list{i}'] = []\n",
    "    globals()[f'MAE_list_t{i}'] = []\n",
    "    globals()[f'FINAL_QP_list_t{i}'] = []\n",
    "    globals()[f'Y_test_list{i}'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, LABEL1, MAE1, FINAL_QP1, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, LABEL2, MAE2, FINAL_QP2, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, LABEL3, MAE3, FINAL_QP3, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, LABEL4, MAE4, FINAL_QP4, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, LABEL5, MAE5, FINAL_QP5, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, LABEL6, MAE6, FINAL_QP6, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, LABEL7, MAE7, FINAL_QP7, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, LABEL8, MAE8, FINAL_QP8, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, LABEL9, MAE9, FINAL_QP9, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df10, LABEL10, MAE10, FINAL_QP10, X_train_list, MAE_list, FINAL_QP_list, Y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 28):\n",
    "    eval(f'append_results_to_lists(test_df{i}, LABEL_t{i}, MAE_t{i}, FINAL_QP_t{i}, X_test_list{i}, MAE_list_t{i}, FINAL_QP_list_t{i}, Y_test_list{i})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "4320\n",
      "480\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "4320\n",
      "480\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "4320\n",
      "480\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "4320\n",
      "480\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "4320\n",
      "480\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "4320\n",
      "480\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "4320\n",
      "480\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "4320\n",
      "480\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "4320\n",
      "480\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "4320\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "# C_values = {'C': [0.01]}\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# データフレームを初期化\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# 1から106までの列名を作成し、データフレームに追加\n",
    "columns = []\n",
    "for i in range(1, 28):\n",
    "    columns.extend([\n",
    "        f'C_RBF{i}', f'Score_RBF{i}', f'tnr_rbf{i}', f'tpr_rbf{i}', f'AUC_RBF{i}',\n",
    "        f'C_LINEAR{i}', f'Score_LINEAR{i}', f'tnr_linear{i}', f'tpr_linear{i}', f'AUC_LINEAR{i}',\n",
    "        f'Threshold{i}', f'Score_old{i}', f'tnr_old{i}', f'tpr_old{i}', f'AUC_old{i}'\n",
    "    ])\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "        \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    print(len(Y_train))\n",
    "    print(len(Y_val))\n",
    "    \n",
    "    # リストの作成（1から106まで）\n",
    "    for i in range(1, 28):\n",
    "        globals()[f'test_data{i}'] = [item for data in globals()[f'X_test_list{i}'] for item in data]\n",
    "        globals()[f'test_label{i}'] = [item for data in globals()[f'Y_test_list{i}'] for item in data]\n",
    "        globals()[f'MAE_data{i}'] = [item for data in globals()[f'MAE_list_t{i}'] for item in data]\n",
    "        globals()[f'FINAL_QP_data{i}'] = [item for data in globals()[f'FINAL_QP_list_t{i}'] for item in data]\n",
    "\n",
    "        globals()[f'best_threshold{i}'] = 0\n",
    "        globals()[f'best_accuracy{i}'] = 0\n",
    "        globals()[f'best_predicted_labels{i}'] = []\n",
    "        globals()[f'best_ground_truth_labels{i}'] = []\n",
    "        globals()[f'tnr_old{i}'] = 0\n",
    "        globals()[f'tpr_old{i}'] = 0\n",
    "        \n",
    "        for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "            test_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(20)])\n",
    "            predicted_labels = test_old.astype(int)\n",
    "            ground_truth_labels = np.array(globals()[f'test_label{i}'])\n",
    "            accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "            if accuracy > globals()[f'best_accuracy{i}']:\n",
    "                globals()[f'best_accuracy{i}'] = accuracy\n",
    "                globals()[f'best_threshold{i}'] = threshold\n",
    "                globals()[f'best_predicted_labels{i}'] = predicted_labels\n",
    "                globals()[f'best_ground_truth_labels{i}'] = ground_truth_labels\n",
    "\n",
    "\n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value, probability=True)\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value, probability=True)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "    \n",
    "    fold_results = {}\n",
    "    for i in range(1, 28):\n",
    "        # RBFモデルの評価\n",
    "        predictions_RBF = best_svm_model_RBF.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_RBF = best_svm_model_RBF.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_RBF = accuracy_score(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'accuracy_RBF{i}'] = accuracy_RBF\n",
    "        report_RBF = classification_report(globals()[f'test_label{i}'], predictions_RBF, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'tnr_rbf{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_rbf{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_rbf, tpr_rbf, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_RBF)\n",
    "        auc_rbf = auc(fpr_rbf, tpr_rbf)\n",
    "        globals()[f'auc_rbf{i}'] = auc_rbf\n",
    "        # print(report_RBF)\n",
    "\n",
    "        # LINEARモデルの評価\n",
    "        predictions_LINEAR = best_svm_model_LINEAR.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_LINEAR = best_svm_model_LINEAR.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_LINEAR = accuracy_score(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'accuracy_LINEAR{i}'] = accuracy_LINEAR\n",
    "        report_LINEAR = classification_report(globals()[f'test_label{i}'], predictions_LINEAR, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'tnr_linear{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_linear{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_linear, tpr_linear, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_LINEAR)\n",
    "        auc_linear = auc(fpr_linear, tpr_linear)\n",
    "        globals()[f'auc_linear{i}'] = auc_linear\n",
    "        # print(report_LINEAR)\n",
    "\n",
    "        # Old modelの評価\n",
    "        thresholds = np.arange(0.00, 1.01, 0.01)\n",
    "        tpr_old_list = []\n",
    "        fpr_old_list = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(20)])\n",
    "            tn, fp, fn, tp = confusion_matrix(globals()[f'test_label{i}'], predicted_labels_old).ravel()\n",
    "            tpr_old = tp / (tp + fn)\n",
    "            fpr_old = fp / (fp + tn)\n",
    "            tpr_old_list.append(tpr_old)\n",
    "            fpr_old_list.append(fpr_old)\n",
    "        \n",
    "        auc_old = auc(fpr_old_list, tpr_old_list)\n",
    "        globals()[f'auc_old{i}'] = auc_old\n",
    "\n",
    "        # fold_resultsに保存\n",
    "        fold_results[f'C_RBF{i}'] = best_c_value_RBF\n",
    "        fold_results[f'Score_RBF{i}'] = globals()[f'accuracy_RBF{i}']\n",
    "        fold_results[f'tnr_rbf{i}'] = globals()[f'tnr_rbf{i}']\n",
    "        fold_results[f'tpr_rbf{i}'] = globals()[f'tpr_rbf{i}']\n",
    "        fold_results[f'AUC_RBF{i}'] = globals()[f'auc_rbf{i}']\n",
    "\n",
    "        fold_results[f'C_LINEAR{i}'] = best_c_value_LINEAR\n",
    "        fold_results[f'Score_LINEAR{i}'] = globals()[f'accuracy_LINEAR{i}']\n",
    "        fold_results[f'tnr_linear{i}'] = globals()[f'tnr_linear{i}']\n",
    "        fold_results[f'tpr_linear{i}'] = globals()[f'tpr_linear{i}']\n",
    "        fold_results[f'AUC_LINEAR{i}'] = globals()[f'auc_linear{i}']\n",
    "\n",
    "        fold_results[f'Threshold{i}'] = globals()[f'best_threshold{i}']\n",
    "        fold_results[f'Score_old{i}'] = globals()[f'best_accuracy{i}']\n",
    "        fold_results[f'tnr_old{i}'] = globals()[f'tnr_old{i}']\n",
    "        fold_results[f'tpr_old{i}'] = globals()[f'tpr_old{i}']\n",
    "        fold_results[f'AUC_old{i}'] = globals()[f'auc_old{i}']\n",
    "\n",
    "    # 結果をデータフレームに追加\n",
    "    results = pd.concat([results, pd.DataFrame(fold_results, index=[fold])], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0    RBF1         73.0         60.0                66.5                4.12            70.0            60.0         0.76     0.02     0.79     0.72\n",
      "1    RBF2         84.0         86.0                85.0                5.27            95.0            80.0         0.97     0.01     0.98     0.96\n",
      "2    RBF3         93.0         90.0                91.5                2.42            95.0            90.0         0.98     0.01     0.99     0.96\n",
      "3    RBF4         72.0         70.0                71.0                3.16            75.0            65.0         0.80     0.03     0.84     0.75\n",
      "4    RBF5         84.0         97.0                90.5                4.97            95.0            80.0         0.98     0.01     1.00     0.96\n",
      "..    ...          ...          ...                 ...                 ...             ...             ...          ...      ...      ...      ...\n",
      "76  OLD23          0.0          0.0                60.0                0.00            60.0            60.0         0.43     0.00     0.43     0.43\n",
      "77  OLD24          0.0          0.0                60.0                0.00            60.0            60.0         0.43     0.00     0.43     0.43\n",
      "78  OLD25          0.0          0.0                50.0                0.00            50.0            50.0         0.20     0.00     0.21     0.21\n",
      "79  OLD26          0.0          0.0                55.0                0.00            55.0            55.0         0.38     0.00     0.38     0.38\n",
      "80  OLD27          0.0          0.0                55.0                0.00            55.0            55.0         0.36     0.00     0.36     0.36\n",
      "\n",
      "[81 rows x 11 columns]\n",
      "          Model  Average TNR  Average TPR  Average Test Score  Test Score STD  Test Score MAX  Test Score MIN  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0      RBF_1_10        83.89        77.67               80.78            1.65            95.0            50.0         0.90     0.01     1.00     0.67\n",
      "1     RBF_10_19        70.44        70.00               70.22            1.70            95.0            50.0         0.77     0.02     0.97     0.58\n",
      "2     RBF_19_28        58.00        54.78               56.39            2.15            75.0            40.0         0.57     0.01     0.81     0.40\n",
      "3   LINEAR_1_10        49.11        85.78               67.44            2.88            95.0            40.0         0.81     0.02     1.00     0.49\n",
      "4  LINEAR_10_19        43.00        82.78               62.89            3.05            90.0            35.0         0.70     0.01     0.95     0.39\n",
      "5  LINEAR_19_28        47.78        62.00               54.89            1.71            75.0            30.0         0.53     0.01     0.83     0.34\n",
      "6      OLD_1_10         0.00         0.00               63.89            0.00            80.0            55.0         0.54     0.00     0.85     0.24\n",
      "7     OLD_10_19         0.00         0.00               54.44            0.00            60.0            50.0         0.34     0.00     0.46     0.23\n",
      "8     OLD_19_28         0.00         0.00               56.67            0.00            60.0            50.0         0.39     0.00     0.47     0.21\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第2位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': [f'RBF{i}' for i in range(1, 28)] + [f'LINEAR{i}' for i in range(1, 28)] + [f'OLD{i}' for i in range(1, 28)],\n",
    "    'Average TNR': [\n",
    "        round(results[f'tnr_rbf{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tnr_linear{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tnr_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average TPR': [\n",
    "        round(results[f'tpr_rbf{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tpr_linear{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tpr_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Standard Deviation': [\n",
    "        round(results[f'Score_RBF{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Max Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Min Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'AUC STD': [\n",
    "        round(results[f'AUC_RBF{i}'].std(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].std(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].std(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Max AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].max(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].max(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].max(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Min AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].min(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].min(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].min(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)\n",
    "\n",
    "\n",
    "\n",
    "# 関数を定義して、各セグメントの統計情報を計算\n",
    "def calculate_statistics(segment, prefix):\n",
    "    # モデル番号を抽出してフラットなリストに変換\n",
    "    model_numbers = statistics_df['Model'].str.extract(r'(\\d+)').astype(int)[0]\n",
    "    is_in_segment = model_numbers.isin(segment)\n",
    "    is_correct_prefix = statistics_df['Model'].str.startswith(prefix)\n",
    "    \n",
    "    tnr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TNR'].mean(), 2)\n",
    "    tpr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TPR'].mean(), 2)\n",
    "    acc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average Test Score'].mean(), 2)\n",
    "    acc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Standard Deviation'].std(), 2)\n",
    "    \n",
    "    acc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max Test Score'].max(), 2)\n",
    "    acc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min Test Score'].min(), 2)\n",
    "\n",
    "    auc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average AUC'].mean(), 2)\n",
    "    auc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'AUC STD'].std(), 2)\n",
    "    auc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max AUC'].max(), 2)\n",
    "    auc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min AUC'].min(), 2)\n",
    "    \n",
    "    return tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min\n",
    "\n",
    "# セグメントを定義\n",
    "segments = {\n",
    "    '1_10': list(range(1, 10)),\n",
    "    '10_19': list(range(10, 19)),\n",
    "    '19_28': list(range(19, 28))\n",
    "}\n",
    "\n",
    "# 結果を保存するリスト\n",
    "results_summary = []\n",
    "\n",
    "# 統計情報を計算して表示\n",
    "for model in ['RBF', 'LINEAR', 'OLD']:\n",
    "    for segment_name, segment in segments.items():\n",
    "        tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min = calculate_statistics(segment, model)\n",
    "        results_summary.append({\n",
    "            'Model': f'{model}_{segment_name}',\n",
    "            'Average TNR': tnr_mean,\n",
    "            'Average TPR': tpr_mean,\n",
    "            'Average Test Score': acc_mean,\n",
    "            'Test Score STD': acc_std,\n",
    "            'Test Score MAX': acc_max,\n",
    "            'Test Score MIN': acc_min,\n",
    "            'Average AUC': auc_mean,\n",
    "            'AUC STD': auc_std,\n",
    "            'Max AUC': auc_max,\n",
    "            'Min AUC': auc_min\n",
    "        })\n",
    "\n",
    "# DataFrameに変換\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# 表示\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    100\n",
      "1     10\n",
      "2    100\n",
      "3     10\n",
      "4    100\n",
      "5     10\n",
      "6     10\n",
      "7     10\n",
      "8     10\n",
      "9    100\n",
      "Name: C_RBF1, dtype: object\n",
      "0      10\n",
      "1       1\n",
      "2       1\n",
      "3     100\n",
      "4    1000\n",
      "5     100\n",
      "6     100\n",
      "7    1000\n",
      "8     100\n",
      "9     100\n",
      "Name: C_LINEAR1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF1'])\n",
    "print(results['C_LINEAR1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
