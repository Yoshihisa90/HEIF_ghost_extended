{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['256', '66', '253', '178', '227', '177', '53', '268', '3', '186', '42', '96', '33', '73', '212', '77', '154', '188', '171', '209', '286', '217', '283', '148', '260', '264', '295', '265', '285', '274'], ['271', '24', '280', '31', '45', '14', '122', '294', '15', '245', '146', '270', '255', '23', '165', '87', '197', '86', '16', '233', '160', '111', '130', '229', '297', '206', '153', '119', '104', '30'], ['232', '68', '62', '1', '47', '234', '191', '155', '57', '216', '281', '203', '277', '102', '190', '208', '278', '158', '261', '237', '243', '224', '252', '36', '162', '226', '91', '142', '150', '133'], ['75', '199', '12', '141', '93', '58', '219', '151', '18', '189', '50', '11', '89', '109', '114', '131', '181', '159', '97', '214', '282', '180', '259', '78', '250', '138', '192', '149', '19', '126'], ['298', '123', '21', '32', '101', '34', '287', '179', '220', '175', '28', '72', '46', '26', '69', '134', '59', '222', '125', '136', '54', '257', '7', '296', '195', '124', '100', '223', '249', '52'], ['215', '170', '248', '144', '291', '113', '289', '198', '60', '168', '300', '292', '201', '236', '241', '22', '35', '29', '64', '115', '147', '38', '242', '172', '137', '163', '76', '167', '43', '288'], ['182', '290', '2', '275', '273', '210', '70', '49', '269', '284', '48', '17', '88', '128', '71', '13', '152', '127', '63', '258', '37', '92', '85', '267', '173', '44', '225', '272', '82', '143'], ['65', '185', '207', '204', '132', '121', '193', '263', '112', '56', '299', '169', '106', '187', '276', '25', '135', '279', '118', '5', '161', '205', '246', '202', '157', '83', '27', '108', '107', '251'], ['231', '156', '81', '20', '254', '55', '235', '145', '247', '8', '176', '139', '228', '184', '39', '84', '94', '41', '194', '103', '238', '9', '218', '174', '129', '6', '80', '240', '230', '99'], ['262', '213', '74', '120', '293', '51', '164', '95', '116', '239', '79', '110', '244', '98', '196', '61', '117', '221', '166', '40', '10', '140', '90', '211', '67', '105', '266', '4', '183', '200']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP5 = [\"_1stQP5_\"]\n",
    "QP10 = [\"_1stQP10_\"]\n",
    "QP16 = [\"_1stQP16_\"]\n",
    "QP20 = [\"_1stQP20_\"]\n",
    "QP24 = [\"_1stQP24_\"]\n",
    "QP27 = [\"_1stQP27_\"]\n",
    "QP32 = [\"_1stQP32_\"]\n",
    "QP39 = [\"_1stQP39_\"]\n",
    "QP42 = [\"_1stQP42_\"]\n",
    "QP45 = [\"_1stQP45_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP10_QP5 = [\"_1stQP10_2ndQP5_\"]\n",
    "\n",
    "QP15_QP5 = [\"_1stQP15_2ndQP5_\"]\n",
    "QP15_QP10 = [\"_1stQP15_2ndQP10_\"]\n",
    "\n",
    "QP20_QP5 = [\"_1stQP20_2ndQP5_\"]\n",
    "QP20_QP10 = [\"_1stQP20_2ndQP10_\"]\n",
    "QP20_QP16 = [\"_1stQP20_2ndQP16_\"]\n",
    "\n",
    "QP25_QP5 = [\"_1stQP25_2ndQP5_\"]\n",
    "QP25_QP10 = [\"_1stQP25_2ndQP10_\"]\n",
    "QP25_QP16 = [\"_1stQP25_2ndQP16_\"]\n",
    "QP25_QP20 = [\"_1stQP25_2ndQP20_\"]\n",
    "QP25_QP24 = [\"_1stQP25_2ndQP24_\"]\n",
    "\n",
    "QP30_QP5 = [\"_1stQP30_2ndQP5_\"]\n",
    "QP30_QP10 = [\"_1stQP30_2ndQP10_\"]\n",
    "QP30_QP16 = [\"_1stQP30_2ndQP16_\"]\n",
    "QP30_QP20 = [\"_1stQP30_2ndQP20_\"]\n",
    "QP30_QP24 = [\"_1stQP30_2ndQP24_\"]\n",
    "QP30_QP27 = [\"_1stQP30_2ndQP27_\"]\n",
    "\n",
    "QP32_QP5 = [\"_1stQP32_2ndQP5_\"]\n",
    "QP32_QP10 = [\"_1stQP32_2ndQP10_\"]\n",
    "QP32_QP16 = [\"_1stQP32_2ndQP16_\"]\n",
    "QP32_QP20 = [\"_1stQP32_2ndQP20_\"]\n",
    "QP32_QP24 = [\"_1stQP32_2ndQP24_\"]\n",
    "QP32_QP27 = [\"_1stQP32_2ndQP27_\"]\n",
    "\n",
    "QP35_QP5 = [\"_1stQP35_2ndQP5_\"]\n",
    "QP35_QP10 = [\"_1stQP35_2ndQP10_\"]\n",
    "QP35_QP16 = [\"_1stQP35_2ndQP16_\"]\n",
    "QP35_QP20 = [\"_1stQP35_2ndQP20_\"]\n",
    "QP35_QP24 = [\"_1stQP35_2ndQP24_\"]\n",
    "QP35_QP27 = [\"_1stQP35_2ndQP27_\"]\n",
    "QP35_QP32 = [\"_1stQP35_2ndQP32_\"]\n",
    "\n",
    "QP40_QP5 = [\"_1stQP40_2ndQP5_\"]\n",
    "QP40_QP10 = [\"_1stQP40_2ndQP10_\"]\n",
    "QP40_QP16 = [\"_1stQP40_2ndQP16_\"]\n",
    "QP40_QP20 = [\"_1stQP40_2ndQP20_\"]\n",
    "QP40_QP24 = [\"_1stQP40_2ndQP24_\"]\n",
    "QP40_QP27 = [\"_1stQP40_2ndQP27_\"]\n",
    "QP40_QP32 = [\"_1stQP40_2ndQP32_\"]\n",
    "QP40_QP39 = [\"_1stQP40_2ndQP39_\"]\n",
    "\n",
    "QP45_QP5 = [\"_1stQP45_2ndQP5_\"]\n",
    "QP45_QP10 = [\"_1stQP45_2ndQP10_\"]\n",
    "QP45_QP16 = [\"_1stQP45_2ndQP16_\"]\n",
    "QP45_QP20 = [\"_1stQP45_2ndQP20_\"]\n",
    "QP45_QP24 = [\"_1stQP45_2ndQP24_\"]\n",
    "QP45_QP27 = [\"_1stQP45_2ndQP27_\"]\n",
    "QP45_QP32 = [\"_1stQP45_2ndQP32_\"]\n",
    "QP45_QP39 = [\"_1stQP45_2ndQP39_\"]\n",
    "QP45_QP42 = [\"_1stQP45_2ndQP42_\"]\n",
    "\n",
    "QP50_QP5 = [\"_1stQP50_2ndQP5_\"]\n",
    "QP50_QP10 = [\"_1stQP50_2ndQP10_\"]\n",
    "QP50_QP16 = [\"_1stQP50_2ndQP16_\"]\n",
    "QP50_QP20 = [\"_1stQP50_2ndQP20_\"]\n",
    "QP50_QP24 = [\"_1stQP50_2ndQP24_\"]\n",
    "QP50_QP27 = [\"_1stQP50_2ndQP27_\"]\n",
    "QP50_QP32 = [\"_1stQP50_2ndQP32_\"]\n",
    "QP50_QP39 = [\"_1stQP50_2ndQP39_\"]\n",
    "QP50_QP42 = [\"_1stQP50_2ndQP42_\"]\n",
    "QP50_QP45 = [\"_1stQP50_2ndQP45_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP5_QP5 = [\"_1stQP5_2ndQP5\"]\n",
    "QP10_QP10 = [\"_1stQP10_2ndQP10\"]\n",
    "QP16_QP16 = [\"_1stQP16_2ndQP16\"]\n",
    "QP20_QP20 = [\"_1stQP20_2ndQP20\"]\n",
    "QP24_QP24 = [\"_1stQP24_2ndQP24\"]\n",
    "QP27_QP27 = [\"_1stQP27_2ndQP27\"]\n",
    "QP32_QP32 = [\"_1stQP32_2ndQP32\"]\n",
    "QP39_QP39 = [\"_1stQP39_2ndQP39\"]\n",
    "QP42_QP42 = [\"_1stQP42_2ndQP42\"]\n",
    "QP45_QP45 = [\"_1stQP45_2ndQP45\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP10_QP16 = [\"_1stQP10_2ndQP16\"]\n",
    "QP10_QP20 = [\"_1stQP10_2ndQP20\"]\n",
    "QP10_QP24 = [\"_1stQP10_2ndQP24\"]\n",
    "QP10_QP27 = [\"_1stQP10_2ndQP27\"]\n",
    "QP10_QP32 = [\"_1stQP10_2ndQP32\"]\n",
    "QP10_QP39 = [\"_1stQP10_2ndQP39\"]\n",
    "QP10_QP42 = [\"_1stQP10_2ndQP42\"]\n",
    "QP10_QP45 = [\"_1stQP10_2ndQP45\"]\n",
    "\n",
    "QP15_QP16 = [\"_1stQP15_2ndQP16\"]\n",
    "QP15_QP20 = [\"_1stQP15_2ndQP20\"]\n",
    "QP15_QP24 = [\"_1stQP15_2ndQP24\"]\n",
    "QP15_QP27 = [\"_1stQP15_2ndQP27\"]\n",
    "QP15_QP32 = [\"_1stQP15_2ndQP32\"]\n",
    "QP15_QP39 = [\"_1stQP15_2ndQP39\"]\n",
    "QP15_QP42 = [\"_1stQP15_2ndQP42\"]\n",
    "QP15_QP45 = [\"_1stQP15_2ndQP45\"]\n",
    "\n",
    "QP20_QP24 = [\"_1stQP20_2ndQP24\"]\n",
    "QP20_QP27 = [\"_1stQP20_2ndQP27\"]\n",
    "QP20_QP32 = [\"_1stQP20_2ndQP32\"]\n",
    "QP20_QP39 = [\"_1stQP20_2ndQP39\"]\n",
    "QP20_QP42 = [\"_1stQP20_2ndQP42\"]\n",
    "QP20_QP45 = [\"_1stQP20_2ndQP45\"]\n",
    "\n",
    "QP25_QP27 = [\"_1stQP25_2ndQP27\"]\n",
    "QP25_QP32 = [\"_1stQP25_2ndQP32\"]\n",
    "QP25_QP39 = [\"_1stQP25_2ndQP39\"]\n",
    "QP25_QP42 = [\"_1stQP25_2ndQP42\"]\n",
    "QP25_QP45 = [\"_1stQP25_2ndQP45\"]\n",
    "\n",
    "QP30_QP32 = [\"_1stQP30_2ndQP32\"]\n",
    "QP30_QP39 = [\"_1stQP30_2ndQP39\"]\n",
    "QP30_QP42 = [\"_1stQP30_2ndQP42\"]\n",
    "QP30_QP45 = [\"_1stQP30_2ndQP45\"]\n",
    "\n",
    "QP32_QP39 = [\"_1stQP32_2ndQP39\"]\n",
    "QP32_QP42 = [\"_1stQP32_2ndQP42\"]\n",
    "QP32_QP45 = [\"_1stQP32_2ndQP45\"]\n",
    "\n",
    "QP35_QP39 = [\"_1stQP35_2ndQP39\"]\n",
    "QP35_QP42 = [\"_1stQP35_2ndQP42\"]\n",
    "QP35_QP45 = [\"_1stQP35_2ndQP45\"]\n",
    "\n",
    "QP40_QP42 = [\"_1stQP40_2ndQP42\"]\n",
    "QP40_QP45 = [\"_1stQP40_2ndQP45\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "('/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/HEIF_images_single_csv/200_1stQP5_CTU64_PRESETmedium.csv', '/Prove/Yoshihisa/HEIF_ghost/PKL/pkl_single/200_1stQP5_CTU64_PRESETmedium.pkl', '/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/HEIF_images_second_sameQP_csv/200_1stQP5_2ndQP5_CTU64_PRESETmedium.csv', '/Prove/Yoshihisa/HEIF_ghost/PKL/pkl_second_sameQP/200_1stQP5_2ndQP5_CTU64_PRESETmedium.pkl')\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print(len(single_csv9))\n",
    "\n",
    "\n",
    "single_QP5 = [item for item in single_csv10 if any(qp in item[0] for qp in QP5)]\n",
    "single_QP10 = [item for item in single_csv10 if any(qp in item[0] for qp in QP10)]\n",
    "single_QP16 = [item for item in single_csv10 if any(qp in item[0] for qp in QP16)]\n",
    "single_QP20 = [item for item in single_csv10 if any(qp in item[0] for qp in QP20)]\n",
    "single_QP24 = [item for item in single_csv10 if any(qp in item[0] for qp in QP24)]\n",
    "single_QP27 = [item for item in single_csv10 if any(qp in item[0] for qp in QP27)]\n",
    "single_QP32 = [item for item in single_csv10 if any(qp in item[0] for qp in QP32)]\n",
    "single_QP39 = [item for item in single_csv10 if any(qp in item[0] for qp in QP39)]\n",
    "single_QP42 = [item for item in single_csv10 if any(qp in item[0] for qp in QP42)]\n",
    "single_QP45 = [item for item in single_csv10 if any(qp in item[0] for qp in QP45)]\n",
    "print((single_QP5[29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "\n",
      "double images train by QP1>QP2:  100\n",
      "\n",
      "double images test by QP1>QP2:  30\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "# second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1>QP2: ', len(second_largeQP1_csv1))\n",
    "\n",
    "second_QP10_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP10_QP5)]\n",
    "second_QP15_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP15_QP5)]\n",
    "second_QP15_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP15_QP10)]\n",
    "second_QP20_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP20_QP5)]\n",
    "second_QP20_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP20_QP10)]\n",
    "second_QP20_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP20_QP16)]\n",
    "second_QP25_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP5)]\n",
    "second_QP25_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP10)]\n",
    "second_QP25_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP16)]\n",
    "second_QP25_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP20)]\n",
    "second_QP25_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP24)]\n",
    "second_QP30_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP5)]\n",
    "second_QP30_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP10)]\n",
    "second_QP30_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP16)]\n",
    "second_QP30_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP20)]\n",
    "second_QP30_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP24)]\n",
    "second_QP30_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP27)]\n",
    "second_QP32_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP5)]\n",
    "second_QP32_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP10)]\n",
    "second_QP32_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP16)]\n",
    "second_QP32_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP20)]\n",
    "second_QP32_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP24)]\n",
    "second_QP32_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP27)]\n",
    "second_QP35_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP5)]\n",
    "second_QP35_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP10)]\n",
    "second_QP35_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP16)]\n",
    "second_QP35_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP20)]\n",
    "second_QP35_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP24)]\n",
    "second_QP35_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP27)]\n",
    "second_QP35_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP32)]\n",
    "second_QP40_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP5)]\n",
    "second_QP40_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP10)]\n",
    "second_QP40_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP16)]\n",
    "second_QP40_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP20)]\n",
    "second_QP40_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP24)]\n",
    "second_QP40_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP27)]\n",
    "second_QP40_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP32)]\n",
    "second_QP40_QP39 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP39)]\n",
    "second_QP45_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP5)]\n",
    "second_QP45_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP10)]\n",
    "second_QP45_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP16)]\n",
    "second_QP45_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP20)]\n",
    "second_QP45_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP24)]\n",
    "second_QP45_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP27)]\n",
    "second_QP45_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP32)]\n",
    "second_QP45_QP39 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP39)]\n",
    "second_QP45_QP42 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP42)]\n",
    "second_QP50_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP5)]\n",
    "second_QP50_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP10)]\n",
    "second_QP50_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP16)]\n",
    "second_QP50_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP20)]\n",
    "second_QP50_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP24)]\n",
    "second_QP50_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP27)]\n",
    "second_QP50_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP32)]\n",
    "second_QP50_QP39 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP39)]\n",
    "second_QP50_QP42 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP42)]\n",
    "second_QP50_QP45 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP45)]\n",
    "print('\\ndouble images test by QP1>QP2: ', len(second_QP50_QP45))\n",
    "\n",
    "# second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "# second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "# second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "# second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "# second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "# second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "# second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "# second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "# second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "# second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "\n",
      "double images train by QP1=QP2:  100\n",
      "\n",
      "double images test by QP1 = QP2:  30\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "print(len(second_sameQP_csv10))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 100)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 100)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 100)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 100)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 100)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 100)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 100)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 100)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 100)\n",
    "print('\\ndouble images train by QP1=QP2: ',len(second_sameQP_csv9))\n",
    "\n",
    "\n",
    "second_QP5_QP5 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP5_QP5)]\n",
    "second_QP10_QP10 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP10_QP10)]\n",
    "second_QP16_QP16 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP16_QP16)]\n",
    "second_QP20_QP20 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP20_QP20)]\n",
    "second_QP24_QP24 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP24_QP24)]\n",
    "second_QP27_QP27 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP27_QP27)]\n",
    "second_QP32_QP32 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP32_QP32)]\n",
    "second_QP39_QP39 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP39_QP39)]\n",
    "second_QP42_QP42 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP42_QP42)]\n",
    "second_QP45_QP45 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP45_QP45)]\n",
    "print('\\ndouble images test by QP1 = QP2: ', len(second_QP5_QP5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170\n",
      "\n",
      "double images train by QP1<QP2:  100\n",
      "\n",
      "Double images by QP (QP1 < QP2):  30\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "print(len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 100)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 100)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 100)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 100)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 100)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 100)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 100)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 100)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 100)\n",
    "# second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1<QP2: ', len(second_largeQP2_csv1))\n",
    "\n",
    "\n",
    "second_QP10_QP16 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP16)]\n",
    "second_QP10_QP20 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP20)]\n",
    "second_QP10_QP24 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP24)]\n",
    "second_QP10_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP27)]\n",
    "second_QP10_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP32)]\n",
    "second_QP10_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP39)]\n",
    "second_QP10_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP42)]\n",
    "second_QP10_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP45)]\n",
    "second_QP15_QP16 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP16)]\n",
    "second_QP15_QP20 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP20)]\n",
    "second_QP15_QP24 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP24)]\n",
    "second_QP15_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP27)]\n",
    "second_QP15_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP32)]\n",
    "second_QP15_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP39)]\n",
    "second_QP15_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP42)]\n",
    "second_QP15_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP45)]\n",
    "second_QP20_QP24 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP24)]\n",
    "second_QP20_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP27)]\n",
    "second_QP20_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP32)]\n",
    "second_QP20_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP39)]\n",
    "second_QP20_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP42)]\n",
    "second_QP20_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP45)]\n",
    "second_QP25_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP27)]\n",
    "second_QP25_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP32)]\n",
    "second_QP25_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP39)]\n",
    "second_QP25_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP42)]\n",
    "second_QP25_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP45)]\n",
    "second_QP30_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP32)]\n",
    "second_QP30_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP39)]\n",
    "second_QP30_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP42)]\n",
    "second_QP30_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP45)]\n",
    "second_QP32_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP32_QP39)]\n",
    "second_QP32_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP32_QP42)]\n",
    "second_QP32_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP32_QP45)]\n",
    "second_QP35_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP35_QP39)]\n",
    "second_QP35_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP35_QP42)]\n",
    "second_QP35_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP35_QP45)]\n",
    "second_QP40_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP40_QP42)]\n",
    "second_QP40_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP40_QP45)]\n",
    "print('\\nDouble images by QP (QP1 < QP2): ', len(second_QP40_QP45))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "print(\"train_csv_list: \", len(train_csv_list9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP50_QP45:  60\n"
     ]
    }
   ],
   "source": [
    "test_QP10_QP5 = second_QP10_QP5 + single_QP5\n",
    "test_QP15_QP5 = second_QP15_QP5 + single_QP5\n",
    "test_QP15_QP10 = second_QP15_QP10 + single_QP10\n",
    "test_QP20_QP5 = second_QP20_QP5 + single_QP5\n",
    "test_QP20_QP10 = second_QP20_QP10 + single_QP10\n",
    "test_QP20_QP16 = second_QP20_QP16 + single_QP16\n",
    "test_QP25_QP5 = second_QP25_QP5 + single_QP5\n",
    "test_QP25_QP10 = second_QP25_QP10 + single_QP10\n",
    "test_QP25_QP16 = second_QP25_QP16 + single_QP16\n",
    "test_QP25_QP20 = second_QP25_QP20 + single_QP20\n",
    "test_QP25_QP24 = second_QP25_QP24 + single_QP24\n",
    "test_QP30_QP5 = second_QP30_QP5 + single_QP5\n",
    "test_QP30_QP10 = second_QP30_QP10 + single_QP10\n",
    "test_QP30_QP16 = second_QP30_QP16 + single_QP16\n",
    "test_QP30_QP20 = second_QP30_QP20 + single_QP20\n",
    "test_QP30_QP24 = second_QP30_QP24 + single_QP24\n",
    "test_QP30_QP27 = second_QP30_QP27 + single_QP27\n",
    "test_QP32_QP5 = second_QP32_QP5 + single_QP5\n",
    "test_QP32_QP10 = second_QP32_QP10 + single_QP10\n",
    "test_QP32_QP16 = second_QP32_QP16 + single_QP16\n",
    "test_QP32_QP20 = second_QP32_QP20 + single_QP20\n",
    "test_QP32_QP24 = second_QP32_QP24 + single_QP24\n",
    "test_QP32_QP27 = second_QP32_QP27 + single_QP27\n",
    "test_QP35_QP5 = second_QP35_QP5 + single_QP5\n",
    "test_QP35_QP10 = second_QP35_QP10 + single_QP10\n",
    "test_QP35_QP16 = second_QP35_QP16 + single_QP16\n",
    "test_QP35_QP20 = second_QP35_QP20 + single_QP20\n",
    "test_QP35_QP24 = second_QP35_QP24 + single_QP24\n",
    "test_QP35_QP27 = second_QP35_QP27 + single_QP27\n",
    "test_QP35_QP32 = second_QP35_QP32 + single_QP32\n",
    "test_QP40_QP5 = second_QP40_QP5 + single_QP5\n",
    "test_QP40_QP10 = second_QP40_QP10 + single_QP10\n",
    "test_QP40_QP16 = second_QP40_QP16 + single_QP16\n",
    "test_QP40_QP20 = second_QP40_QP20 + single_QP20\n",
    "test_QP40_QP24 = second_QP40_QP24 + single_QP24\n",
    "test_QP40_QP27 = second_QP40_QP27 + single_QP27\n",
    "test_QP40_QP32 = second_QP40_QP32 + single_QP32\n",
    "test_QP40_QP39 = second_QP40_QP39 + single_QP39\n",
    "test_QP45_QP5 = second_QP45_QP5 + single_QP5\n",
    "test_QP45_QP10 = second_QP45_QP10 + single_QP10\n",
    "test_QP45_QP16 = second_QP45_QP16 + single_QP16\n",
    "test_QP45_QP20 = second_QP45_QP20 + single_QP20\n",
    "test_QP45_QP24 = second_QP45_QP24 + single_QP24\n",
    "test_QP45_QP27 = second_QP45_QP27 + single_QP27\n",
    "test_QP45_QP32 = second_QP45_QP32 + single_QP32\n",
    "test_QP45_QP39 = second_QP45_QP39 + single_QP39\n",
    "test_QP45_QP42 = second_QP45_QP42 + single_QP42\n",
    "test_QP50_QP5 = second_QP50_QP5 + single_QP5\n",
    "test_QP50_QP10 = second_QP50_QP10 + single_QP10\n",
    "test_QP50_QP16 = second_QP50_QP16 + single_QP16\n",
    "test_QP50_QP20 = second_QP50_QP20 + single_QP20\n",
    "test_QP50_QP24 = second_QP50_QP24 + single_QP24\n",
    "test_QP50_QP27 = second_QP50_QP27 + single_QP27\n",
    "test_QP50_QP32 = second_QP50_QP32 + single_QP32\n",
    "test_QP50_QP39 = second_QP50_QP39 + single_QP39\n",
    "test_QP50_QP42 = second_QP50_QP42 + single_QP42\n",
    "test_QP50_QP45 = second_QP50_QP45 + single_QP45\n",
    "\n",
    "print('test_QP50_QP45: ', len(test_QP50_QP45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP45_QP45:  60\n"
     ]
    }
   ],
   "source": [
    "test_QP5_QP5 = second_QP5_QP5 + single_QP5\n",
    "test_QP10_QP10 = second_QP10_QP10 + single_QP10\n",
    "test_QP16_QP16 = second_QP16_QP16 + single_QP16\n",
    "test_QP20_QP20 = second_QP20_QP20 + single_QP20\n",
    "test_QP24_QP24 = second_QP24_QP24 + single_QP24\n",
    "test_QP27_QP27 = second_QP27_QP27 + single_QP27\n",
    "test_QP32_QP32 = second_QP32_QP32 + single_QP32\n",
    "test_QP39_QP39 = second_QP39_QP39 + single_QP39\n",
    "test_QP42_QP42 = second_QP42_QP42 + single_QP42\n",
    "test_QP45_QP45 = second_QP45_QP45 + single_QP45\n",
    "\n",
    "print('test_QP45_QP45: ', len(test_QP45_QP45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP40_QP45:  60\n"
     ]
    }
   ],
   "source": [
    "test_QP10_QP16 = second_QP10_QP16 + single_QP16\n",
    "test_QP10_QP20 = second_QP10_QP20 + single_QP20\n",
    "test_QP10_QP24 = second_QP10_QP24 + single_QP24\n",
    "test_QP10_QP27 = second_QP10_QP27 + single_QP27\n",
    "test_QP10_QP32 = second_QP10_QP32 + single_QP32\n",
    "test_QP10_QP39 = second_QP10_QP39 + single_QP39\n",
    "test_QP10_QP42 = second_QP10_QP42 + single_QP42\n",
    "test_QP10_QP45 = second_QP10_QP45 + single_QP45\n",
    "test_QP15_QP16 = second_QP15_QP16 + single_QP16\n",
    "test_QP15_QP20 = second_QP15_QP20 + single_QP20\n",
    "test_QP15_QP24 = second_QP15_QP24 + single_QP24\n",
    "test_QP15_QP27 = second_QP15_QP27 + single_QP27\n",
    "test_QP15_QP32 = second_QP15_QP32 + single_QP32\n",
    "test_QP15_QP39 = second_QP15_QP39 + single_QP39\n",
    "test_QP15_QP42 = second_QP15_QP42 + single_QP42\n",
    "test_QP15_QP45 = second_QP15_QP45 + single_QP45\n",
    "test_QP20_QP24 = second_QP20_QP24 + single_QP24\n",
    "test_QP20_QP27 = second_QP20_QP27 + single_QP27\n",
    "test_QP20_QP32 = second_QP20_QP32 + single_QP32\n",
    "test_QP20_QP39 = second_QP20_QP39 + single_QP39\n",
    "test_QP20_QP42 = second_QP20_QP42 + single_QP42\n",
    "test_QP20_QP45 = second_QP20_QP45 + single_QP45\n",
    "test_QP25_QP27 = second_QP25_QP27 + single_QP27\n",
    "test_QP25_QP32 = second_QP25_QP32 + single_QP32\n",
    "test_QP25_QP39 = second_QP25_QP39 + single_QP39\n",
    "test_QP25_QP42 = second_QP25_QP42 + single_QP42\n",
    "test_QP25_QP45 = second_QP25_QP45 + single_QP45\n",
    "test_QP30_QP32 = second_QP30_QP32 + single_QP32\n",
    "test_QP30_QP39 = second_QP30_QP39 + single_QP39\n",
    "test_QP30_QP42 = second_QP30_QP42 + single_QP42\n",
    "test_QP30_QP45 = second_QP30_QP45 + single_QP45\n",
    "test_QP32_QP39 = second_QP32_QP39 + single_QP39\n",
    "test_QP32_QP42 = second_QP32_QP42 + single_QP42\n",
    "test_QP32_QP45 = second_QP32_QP45 + single_QP45\n",
    "test_QP35_QP39 = second_QP35_QP39 + single_QP39\n",
    "test_QP35_QP42 = second_QP35_QP42 + single_QP42\n",
    "test_QP35_QP45 = second_QP35_QP45 + single_QP45\n",
    "test_QP40_QP42 = second_QP40_QP42 + single_QP42\n",
    "test_QP40_QP45 = second_QP40_QP45 + single_QP45\n",
    "\n",
    "\n",
    "print('test_QP40_QP45: ', len(test_QP40_QP45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_OG = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1番目のCSVファイルを処理する\n",
    "test_df1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_QP10_QP5)\n",
    "\n",
    "# 2番目のCSVファイルを処理する\n",
    "test_df2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_QP15_QP5)\n",
    "\n",
    "# 3番目のCSVファイルを処理する\n",
    "test_df3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_QP15_QP10)\n",
    "\n",
    "# 4番目のCSVファイルを処理する\n",
    "test_df4, LABEL_t4, MAE_t4, FINAL_QP_t4 = process_train_csv_lists(test_QP20_QP5)\n",
    "\n",
    "# 5番目のCSVファイルを処理する\n",
    "test_df5, LABEL_t5, MAE_t5, FINAL_QP_t5 = process_train_csv_lists(test_QP20_QP10)\n",
    "\n",
    "# 6番目のCSVファイルを処理する\n",
    "test_df6, LABEL_t6, MAE_t6, FINAL_QP_t6 = process_train_csv_lists(test_QP20_QP16)\n",
    "\n",
    "# 7番目のCSVファイルを処理する\n",
    "test_df7, LABEL_t7, MAE_t7, FINAL_QP_t7 = process_train_csv_lists(test_QP25_QP5)\n",
    "\n",
    "# 8番目のCSVファイルを処理する\n",
    "test_df8, LABEL_t8, MAE_t8, FINAL_QP_t8 = process_train_csv_lists(test_QP25_QP10)\n",
    "\n",
    "# 9番目のCSVファイルを処理する\n",
    "test_df9, LABEL_t9, MAE_t9, FINAL_QP_t9 = process_train_csv_lists(test_QP25_QP16)\n",
    "\n",
    "# 10番目のCSVファイルを処理する\n",
    "test_df10, LABEL_t10, MAE_t10, FINAL_QP_t10 = process_train_csv_lists(test_QP25_QP20)\n",
    "\n",
    "# 11番目のCSVファイルを処理する\n",
    "test_df11, LABEL_t11, MAE_t11, FINAL_QP_t11 = process_train_csv_lists(test_QP25_QP24)\n",
    "\n",
    "# 12番目のCSVファイルを処理する\n",
    "test_df12, LABEL_t12, MAE_t12, FINAL_QP_t12 = process_train_csv_lists(test_QP30_QP5)\n",
    "\n",
    "# 13番目のCSVファイルを処理する\n",
    "test_df13, LABEL_t13, MAE_t13, FINAL_QP_t13 = process_train_csv_lists(test_QP30_QP10)\n",
    "\n",
    "# 14番目のCSVファイルを処理する\n",
    "test_df14, LABEL_t14, MAE_t14, FINAL_QP_t14 = process_train_csv_lists(test_QP30_QP16)\n",
    "\n",
    "# 15番目のCSVファイルを処理する\n",
    "test_df15, LABEL_t15, MAE_t15, FINAL_QP_t15 = process_train_csv_lists(test_QP30_QP20)\n",
    "\n",
    "# 16番目のCSVファイルを処理する\n",
    "test_df16, LABEL_t16, MAE_t16, FINAL_QP_t16 = process_train_csv_lists(test_QP30_QP24)\n",
    "\n",
    "# 17番目のCSVファイルを処理する\n",
    "test_df17, LABEL_t17, MAE_t17, FINAL_QP_t17 = process_train_csv_lists(test_QP30_QP27)\n",
    "\n",
    "# 18番目のCSVファイルを処理する\n",
    "test_df18, LABEL_t18, MAE_t18, FINAL_QP_t18 = process_train_csv_lists(test_QP32_QP5)\n",
    "\n",
    "# 19番目のCSVファイルを処理する\n",
    "test_df19, LABEL_t19, MAE_t19, FINAL_QP_t19 = process_train_csv_lists(test_QP32_QP10)\n",
    "\n",
    "# 20番目のCSVファイルを処理する\n",
    "test_df20, LABEL_t20, MAE_t20, FINAL_QP_t20 = process_train_csv_lists(test_QP32_QP16)\n",
    "\n",
    "# 21番目のCSVファイルを処理する\n",
    "test_df21, LABEL_t21, MAE_t21, FINAL_QP_t21 = process_train_csv_lists(test_QP32_QP20)\n",
    "\n",
    "# 22番目のCSVファイルを処理する\n",
    "test_df22, LABEL_t22, MAE_t22, FINAL_QP_t22 = process_train_csv_lists(test_QP32_QP24)\n",
    "\n",
    "# 23番目のCSVファイルを処理する\n",
    "test_df23, LABEL_t23, MAE_t23, FINAL_QP_t23 = process_train_csv_lists(test_QP32_QP27)\n",
    "\n",
    "# 24番目のCSVファイルを処理する\n",
    "test_df24, LABEL_t24, MAE_t24, FINAL_QP_t24 = process_train_csv_lists(test_QP35_QP5)\n",
    "\n",
    "# 25番目のCSVファイルを処理する\n",
    "test_df25, LABEL_t25, MAE_t25, FINAL_QP_t25 = process_train_csv_lists(test_QP35_QP10)\n",
    "\n",
    "# 26番目のCSVファイルを処理する\n",
    "test_df26, LABEL_t26, MAE_t26, FINAL_QP_t26 = process_train_csv_lists(test_QP35_QP16)\n",
    "\n",
    "# 27番目のCSVファイルを処理する\n",
    "test_df27, LABEL_t27, MAE_t27, FINAL_QP_t27 = process_train_csv_lists(test_QP35_QP20)\n",
    "\n",
    "# 28番目のCSVファイルを処理する\n",
    "test_df28, LABEL_t28, MAE_t28, FINAL_QP_t28 = process_train_csv_lists(test_QP35_QP24)\n",
    "\n",
    "# 29番目のCSVファイルを処理する\n",
    "test_df29, LABEL_t29, MAE_t29, FINAL_QP_t29 = process_train_csv_lists(test_QP35_QP27)\n",
    "\n",
    "# 30番目のCSVファイルを処理する\n",
    "test_df30, LABEL_t30, MAE_t30, FINAL_QP_t30 = process_train_csv_lists(test_QP35_QP32)\n",
    "\n",
    "# 31番目のCSVファイルを処理する\n",
    "test_df31, LABEL_t31, MAE_t31, FINAL_QP_t31 = process_train_csv_lists(test_QP40_QP5)\n",
    "\n",
    "# 32番目のCSVファイルを処理する\n",
    "test_df32, LABEL_t32, MAE_t32, FINAL_QP_t32 = process_train_csv_lists(test_QP40_QP10)\n",
    "\n",
    "# 33番目のCSVファイルを処理する\n",
    "test_df33, LABEL_t33, MAE_t33, FINAL_QP_t33 = process_train_csv_lists(test_QP40_QP16)\n",
    "\n",
    "# 34番目のCSVファイルを処理する\n",
    "test_df34, LABEL_t34, MAE_t34, FINAL_QP_t34 = process_train_csv_lists(test_QP40_QP20)\n",
    "\n",
    "# 35番目のCSVファイルを処理する\n",
    "test_df35, LABEL_t35, MAE_t35, FINAL_QP_t35 = process_train_csv_lists(test_QP40_QP24)\n",
    "\n",
    "# 36番目のCSVファイルを処理する\n",
    "test_df36, LABEL_t36, MAE_t36, FINAL_QP_t36 = process_train_csv_lists(test_QP40_QP27)\n",
    "\n",
    "# 37番目のCSVファイルを処理する\n",
    "test_df37, LABEL_t37, MAE_t37, FINAL_QP_t37 = process_train_csv_lists(test_QP40_QP32)\n",
    "\n",
    "# 38番目のCSVファイルを処理する\n",
    "test_df38, LABEL_t38, MAE_t38, FINAL_QP_t38 = process_train_csv_lists(test_QP40_QP39)\n",
    "\n",
    "# 39番目のCSVファイルを処理する\n",
    "test_df39, LABEL_t39, MAE_t39, FINAL_QP_t39 = process_train_csv_lists(test_QP45_QP5)\n",
    "\n",
    "# 40番目のCSVファイルを処理する\n",
    "test_df40, LABEL_t40, MAE_t40, FINAL_QP_t40 = process_train_csv_lists(test_QP45_QP10)\n",
    "\n",
    "# 41番目のCSVファイルを処理する\n",
    "test_df41, LABEL_t41, MAE_t41, FINAL_QP_t41 = process_train_csv_lists(test_QP45_QP16)\n",
    "\n",
    "# 42番目のCSVファイルを処理する\n",
    "test_df42, LABEL_t42, MAE_t42, FINAL_QP_t42 = process_train_csv_lists(test_QP45_QP20)\n",
    "\n",
    "# 43番目のCSVファイルを処理する\n",
    "test_df43, LABEL_t43, MAE_t43, FINAL_QP_t43 = process_train_csv_lists(test_QP45_QP24)\n",
    "\n",
    "# 44番目のCSVファイルを処理する\n",
    "test_df44, LABEL_t44, MAE_t44, FINAL_QP_t44 = process_train_csv_lists(test_QP45_QP27)\n",
    "\n",
    "# 45番目のCSVファイルを処理する\n",
    "test_df45, LABEL_t45, MAE_t45, FINAL_QP_t45 = process_train_csv_lists(test_QP45_QP32)\n",
    "\n",
    "# 46番目のCSVファイルを処理する\n",
    "test_df46, LABEL_t46, MAE_t46, FINAL_QP_t46 = process_train_csv_lists(test_QP45_QP39)\n",
    "\n",
    "# 47番目のCSVファイルを処理する\n",
    "test_df47, LABEL_t47, MAE_t47, FINAL_QP_t47 = process_train_csv_lists(test_QP45_QP42)\n",
    "\n",
    "# 48番目のCSVファイルを処理する\n",
    "test_df48, LABEL_t48, MAE_t48, FINAL_QP_t48 = process_train_csv_lists(test_QP50_QP5)\n",
    "\n",
    "# 49番目のCSVファイルを処理する\n",
    "test_df49, LABEL_t49, MAE_t49, FINAL_QP_t49 = process_train_csv_lists(test_QP50_QP10)\n",
    "\n",
    "# 50番目のCSVファイルを処理する\n",
    "test_df50, LABEL_t50, MAE_t50, FINAL_QP_t50 = process_train_csv_lists(test_QP50_QP16)\n",
    "\n",
    "# 51番目のCSVファイルを処理する\n",
    "test_df51, LABEL_t51, MAE_t51, FINAL_QP_t51 = process_train_csv_lists(test_QP50_QP20)\n",
    "\n",
    "# 52番目のCSVファイルを処理する\n",
    "test_df52, LABEL_t52, MAE_t52, FINAL_QP_t52 = process_train_csv_lists(test_QP50_QP24)\n",
    "\n",
    "# 53番目のCSVファイルを処理する\n",
    "test_df53, LABEL_t53, MAE_t53, FINAL_QP_t53 = process_train_csv_lists(test_QP50_QP27)\n",
    "\n",
    "# 54番目のCSVファイルを処理する\n",
    "test_df54, LABEL_t54, MAE_t54, FINAL_QP_t54 = process_train_csv_lists(test_QP50_QP32)\n",
    "\n",
    "# 55番目のCSVファイルを処理する\n",
    "test_df55, LABEL_t55, MAE_t55, FINAL_QP_t55 = process_train_csv_lists(test_QP50_QP39)\n",
    "\n",
    "# 56番目のCSVファイルを処理する\n",
    "test_df56, LABEL_t56, MAE_t56, FINAL_QP_t56 = process_train_csv_lists(test_QP50_QP42)\n",
    "\n",
    "# 57番目のCSVファイルを処理する\n",
    "test_df57, LABEL_t57, MAE_t57, FINAL_QP_t57 = process_train_csv_lists(test_QP50_QP45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58番目のCSVファイルを処理する\n",
    "test_df58, LABEL_t58, MAE_t58, FINAL_QP_t58 = process_train_csv_lists(test_QP5_QP5)\n",
    "\n",
    "# 59番目のCSVファイルを処理する\n",
    "test_df59, LABEL_t59, MAE_t59, FINAL_QP_t59 = process_train_csv_lists(test_QP10_QP10)\n",
    "\n",
    "# 60番目のCSVファイルを処理する\n",
    "test_df60, LABEL_t60, MAE_t60, FINAL_QP_t60 = process_train_csv_lists(test_QP16_QP16)\n",
    "\n",
    "# 61番目のCSVファイルを処理する\n",
    "test_df61, LABEL_t61, MAE_t61, FINAL_QP_t61 = process_train_csv_lists(test_QP20_QP20)\n",
    "\n",
    "# 62番目のCSVファイルを処理する\n",
    "test_df62, LABEL_t62, MAE_t62, FINAL_QP_t62 = process_train_csv_lists(test_QP24_QP24)\n",
    "\n",
    "# 63番目のCSVファイルを処理する\n",
    "test_df63, LABEL_t63, MAE_t63, FINAL_QP_t63 = process_train_csv_lists(test_QP27_QP27)\n",
    "\n",
    "# 64番目のCSVファイルを処理する\n",
    "test_df64, LABEL_t64, MAE_t64, FINAL_QP_t64 = process_train_csv_lists(test_QP32_QP32)\n",
    "\n",
    "# 65番目のCSVファイルを処理する\n",
    "test_df65, LABEL_t65, MAE_t65, FINAL_QP_t65 = process_train_csv_lists(test_QP39_QP39)\n",
    "\n",
    "# 66番目のCSVファイルを処理する\n",
    "test_df66, LABEL_t66, MAE_t66, FINAL_QP_t66 = process_train_csv_lists(test_QP42_QP42)\n",
    "\n",
    "# 67番目のCSVファイルを処理する\n",
    "test_df67, LABEL_t67, MAE_t67, FINAL_QP_t67 = process_train_csv_lists(test_QP45_QP45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68番目のCSVファイルを処理する\n",
    "test_df68, LABEL_t68, MAE_t68, FINAL_QP_t68 = process_train_csv_lists(test_QP10_QP16)\n",
    "\n",
    "# 69番目のCSVファイルを処理する\n",
    "test_df69, LABEL_t69, MAE_t69, FINAL_QP_t69 = process_train_csv_lists(test_QP10_QP20)\n",
    "\n",
    "# 70番目のCSVファイルを処理する\n",
    "test_df70, LABEL_t70, MAE_t70, FINAL_QP_t70 = process_train_csv_lists(test_QP10_QP24)\n",
    "\n",
    "# 71番目のCSVファイルを処理する\n",
    "test_df71, LABEL_t71, MAE_t71, FINAL_QP_t71 = process_train_csv_lists(test_QP10_QP27)\n",
    "\n",
    "# 72番目のCSVファイルを処理する\n",
    "test_df72, LABEL_t72, MAE_t72, FINAL_QP_t72 = process_train_csv_lists(test_QP10_QP32)\n",
    "\n",
    "# 73番目のCSVファイルを処理する\n",
    "test_df73, LABEL_t73, MAE_t73, FINAL_QP_t73 = process_train_csv_lists(test_QP10_QP39)\n",
    "\n",
    "# 74番目のCSVファイルを処理する\n",
    "test_df74, LABEL_t74, MAE_t74, FINAL_QP_t74 = process_train_csv_lists(test_QP10_QP42)\n",
    "\n",
    "# 75番目のCSVファイルを処理する\n",
    "test_df75, LABEL_t75, MAE_t75, FINAL_QP_t75 = process_train_csv_lists(test_QP10_QP45)\n",
    "\n",
    "# 76番目のCSVファイルを処理する\n",
    "test_df76, LABEL_t76, MAE_t76, FINAL_QP_t76 = process_train_csv_lists(test_QP15_QP16)\n",
    "\n",
    "# 77番目のCSVファイルを処理する\n",
    "test_df77, LABEL_t77, MAE_t77, FINAL_QP_t77 = process_train_csv_lists(test_QP15_QP20)\n",
    "\n",
    "# 78番目のCSVファイルを処理する\n",
    "test_df78, LABEL_t78, MAE_t78, FINAL_QP_t78 = process_train_csv_lists(test_QP15_QP24)\n",
    "\n",
    "# 79番目のCSVファイルを処理する\n",
    "test_df79, LABEL_t79, MAE_t79, FINAL_QP_t79 = process_train_csv_lists(test_QP15_QP27)\n",
    "\n",
    "# 80番目のCSVファイルを処理する\n",
    "test_df80, LABEL_t80, MAE_t80, FINAL_QP_t80 = process_train_csv_lists(test_QP15_QP32)\n",
    "\n",
    "# 81番目のCSVファイルを処理する\n",
    "test_df81, LABEL_t81, MAE_t81, FINAL_QP_t81 = process_train_csv_lists(test_QP15_QP39)\n",
    "\n",
    "# 82番目のCSVファイルを処理する\n",
    "test_df82, LABEL_t82, MAE_t82, FINAL_QP_t82 = process_train_csv_lists(test_QP15_QP42)\n",
    "\n",
    "# 83番目のCSVファイルを処理する\n",
    "test_df83, LABEL_t83, MAE_t83, FINAL_QP_t83 = process_train_csv_lists(test_QP15_QP45)\n",
    "\n",
    "# 84番目のCSVファイルを処理する\n",
    "test_df84, LABEL_t84, MAE_t84, FINAL_QP_t84 = process_train_csv_lists(test_QP20_QP24)\n",
    "\n",
    "# 85番目のCSVファイルを処理する\n",
    "test_df85, LABEL_t85, MAE_t85, FINAL_QP_t85 = process_train_csv_lists(test_QP20_QP27)\n",
    "\n",
    "# 86番目のCSVファイルを処理する\n",
    "test_df86, LABEL_t86, MAE_t86, FINAL_QP_t86 = process_train_csv_lists(test_QP20_QP32)\n",
    "\n",
    "# 87番目のCSVファイルを処理する\n",
    "test_df87, LABEL_t87, MAE_t87, FINAL_QP_t87 = process_train_csv_lists(test_QP20_QP39)\n",
    "\n",
    "# 88番目のCSVファイルを処理する\n",
    "test_df88, LABEL_t88, MAE_t88, FINAL_QP_t88 = process_train_csv_lists(test_QP20_QP42)\n",
    "\n",
    "# 89番目のCSVファイルを処理する\n",
    "test_df89, LABEL_t89, MAE_t89, FINAL_QP_t89 = process_train_csv_lists(test_QP20_QP45)\n",
    "\n",
    "# 90番目のCSVファイルを処理する\n",
    "test_df90, LABEL_t90, MAE_t90, FINAL_QP_t90 = process_train_csv_lists(test_QP25_QP27)\n",
    "\n",
    "# 91番目のCSVファイルを処理する\n",
    "test_df91, LABEL_t91, MAE_t91, FINAL_QP_t91 = process_train_csv_lists(test_QP25_QP32)\n",
    "\n",
    "# 92番目のCSVファイルを処理する\n",
    "test_df92, LABEL_t92, MAE_t92, FINAL_QP_t92 = process_train_csv_lists(test_QP25_QP39)\n",
    "\n",
    "# 93番目のCSVファイルを処理する\n",
    "test_df93, LABEL_t93, MAE_t93, FINAL_QP_t93 = process_train_csv_lists(test_QP25_QP42)\n",
    "\n",
    "# 94番目のCSVファイルを処理する\n",
    "test_df94, LABEL_t94, MAE_t94, FINAL_QP_t94 = process_train_csv_lists(test_QP25_QP45)\n",
    "\n",
    "# 95番目のCSVファイルを処理する\n",
    "test_df95, LABEL_t95, MAE_t95, FINAL_QP_t95 = process_train_csv_lists(test_QP30_QP32)\n",
    "\n",
    "# 96番目のCSVファイルを処理する\n",
    "test_df96, LABEL_t96, MAE_t96, FINAL_QP_t96 = process_train_csv_lists(test_QP30_QP39)\n",
    "\n",
    "# 97番目のCSVファイルを処理する\n",
    "test_df97, LABEL_t97, MAE_t97, FINAL_QP_t97 = process_train_csv_lists(test_QP30_QP42)\n",
    "\n",
    "# 98番目のCSVファイルを処理する\n",
    "test_df98, LABEL_t98, MAE_t98, FINAL_QP_t98 = process_train_csv_lists(test_QP30_QP45)\n",
    "\n",
    "# 99番目のCSVファイルを処理する\n",
    "test_df99, LABEL_t99, MAE_t99, FINAL_QP_t99 = process_train_csv_lists(test_QP32_QP39)\n",
    "\n",
    "# 100番目のCSVファイルを処理する\n",
    "test_df100, LABEL_t100, MAE_t100, FINAL_QP_t100 = process_train_csv_lists(test_QP32_QP42)\n",
    "\n",
    "# 101番目のCSVファイルを処理する\n",
    "test_df101, LABEL_t101, MAE_t101, FINAL_QP_t101 = process_train_csv_lists(test_QP32_QP45)\n",
    "\n",
    "# 102番目のCSVファイルを処理する\n",
    "test_df102, LABEL_t102, MAE_t102, FINAL_QP_t102 = process_train_csv_lists(test_QP35_QP39)\n",
    "\n",
    "# 103番目のCSVファイルを処理する\n",
    "test_df103, LABEL_t103, MAE_t103, FINAL_QP_t103 = process_train_csv_lists(test_QP35_QP42)\n",
    "\n",
    "# 104番目のCSVファイルを処理する\n",
    "test_df104, LABEL_t104, MAE_t104, FINAL_QP_t104 = process_train_csv_lists(test_QP35_QP45)\n",
    "\n",
    "# 105番目のCSVファイルを処理する\n",
    "test_df105, LABEL_t105, MAE_t105, FINAL_QP_t105 = process_train_csv_lists(test_QP40_QP42)\n",
    "\n",
    "# 106番目のCSVファイルを処理する\n",
    "test_df106, LABEL_t106, MAE_t106, FINAL_QP_t106 = process_train_csv_lists(test_QP40_QP45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データフレームを結合\n",
    "combined_train_df = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9], ignore_index=True)\n",
    "combined_LABEL = pd.concat([LABEL1, LABEL2, LABEL3, LABEL4, LABEL5, LABEL6, LABEL7, LABEL8, LABEL9], ignore_index=True)\n",
    "combined_MAE = pd.concat([MAE1, MAE2, MAE3, MAE4, MAE5, MAE6, MAE7, MAE8, MAE9], ignore_index=True)\n",
    "combined_FINAL_QP = pd.concat([FINAL_QP1, FINAL_QP2, FINAL_QP3, FINAL_QP4, FINAL_QP5, FINAL_QP6, FINAL_QP7, FINAL_QP8, FINAL_QP9], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 44)\n",
      "(5400, 1)\n",
      "(5400, 1)\n",
      "(5400, 1)\n",
      "(60, 44)\n",
      "(60, 1)\n",
      "(60, 1)\n",
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df.shape)\n",
    "print(combined_LABEL.shape)\n",
    "print(combined_MAE.shape)\n",
    "print(combined_FINAL_QP.shape)\n",
    "\n",
    "print(test_df1.shape)\n",
    "print(LABEL_t1.shape)\n",
    "print(MAE_t1.shape)\n",
    "print(FINAL_QP_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "Combined Train DF:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10      0   3200   8832  17800  30168      0   3200   8768  17204  30828  10536  10279  1383   1986   1571   1463   2955   2635  10594  11555  1331   1956   1562   1469   2910   2614  14660  10796   6988   8392   2496  16668  14804  11088   7256   8648   2412  15792  0.000293  0.001652   0.000645   0.14634  0.084899\n",
      "1       16      0  22336   7488  10212  19964      0  22720   7312   9540  20428   7696   8890  2182   1923   2319   1186   2929   2591   7571  10194  2202   1872   2338   1207   2880   2685  13856   7180   6340   5616   1288  25720  12800   7476   7200   5644   1280  25600  0.000563  0.002236   0.001704  0.123604  0.046563\n",
      "2       20      0  24704   6288  11760  17248      0  24640   6400  11500  17460   7837   5424  2072   2535   2101    972   3454   2629   7959   6692  2188   2627   2114    838   3425   2848  10884   4064   4776   3956    808  35512  10600   3924   5104   4080    840  35452  0.000088   0.00384   0.000326  0.069767  0.029122\n",
      "3       24      0  27072   6576  11988  14364      0  27008   6816  11572  14604   7757   4049  1396   3318   1921    813   3362   2530   8222   4361  1424   3330   1893    761   3342   2584  10456   2268   3952   2424    760  40140  10288   2416   3912   2852    652  39880  0.000229  0.000641   0.000823  0.026323  0.014414\n",
      "4       27      0  27776   7536  12620  12068      0  27584   7904  12396  12116   8997   3606   884   3473   1519    722   3249   2494   9140   4250   896   3302   1479    683   3309   2465   6468   1892   2564   1860    716  46500   6428   1968   2484   1900    560  46660  0.000191  0.002113   0.000392  0.185069  0.173331\n",
      "\n",
      "Before scaling:\n",
      "Combined Test DF:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0        5      0  13312  13104  15800  17784      0  13376  12800  15088  18736  12986   6064  1360   2130   1361    872   1672   1504  12915   6470  1399   2112   1442    913   2034   1469  12460  14224   9344   8740    960  14272  12436  15220   8416   8820   1004  14104  0.000748  0.001446   0.001417  0.542942  0.356737\n",
      "1        5      0   6400  14432  20068  19100      0   6208  14528  19516  19748  10085  14943  1257   1599   1633   2081   2245   2548  10199  15664  1181   1511   1599   1984   2266   2350  14756   9808   8908  10528   3180  12820  14756  10004   8904  10632   3060  12644  0.000362  0.000865     0.0001   0.85164  0.674951\n",
      "2        5      0   8128   6640  13516  31716      0   8000   6784  12132  33084   7048   8126  4316   3839   2742   1238   1928    987   7345   8081  4275   3794   2786   1070   1847    962  15584   9276  12324   8764   2276  11776  15424   9828  12624   8808   2368  10948  0.001789  0.000713   0.000878  0.628591  0.448065\n",
      "3        5      0  29504   6496   7996  16004      0  29504   6496   7200  16800   7035  22528  1365   1129   1013    407    616    420   6756  23229  1294   1255    980    314    624    545   7660  15708   5892   4532    784  25424   7644  15644   5736   4544    768  25664  0.001027  0.001453   0.000059  0.406355   0.30084\n",
      "4        5      0   7872  11552  16996  23580      0   7808  11776  15888  24528  13776  12575  1260   1385    688   1190   1307   1045  14111  13067  1534   1196    636   1203   1522    976  14784  11540   9876   9476   4868   9456  14452  11780   9292   9736   5056   9684  0.000979  0.001769   0.000566  0.675589   0.42958\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\")\n",
    "print(\"Combined Train DF:\")\n",
    "print(combined_train_df.head())\n",
    "\n",
    "print(\"\\nBefore scaling:\")\n",
    "print(\"Combined Test DF:\")\n",
    "print(test_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling:\n",
      "X_train:\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.125  0.0  0.054348  0.288784  0.617027  0.525685  0.0  0.054289  0.287212  0.587247  0.535766  0.249845  0.278145  0.106182  0.081309  0.128766  0.102408  0.093499  0.275772  0.261058  0.309961  0.101636  0.079129  0.129747  0.099586  0.087802  0.280593  0.449693  0.433714  0.375215  0.473910  0.159306  0.215062  0.466768  0.434483  0.372561  0.476002  0.143743  0.204090  0.012933  0.009490  0.001161  0.144579  0.083548\n",
      "1  0.275  0.0  0.379348  0.244759  0.353993  0.347878  0.0  0.385451  0.239518  0.325642  0.355023  0.152217  0.231564  0.167929  0.078725  0.191027  0.083018  0.092585  0.271167  0.152520  0.264862  0.168549  0.075692  0.194370  0.081825  0.086794  0.288214  0.425031  0.288446  0.340421  0.317145  0.082206  0.379095  0.403582  0.292947  0.369686  0.310656  0.076281  0.380671  0.024907  0.012853  0.003066  0.121795  0.045154\n",
      "2  0.375  0.0  0.419565  0.205451  0.407654  0.300551  0.0  0.418024  0.209644  0.392545  0.303441  0.157064  0.115329  0.159428  0.103832  0.172882  0.068039  0.111052  0.275144  0.166451  0.148817  0.167473  0.106583  0.175716  0.056810  0.105108  0.305711  0.333865  0.163265  0.256443  0.223402  0.051570  0.556538  0.334216  0.153762  0.262066  0.224571  0.050060  0.558044  0.003885  0.022104  0.000587  0.067847  0.027687\n",
      "3  0.475  0.0  0.459783  0.214885  0.415557  0.250296  0.0  0.458198  0.223270  0.395003  0.253806  0.154314  0.069218  0.107187  0.135953  0.157899  0.056909  0.107816  0.264783  0.175894  0.071575  0.108781  0.135346  0.157312  0.051590  0.102319  0.277372  0.320736  0.091114  0.212199  0.136887  0.048507  0.640403  0.324379  0.094671  0.200863  0.156979  0.038856  0.637765  0.010109  0.003658  0.001480  0.024313  0.012956\n",
      "4  0.550  0.0  0.471739  0.246331  0.437465  0.210288  0.0  0.467970  0.258910  0.423129  0.210567  0.196941  0.054361  0.067620  0.142312  0.124438  0.050539  0.103841  0.261015  0.208854  0.067897  0.068218  0.134201  0.122835  0.046302  0.101210  0.264599  0.198405  0.076008  0.137672  0.105037  0.045698  0.755654  0.202674  0.077116  0.127542  0.104579  0.033373  0.759830  0.008453  0.012143  0.000705  0.183388  0.172113\n",
      "\n",
      "Test data after scaling (test_df1):\n",
      "    0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.0  0.0  0.226087  0.428721  0.547698  0.309891  0.0  0.226927  0.419287  0.515019  0.325617  0.334067  0.136792  0.104405  0.087217  0.111287  0.061039  0.048368  0.157405  0.344392  0.141461  0.106860  0.085512  0.119753  0.061894  0.058367  0.157686  0.382209  0.571429  0.501718  0.493562  0.061271  0.171644  0.392105  0.596395  0.432122  0.485469  0.059833  0.173700  0.033071  0.008298  0.002550  0.542003  0.355794\n",
      "1  0.0  0.0  0.108696  0.472222  0.695646  0.332822  0.0  0.105320  0.475891  0.666166  0.343205  0.234342  0.434555  0.096445  0.065433  0.133927  0.145667  0.068524  0.266667  0.246876  0.446120  0.090113  0.060922  0.132828  0.134499  0.066163  0.252254  0.452638  0.394022  0.478308  0.594534  0.202961  0.145332  0.465254  0.392006  0.457178  0.585205  0.182360  0.147415  0.016017  0.004952  0.000179  0.851341  0.674486\n",
      "2  0.0  0.0  0.138043  0.216981  0.468525  0.552659  0.0  0.135722  0.222222  0.414118  0.574974  0.129942  0.205943  0.332844  0.157327  0.226236  0.086658  0.057373  0.103297  0.144406  0.194844  0.327802  0.154331  0.231679  0.072537  0.052083  0.103263  0.478037  0.372650  0.661727  0.494918  0.145264  0.126413  0.486316  0.385110  0.648182  0.484808  0.141120  0.116880  0.079096  0.004074  0.001580  0.627830  0.447259\n",
      "3  0.0  0.0  0.501087  0.212264  0.277177  0.278874  0.0  0.500543  0.212788  0.245767  0.291971  0.129495  0.688923  0.104791  0.046152  0.082321  0.028489  0.011221  0.043956  0.123259  0.696799  0.098794  0.050448  0.081279  0.021287  0.010988  0.058502  0.234969  0.631046  0.316366  0.255930  0.050038  0.373732  0.241014  0.613009  0.294516  0.250110  0.045769  0.381823  0.045407  0.008339  0.000107  0.405132  0.299813\n",
      "4  0.0  0.0  0.133696  0.377883  0.589157  0.410887  0.0  0.132465  0.385744  0.542327  0.426277  0.361224  0.355143  0.096677  0.056654  0.055269  0.083298  0.035528  0.109367  0.387333  0.360064  0.117231  0.048034  0.052632  0.081554  0.041163  0.104766  0.453497  0.463603  0.530284  0.535125  0.310697  0.084372  0.455669  0.461599  0.477100  0.535887  0.301311  0.094124  0.043278  0.010162  0.001019  0.674926  0.428746\n"
     ]
    }
   ],
   "source": [
    "def process_results_to_lists(train_df, LABEL, MAE, FINAL_QP, scaler_main=None, fit_scaler=True):\n",
    "    if fit_scaler:\n",
    "        scaler_main = MinMaxScaler()\n",
    "        X_train = scaler_main.fit_transform(train_df)\n",
    "    else:\n",
    "        X_train = scaler_main.transform(train_df)\n",
    "\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "    Y_train = LABEL['LABEL'].astype(int).values\n",
    "\n",
    "    return X_train, MAE_array, FINAL_QP_array, Y_train, scaler_main\n",
    "\n",
    "# 訓練データのスケーリング\n",
    "X_train, MAE_array, FINAL_QP_array, Y_train, scaler_main = process_results_to_lists(\n",
    "    combined_train_df, combined_LABEL, combined_MAE, combined_FINAL_QP, fit_scaler=True\n",
    ")\n",
    "\n",
    "# スケーリング後のデータを表示（任意）\n",
    "print(\"After scaling:\")\n",
    "print(\"X_train:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "\n",
    "# データを元に戻すための関数\n",
    "def restore_data_to_original_order(data, original_lengths):\n",
    "    restored_data = []\n",
    "    start_index = 0\n",
    "    for length in original_lengths:\n",
    "        restored_data.append(data[start_index:start_index + length])\n",
    "        start_index += length\n",
    "    return restored_data\n",
    "\n",
    "# 元のデータフレームの長さ\n",
    "original_lengths = [len(train_df1), len(train_df2), len(train_df3), len(train_df4), len(train_df5), \n",
    "                    len(train_df6), len(train_df7), len(train_df8), len(train_df9)]\n",
    "\n",
    "# データを元の順序に戻す\n",
    "X_train_list = restore_data_to_original_order(X_train, original_lengths)\n",
    "MAE_list = restore_data_to_original_order(MAE_array, original_lengths)\n",
    "FINAL_QP_list = restore_data_to_original_order(FINAL_QP_array, original_lengths)\n",
    "Y_train_list = restore_data_to_original_order(Y_train, original_lengths)\n",
    "\n",
    "# テストデータのスケーリング関数\n",
    "def append_results_to_lists(train_df, LABEL, MAE, FINAL_QP, X_train_list, MAE_list, FINAL_QP_list, Y_train_list, scaler_main=None, fit_scaler=True):\n",
    "    X_train, MAE_array, FINAL_QP_array, Y_train, _ = process_results_to_lists(\n",
    "        train_df, LABEL, MAE, FINAL_QP, scaler_main, fit_scaler)\n",
    "    X_train_list.append(X_train)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "    return X_train_list, MAE_list, FINAL_QP_list, Y_train_list\n",
    "\n",
    "# テストデータ用の辞書を初期化\n",
    "X_test_dict = {}\n",
    "MAE_test_dict = {}\n",
    "FINAL_QP_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "# テストデータを処理して辞書に追加\n",
    "for i in range(1, 107):\n",
    "    test_df = globals()[f'test_df{i}']\n",
    "    LABEL_t = globals()[f'LABEL_t{i}']\n",
    "    MAE_t = globals()[f'MAE_t{i}']\n",
    "    FINAL_QP_t = globals()[f'FINAL_QP_t{i}']\n",
    "    \n",
    "    X_test_dict[i] = []\n",
    "    MAE_test_dict[i] = []\n",
    "    FINAL_QP_test_dict[i] = []\n",
    "    Y_test_dict[i] = []\n",
    "    \n",
    "    X_test_dict[i], MAE_test_dict[i], FINAL_QP_test_dict[i], Y_test_dict[i] = append_results_to_lists(\n",
    "        test_df, LABEL_t, MAE_t, FINAL_QP_t, X_test_dict[i], MAE_test_dict[i], FINAL_QP_test_dict[i], Y_test_dict[i], scaler_main, fit_scaler=False\n",
    "    )\n",
    "\n",
    "# 確認用の出力\n",
    "for i in range(1, 2):\n",
    "    print(f\"\\nTest data after scaling (test_df{i}):\")\n",
    "    print(pd.DataFrame(X_test_dict[i][0]).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 8]\n",
      "Test indices: [7]\n",
      "4800\n",
      "600\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8]\n",
      "Test indices: [1]\n",
      "4800\n",
      "600\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8]\n",
      "Test indices: [5]\n",
      "4800\n",
      "600\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8]\n",
      "Test indices: [0]\n",
      "4800\n",
      "600\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [8]\n",
      "4800\n",
      "600\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8]\n",
      "Test indices: [2]\n",
      "4800\n",
      "600\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 5 6 7 8]\n",
      "Test indices: [4]\n",
      "4800\n",
      "600\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 4 5 6 7 8]\n",
      "Test indices: [3]\n",
      "4800\n",
      "600\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 3 4 5 7 8]\n",
      "Test indices: [6]\n",
      "4800\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "kfold = KFold(n_splits=9, shuffle=True, random_state=42)\n",
    "\n",
    "# データフレームを初期化\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# 1から106までの列名を作成し、データフレームに追加\n",
    "columns = []\n",
    "for i in range(1, 107):\n",
    "    columns.extend([\n",
    "        f'C_RBF{i}', f'Score_RBF{i}', f'tnr_rbf{i}', f'tpr_rbf{i}', f'AUC_RBF{i}',\n",
    "        f'C_LINEAR{i}', f'Score_LINEAR{i}', f'tnr_linear{i}', f'tpr_linear{i}', f'AUC_LINEAR{i}',\n",
    "        f'Threshold{i}', f'Score_old{i}', f'tnr_old{i}', f'tpr_old{i}', f'AUC_old{i}'\n",
    "    ])\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "X_index = np.arange(9)  # インデックスとして0から8までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "        \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    print(len(Y_train))\n",
    "    print(len(Y_val))\n",
    "    \n",
    "    # リストの作成（1から106まで）\n",
    "    for i in range(1, 107):\n",
    "        globals()[f'test_data{i}'] = [item for data in X_test_dict[i] for item in data]\n",
    "        globals()[f'test_label{i}'] = [item for data in Y_test_dict[i] for item in data]\n",
    "        globals()[f'MAE_data{i}'] = [item for data in MAE_test_dict[i] for item in data]\n",
    "        globals()[f'FINAL_QP_data{i}'] = [item for data in FINAL_QP_test_dict[i] for item in data]\n",
    "\n",
    "        globals()[f'best_threshold{i}'] = 0\n",
    "        globals()[f'best_accuracy{i}'] = 0\n",
    "        globals()[f'best_predicted_labels{i}'] = []\n",
    "        globals()[f'best_ground_truth_labels{i}'] = []\n",
    "        globals()[f'tnr_old{i}'] = 0\n",
    "        globals()[f'tpr_old{i}'] = 0\n",
    "        \n",
    "        for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "            test_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(60)])\n",
    "            predicted_labels = test_old.astype(int)\n",
    "            ground_truth_labels = np.array(globals()[f'test_label{i}'])\n",
    "            accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "            if accuracy > globals()[f'best_accuracy{i}']:\n",
    "                globals()[f'best_accuracy{i}'] = accuracy\n",
    "                globals()[f'best_threshold{i}'] = threshold\n",
    "                globals()[f'best_predicted_labels{i}'] = predicted_labels\n",
    "                globals()[f'best_ground_truth_labels{i}'] = ground_truth_labels\n",
    "\n",
    "\n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value, probability=True)\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value, probability=True)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "    \n",
    "    fold_results = {}\n",
    "    for i in range(1, 107):\n",
    "        # RBFモデルの評価\n",
    "        predictions_RBF = best_svm_model_RBF.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_RBF = best_svm_model_RBF.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_RBF = accuracy_score(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'accuracy_RBF{i}'] = accuracy_RBF\n",
    "        report_RBF = classification_report(globals()[f'test_label{i}'], predictions_RBF, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'tnr_rbf{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_rbf{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_rbf, tpr_rbf, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_RBF)\n",
    "        auc_rbf = auc(fpr_rbf, tpr_rbf)\n",
    "        globals()[f'auc_rbf{i}'] = auc_rbf\n",
    "        # print(report_RBF)\n",
    "\n",
    "        # LINEARモデルの評価\n",
    "        predictions_LINEAR = best_svm_model_LINEAR.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_LINEAR = best_svm_model_LINEAR.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_LINEAR = accuracy_score(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'accuracy_LINEAR{i}'] = accuracy_LINEAR\n",
    "        report_LINEAR = classification_report(globals()[f'test_label{i}'], predictions_LINEAR, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'tnr_linear{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_linear{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_linear, tpr_linear, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_LINEAR)\n",
    "        auc_linear = auc(fpr_linear, tpr_linear)\n",
    "        globals()[f'auc_linear{i}'] = auc_linear\n",
    "\n",
    "        # Old modelの評価\n",
    "        thresholds = np.arange(0.00, 1.01, 0.01)\n",
    "        tpr_old_list = []\n",
    "        fpr_old_list = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(60)])\n",
    "            tn, fp, fn, tp = confusion_matrix(globals()[f'test_label{i}'], predicted_labels_old).ravel()\n",
    "            tpr_old = tp / (tp + fn)\n",
    "            fpr_old = fp / (fp + tn)\n",
    "            tpr_old_list.append(tpr_old)\n",
    "            fpr_old_list.append(fpr_old)\n",
    "        \n",
    "        auc_old = auc(fpr_old_list, tpr_old_list)\n",
    "        globals()[f'auc_old{i}'] = auc_old\n",
    "\n",
    "        # fold_resultsに保存\n",
    "        fold_results[f'C_RBF{i}'] = best_c_value_RBF\n",
    "        fold_results[f'Score_RBF{i}'] = globals()[f'accuracy_RBF{i}']\n",
    "        fold_results[f'tnr_rbf{i}'] = globals()[f'tnr_rbf{i}']\n",
    "        fold_results[f'tpr_rbf{i}'] = globals()[f'tpr_rbf{i}']\n",
    "        fold_results[f'AUC_RBF{i}'] = globals()[f'auc_rbf{i}']\n",
    "\n",
    "        fold_results[f'C_LINEAR{i}'] = best_c_value_LINEAR\n",
    "        fold_results[f'Score_LINEAR{i}'] = globals()[f'accuracy_LINEAR{i}']\n",
    "        fold_results[f'tnr_linear{i}'] = globals()[f'tnr_linear{i}']\n",
    "        fold_results[f'tpr_linear{i}'] = globals()[f'tpr_linear{i}']\n",
    "        fold_results[f'AUC_LINEAR{i}'] = globals()[f'auc_linear{i}']\n",
    "\n",
    "        fold_results[f'Threshold{i}'] = globals()[f'best_threshold{i}']\n",
    "        fold_results[f'Score_old{i}'] = globals()[f'best_accuracy{i}']\n",
    "        fold_results[f'tnr_old{i}'] = globals()[f'tnr_old{i}']\n",
    "        fold_results[f'tpr_old{i}'] = globals()[f'tpr_old{i}']\n",
    "        fold_results[f'AUC_old{i}'] = globals()[f'auc_old{i}']\n",
    "\n",
    "    # 結果をデータフレームに追加\n",
    "    results = pd.concat([results, pd.DataFrame(fold_results, index=[fold])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0      RBF1        99.26        81.85               90.56                2.04           93.33           86.67         0.99     0.00     1.00     0.99\n",
      "1      RBF2        99.26        97.78               98.52                1.55          100.00           95.00         1.00     0.01     1.00     0.98\n",
      "2      RBF3        87.04        76.30               81.67                2.64           85.00           78.33         0.92     0.01     0.94     0.90\n",
      "3      RBF4        99.26       100.00               99.63                0.73          100.00           98.33         1.00     0.00     1.00     1.00\n",
      "4      RBF5        87.04        94.81               90.93                1.88           93.33           86.67         0.97     0.01     0.98     0.93\n",
      "..      ...          ...          ...                 ...                 ...             ...             ...          ...      ...      ...      ...\n",
      "313  OLD102         0.00         0.00               65.00                0.00           65.00           65.00         0.65     0.00     0.65     0.65\n",
      "314  OLD103         0.00         0.00               60.00                0.00           60.00           60.00         0.58     0.00     0.58     0.58\n",
      "315  OLD104         0.00         0.00               60.00                0.00           60.00           60.00         0.60     0.00     0.60     0.60\n",
      "316  OLD105         0.00         0.00               68.33                0.00           68.33           68.33         0.68     0.00     0.68     0.68\n",
      "317  OLD106         0.00         0.00               61.67                0.00           61.67           61.67         0.58     0.00     0.58     0.58\n",
      "\n",
      "[318 rows x 11 columns]\n",
      "           Model  Average TNR  Average TPR  Average Test Score  Test Score STD  Test Score MAX  Test Score MIN  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0       RBF_1_57        89.20        97.34               93.27            0.73          100.00           75.00         0.99     0.01     1.00     0.85\n",
      "1      RBF_58_67        83.85        90.74               87.30            0.92           98.33           68.33         0.93     0.01     1.00     0.81\n",
      "2     RBF_68_106        76.42        59.09               67.75            1.23           95.00           41.67         0.74     0.01     0.99     0.45\n",
      "3    LINEAR_1_57        84.04        97.35               90.69            0.44          100.00           75.00         0.98     0.00     1.00     0.86\n",
      "4   LINEAR_58_67        78.48        89.96               84.22            0.74           98.33           73.33         0.91     0.01     0.99     0.81\n",
      "5  LINEAR_68_106        70.77        58.97               64.87            0.54           91.67           45.00         0.71     0.00     0.97     0.48\n",
      "6       OLD_1_57         0.00         0.00               93.74            0.00          100.00           50.00         0.94     0.00     1.00     0.41\n",
      "7      OLD_58_67         0.00         0.00               54.17            0.00           65.00           50.00         0.40     0.00     0.60     0.18\n",
      "8     OLD_68_106         0.00         0.00               56.71            0.00           68.33           50.00         0.53     0.00     0.69     0.33\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第2位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': [f'RBF{i}' for i in range(1, 107)] + [f'LINEAR{i}' for i in range(1, 107)] + [f'OLD{i}' for i in range(1, 107)],\n",
    "    'Average TNR': [\n",
    "        round(results[f'tnr_rbf{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tnr_linear{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tnr_old{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Average TPR': [\n",
    "        round(results[f'tpr_rbf{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tpr_linear{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tpr_old{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Average Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Standard Deviation': [\n",
    "        round(results[f'Score_RBF{i}'].std() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].std() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].std() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Max Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].max() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].max() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].max() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Min Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].min() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].min() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].min() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Average AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].mean(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].mean(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].mean(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'AUC STD': [\n",
    "        round(results[f'AUC_RBF{i}'].std(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].std(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].std(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Max AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].max(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].max(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].max(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Min AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].min(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].min(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].min(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)\n",
    "\n",
    "\n",
    "\n",
    "# 関数を定義して、各セグメントの統計情報を計算\n",
    "def calculate_statistics(segment, prefix):\n",
    "    # モデル番号を抽出してフラットなリストに変換\n",
    "    model_numbers = statistics_df['Model'].str.extract(r'(\\d+)').astype(int)[0]\n",
    "    is_in_segment = model_numbers.isin(segment)\n",
    "    is_correct_prefix = statistics_df['Model'].str.startswith(prefix)\n",
    "    \n",
    "    tnr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TNR'].mean(), 2)\n",
    "    tpr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TPR'].mean(), 2)\n",
    "    acc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average Test Score'].mean(), 2)\n",
    "    acc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Standard Deviation'].std(), 2)\n",
    "    \n",
    "    acc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max Test Score'].max(), 2)\n",
    "    acc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min Test Score'].min(), 2)\n",
    "\n",
    "    auc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average AUC'].mean(), 2)\n",
    "    auc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'AUC STD'].std(), 2)\n",
    "    auc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max AUC'].max(), 2)\n",
    "    auc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min AUC'].min(), 2)\n",
    "    \n",
    "    return tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min\n",
    "\n",
    "# セグメントを定義\n",
    "segments = {\n",
    "    '1_57': list(range(1, 58)),\n",
    "    '58_67': list(range(58, 68)),\n",
    "    '68_106': list(range(68, 107))\n",
    "}\n",
    "\n",
    "# 結果を保存するリスト\n",
    "results_summary = []\n",
    "\n",
    "# 統計情報を計算して表示\n",
    "for model in ['RBF', 'LINEAR', 'OLD']:\n",
    "    for segment_name, segment in segments.items():\n",
    "        tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min = calculate_statistics(segment, model)\n",
    "        results_summary.append({\n",
    "            'Model': f'{model}_{segment_name}',\n",
    "            'Average TNR': tnr_mean,\n",
    "            'Average TPR': tpr_mean,\n",
    "            'Average Test Score': acc_mean,\n",
    "            'Test Score STD': acc_std,\n",
    "            'Test Score MAX': acc_max,\n",
    "            'Test Score MIN': acc_min,\n",
    "            'Average AUC': auc_mean,\n",
    "            'AUC STD': auc_std,\n",
    "            'Max AUC': auc_max,\n",
    "            'Min AUC': auc_min\n",
    "        })\n",
    "\n",
    "# DataFrameに変換\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# 表示\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     100\n",
      "1     100\n",
      "2    2000\n",
      "3     100\n",
      "4     100\n",
      "5     100\n",
      "6     100\n",
      "7     100\n",
      "8     100\n",
      "Name: C_RBF1, dtype: object\n",
      "0     100\n",
      "1    2000\n",
      "2     100\n",
      "3    4000\n",
      "4    2000\n",
      "5     100\n",
      "6    2000\n",
      "7    1000\n",
      "8      10\n",
      "Name: C_LINEAR1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF1'])\n",
    "print(results['C_LINEAR1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
