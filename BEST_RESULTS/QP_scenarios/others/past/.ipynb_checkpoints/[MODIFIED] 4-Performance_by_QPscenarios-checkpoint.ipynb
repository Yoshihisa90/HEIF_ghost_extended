{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示\n",
    "\n",
    "plt.rcParams[\"font.size\"]=5\n",
    "plt.rcParams[\"figure.figsize\"]=(2.0, 1.0)\n",
    "plt.rcParams[\"figure.dpi\"]= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, 0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        if (right_energy / energy) > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False  \n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference, mae_difference_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_list1:  3080\n",
      "single_list2:  3080\n",
      "\n",
      "second_largeQP1_list1:  17556\n",
      "second_largeQP1_list2:  17556\n",
      "second_sameQP_list1:  3080\n",
      "second_sameQP_list2:  3080\n",
      "second_largeQP_list1:  12012\n",
      "second_largeQP_list2:  12012\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "\n",
    "single_path1 = os.path.join(rootpath_csv, 'HEIF_images_single_csv')\n",
    "single_path2 = os.path.join(rootpath_csv, 'HEIF_images_second_sameQP_csv')\n",
    "single_list1 = [os.path.join(single_path1, file) for file in sorted(os.listdir(single_path1))]\n",
    "single_list2 = [os.path.join(single_path2, file) for file in sorted(os.listdir(single_path2))]\n",
    "\n",
    "second_largeQP1_path1 = os.path.join(rootpath_csv, 'HEIF_images_second_csv')\n",
    "second_largeQP1_path2 = os.path.join(rootpath_csv, 'HEIF_images_triple_csv')\n",
    "second_largeQP1_list1 = [os.path.join(second_largeQP1_path1, file) for file in sorted(os.listdir(second_largeQP1_path1))]\n",
    "second_largeQP1_list2 = [os.path.join(second_largeQP1_path2, file) for file in sorted(os.listdir(second_largeQP1_path2))]\n",
    "\n",
    "second_sameQP_path1 = os.path.join(rootpath_csv, 'HEIF_images_second_sameQP_csv')\n",
    "second_sameQP_path2 = os.path.join(rootpath_csv, 'HEIF_images_triple_sameQP_csv')\n",
    "second_sameQP_list1 = [os.path.join(second_sameQP_path1, file) for file in sorted(os.listdir(second_sameQP_path1))]\n",
    "second_sameQP_list2 = [os.path.join(second_sameQP_path2, file) for file in sorted(os.listdir(second_sameQP_path2))]\n",
    "\n",
    "second_largeQP2_path1 = os.path.join(rootpath_csv, 'HEIF_images_second_largeQP_csv')\n",
    "second_largeQP2_path2 = os.path.join(rootpath_csv, 'HEIF_images_triple_largeQP_csv')\n",
    "second_largeQP2_list1 = [os.path.join(second_largeQP2_path1, file) for file in sorted(os.listdir(second_largeQP2_path1))]\n",
    "second_largeQP2_list2 = [os.path.join(second_largeQP2_path2, file) for file in sorted(os.listdir(second_largeQP2_path2))]\n",
    "\n",
    "print(\"single_list1: \", len(single_list1))\n",
    "print(\"single_list2: \", len(single_list2))\n",
    "print()\n",
    "print(\"second_largeQP1_list1: \", len(second_largeQP1_list1))\n",
    "print(\"second_largeQP1_list2: \", len(second_largeQP1_list2))\n",
    "print(\"second_sameQP_list1: \", len(second_sameQP_list1))\n",
    "print(\"second_sameQP_list2: \", len(second_sameQP_list2))\n",
    "print(\"second_largeQP_list1: \", len(second_largeQP2_list1))\n",
    "print(\"second_largeQP_list2: \", len(second_largeQP2_list2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_listA:  3080\n",
      "single_listB:  3080\n",
      "\n",
      "second_largeQP1_listA:  17556\n",
      "second_largeQP1_listB:  17556\n",
      "second_sameQP_listA:  3080\n",
      "second_sameQP_listB:  3080\n",
      "second_largeQP2_listA:  12012\n",
      "second_largeQP2_listB:  12012\n"
     ]
    }
   ],
   "source": [
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "single_pathA = os.path.join(rootpath_pkl, 'pkl_single')\n",
    "single_pathB = os.path.join(rootpath_pkl, 'pkl_second_sameQP')\n",
    "single_listA = [os.path.join(single_pathA, file) for file in sorted(os.listdir(single_pathA))]\n",
    "single_listB = [os.path.join(single_pathB, file) for file in sorted(os.listdir(single_pathB))]\n",
    "\n",
    "second_largeQP1_pathA = os.path.join(rootpath_pkl, 'pkl_second')\n",
    "second_largeQP1_pathB = os.path.join(rootpath_pkl, 'pkl_triple')\n",
    "second_largeQP1_listA = [os.path.join(second_largeQP1_pathA, file) for file in sorted(os.listdir(second_largeQP1_pathA))]\n",
    "second_largeQP1_listB = [os.path.join(second_largeQP1_pathB, file) for file in sorted(os.listdir(second_largeQP1_pathB))]\n",
    "\n",
    "second_sameQP_pathA = os.path.join(rootpath_pkl, 'pkl_second_sameQP')\n",
    "second_sameQP_pathB = os.path.join(rootpath_pkl, 'pkl_triple_sameQP')\n",
    "second_sameQP_listA = [os.path.join(second_sameQP_pathA, file) for file in sorted(os.listdir(second_sameQP_pathA))]\n",
    "second_sameQP_listB = [os.path.join(second_sameQP_pathB, file) for file in sorted(os.listdir(second_sameQP_pathB))]\n",
    "\n",
    "second_largeQP2_pathA = os.path.join(rootpath_pkl, 'pkl_second_largeQP')\n",
    "second_largeQP2_pathB = os.path.join(rootpath_pkl, 'pkl_triple_largeQP')\n",
    "second_largeQP2_listA = [os.path.join(second_largeQP2_pathA, file) for file in sorted(os.listdir(second_largeQP2_pathA))]\n",
    "second_largeQP2_listB = [os.path.join(second_largeQP2_pathB, file) for file in sorted(os.listdir(second_largeQP2_pathB))]\n",
    "\n",
    "print(\"single_listA: \", len(single_listA))\n",
    "print(\"single_listB: \", len(single_listB))\n",
    "print()\n",
    "print(\"second_largeQP1_listA: \", len(second_largeQP1_listA))\n",
    "print(\"second_largeQP1_listB: \", len(second_largeQP1_listB))\n",
    "print(\"second_sameQP_listA: \", len(second_sameQP_listA))\n",
    "print(\"second_sameQP_listB: \", len(second_sameQP_listB))\n",
    "print(\"second_largeQP2_listA: \", len(second_largeQP2_listA))\n",
    "print(\"second_largeQP2_listB: \", len(second_largeQP2_listB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  5400\n"
     ]
    }
   ],
   "source": [
    "single_csv1 = list(zip(single_list1, single_listA, single_list2, single_listB))\n",
    "single_csv = random.sample(single_csv1, 2700)\n",
    "\n",
    "second_largeQP1_csv1 = list(zip(second_largeQP1_list1, second_largeQP1_listA, second_largeQP1_list2, second_largeQP1_listB))\n",
    "second_largeQP1_csv = random.sample(second_largeQP1_csv1, 900)\n",
    "\n",
    "second_sameQP_csv1 = list(zip(second_sameQP_list1, second_sameQP_listA, second_sameQP_list2, second_sameQP_listB))\n",
    "second_sameQP_csv = random.sample(second_sameQP_csv1, 900)\n",
    "\n",
    "second_largeQP2_csv1 = list(zip(second_largeQP2_list1, second_largeQP2_listA, second_largeQP2_list2, second_largeQP2_listB))\n",
    "second_largeQP2_csv = random.sample(second_largeQP2_csv1, 900)\n",
    "\n",
    "\n",
    "train_csv_list = single_csv + second_largeQP1_csv + second_sameQP_csv + second_largeQP2_csv\n",
    "# train_csv_list = second_largeQP1_csv\n",
    "print(\"train_csv_list: \", len(train_csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv_largeQP1_list:  600\n",
      "test_csv_sameQP_list:  600\n",
      "test_csv_largeQP2_list:  600\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "single_test_csv = [item for item in single_csv1 if item not in single_csv]\n",
    "second_largeQP1_test_csv = [item for item in second_largeQP1_csv1 if item not in second_largeQP1_csv]\n",
    "second_sameQP_test_csv = [item for item in second_sameQP_csv1 if item not in second_sameQP_csv]\n",
    "second_largeQP2_test_csv = [item for item in second_largeQP2_csv1 if item not in second_largeQP2_csv]\n",
    "\n",
    "\n",
    "single_test_csv = random.sample(single_test_csv, 300)\n",
    "second_largeQP1_test_csv = random.sample(second_largeQP1_test_csv, 300)\n",
    "second_sameQP_test_csv = random.sample(second_sameQP_test_csv, 300)\n",
    "second_largeQP2_test_csv = random.sample(second_largeQP2_test_csv, 300)\n",
    "\n",
    "test_csv_largeQP1_list = single_test_csv + second_largeQP1_test_csv\n",
    "print(\"test_csv_largeQP1_list: \", len(test_csv_largeQP1_list))\n",
    "\n",
    "test_csv_sameQP_list = single_test_csv + second_sameQP_test_csv\n",
    "print(\"test_csv_sameQP_list: \", len(test_csv_sameQP_list))\n",
    "\n",
    "test_csv_largeQP2_list = single_test_csv + second_largeQP2_test_csv\n",
    "print(\"test_csv_largeQP2_list: \", len(test_csv_largeQP2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "              \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "                     \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "                     \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "                     \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "                     \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "                     \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "                     \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                     \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "                     \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                     \n",
    "                     \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "                     \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "                     \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "                     \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "                     \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "                     \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "                     \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "                     \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "                     \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "\n",
    "chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                       \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "\n",
    "label_columns = [\"LABEL\"]\n",
    "mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "mae_columns = [\"MAE\"]\n",
    "final_qp_columns = [\"FINAL_QP\"]\n",
    "\n",
    "# データフレームを初期化\n",
    "train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "train_df2 = pd.DataFrame(columns=label_columns)\n",
    "train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "train_df5 = pd.DataFrame(columns=mae_columns)\n",
    "train_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in train_csv_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    train_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    # lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "    \n",
    "#     lu_values_10_1 = [df1.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "#     lu_values_10_2 = [df2.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "    \n",
    "#     lu_values_26_1 = [df1.loc[i, \"luminance_counts\"] for i in [25,26,27]] \n",
    "#     lu_values_26_2 = [df2.loc[i, \"luminance_counts\"] for i in [25,26,27]]\n",
    "\n",
    "#     average_10_1 = np.mean(lu_values_10_1)\n",
    "#     average_10_2 = np.mean(lu_values_10_2)\n",
    "#     average_26_1 = np.mean(lu_values_26_1)\n",
    "#     average_26_2 = np.mean(lu_values_26_2)\n",
    "    \n",
    "    \n",
    "#     lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_1)] + [int(average_26_1)] + [df2.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_2)] + [int(average_26_2)]\n",
    "    \n",
    "    \n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "    \n",
    "    train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "\n",
    "    # label_columnsの値を取得\n",
    "    train_df2 = pd.concat([train_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_positive = calculate_mae(train_pkl_list[0])\n",
    "    _, mae_d2_positive = calculate_mae(train_pkl_list[1])\n",
    "    \n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "    \n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    train_df5 = pd.concat([train_df5, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    train_df6 = pd.concat([train_df6, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "# インデックスをリセット\n",
    "train_df1_1.reset_index(drop=True, inplace=True)\n",
    "train_df1_2.reset_index(drop=True, inplace=True)\n",
    "train_df1_3.reset_index(drop=True, inplace=True)\n",
    "train_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "train_df_onlyGhost = pd.concat([train_df3, train_df4], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.00494826, -0.00534774, -0.00591996, -0.00739097, -0.00924989,\n",
      "        -0.01015805, -0.01309762, -0.01419954, -0.02078163, -0.02188258,\n",
      "        -0.02510411, -0.02583705, -0.03375378, -0.03098781, -0.04228424,\n",
      "        -0.02620586, -0.04843276, -0.0210853 , -0.05460354, -0.02420201,\n",
      "         0.03224501,  0.09337155, -0.11092163, -0.20282124, -0.15733879,\n",
      "         0.07156923,  0.38986268,  0.87613081,  0.40213897,  0.15887546,\n",
      "        -0.00105449, -0.10485349, -0.1176418 , -0.11154701, -0.08284428,\n",
      "        -0.07496825, -0.08898361, -0.12534001, -0.10930927, -0.10393965,\n",
      "        -0.09932991, -0.13316591, -0.12646066, -0.12338323, -0.09934324,\n",
      "        -0.12927381, -0.07989486, -0.11659081, -0.12796584, -0.1425371 ,\n",
      "        -0.07236386, -0.02385576])                                      ]\n",
      "[[42]\n",
      " [32]\n",
      " [24]\n",
      " ...\n",
      " [39]\n",
      " [39]\n",
      " [32]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df5.values[10])\n",
    "print(train_df6.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 5400\n",
      "Length of X_train_onlyGhost: 5400\n",
      "Length of Y_train: 5400\n",
      "Length of MAE: 5400\n",
      "Length of FINAL_QP: 5400\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_train = scaler.fit_transform(train_df)\n",
    "X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "MAE = train_df5.values\n",
    "FINAL_QP = train_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_train = train_df2['LABEL'].astype(int)\n",
    "\n",
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_train_onlyGhost: {len(X_train_onlyGhost)}')\n",
    "print(f'Length of Y_train: {len(Y_train)}')\n",
    "print(f'Length of MAE: {len(MAE)}')\n",
    "print(f'Length of FINAL_QP: {len(FINAL_QP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　LargeQP1\n",
    "\n",
    "# データフレームを初期化\n",
    "test_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "test_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "test_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "test_df2 = pd.DataFrame(columns=label_columns)\n",
    "test_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "test_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "test_df5 = pd.DataFrame(columns=mae_columns)\n",
    "test_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in test_csv_largeQP1_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    test_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "    \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    # lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "    \n",
    "#     lu_values_10_1 = [df1.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "#     lu_values_10_2 = [df2.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "    \n",
    "#     lu_values_26_1 = [df1.loc[i, \"luminance_counts\"] for i in [25,26,27]] \n",
    "#     lu_values_26_2 = [df2.loc[i, \"luminance_counts\"] for i in [25,26,27]]\n",
    "\n",
    "#     average_10_1 = np.mean(lu_values_10_1)\n",
    "#     average_10_2 = np.mean(lu_values_10_2)\n",
    "#     average_26_1 = np.mean(lu_values_26_1)\n",
    "#     average_26_2 = np.mean(lu_values_26_2)\n",
    "    \n",
    "    \n",
    "#     lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_1)] + [int(average_26_1)] + [df2.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_2)] + [int(average_26_2)]\n",
    "    \n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "\n",
    "    test_df1_1 = pd.concat([test_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    test_df1_2= pd.concat([test_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    test_df1_3 = pd.concat([test_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "    # label_columnsの値を取得\n",
    "    test_df2 = pd.concat([test_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    test_final_QP = extract_finalQP(test_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_positive = calculate_mae(test_pkl_list[0])\n",
    "    _, mae_d2_positive = calculate_mae(test_pkl_list[1])\n",
    "\n",
    "\n",
    "    # mae1_columnsの値を取得\n",
    "    test_df3 = pd.concat([test_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    test_df4 = pd.concat([test_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "    \n",
    "    # mae_columnsの値を取得\n",
    "    test_df5 = pd.concat([test_df5, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    test_df6 = pd.concat([test_df6, pd.DataFrame({\"FINAL_QP\": [test_final_QP]})], ignore_index=True)\n",
    "\n",
    "\n",
    "# インデックスをリセット\n",
    "test_df1_1.reset_index(drop=True, inplace=True)\n",
    "test_df1_2.reset_index(drop=True, inplace=True)\n",
    "test_df1_3.reset_index(drop=True, inplace=True)\n",
    "test_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "test_largeQP1 = pd.concat([test_df1_1, test_df1_2, test_df1_3, test_df3, test_df4], axis=1)\n",
    "test_largeQP1_onlyGhost = pd.concat([test_df3, test_df4], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを初期化\n",
    "test_sameQP_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "test_sameQP_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "test_sameQP_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "test_sameQP_df2 = pd.DataFrame(columns=label_columns)\n",
    "test_sameQP_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "test_sameQP_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "test_sameQP_df5 = pd.DataFrame(columns=mae_columns)\n",
    "test_sameQP_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in test_csv_sameQP_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    test_csv_sameQP_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "        \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    # lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "    \n",
    "    \n",
    "#     lu_values_10_1 = [df1.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "#     lu_values_10_2 = [df2.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "    \n",
    "#     lu_values_26_1 = [df1.loc[i, \"luminance_counts\"] for i in [25,26,27]] \n",
    "#     lu_values_26_2 = [df2.loc[i, \"luminance_counts\"] for i in [25,26,27]]\n",
    "\n",
    "#     average_10_1 = np.mean(lu_values_10_1)\n",
    "#     average_10_2 = np.mean(lu_values_10_2)\n",
    "#     average_26_1 = np.mean(lu_values_26_1)\n",
    "#     average_26_2 = np.mean(lu_values_26_2)\n",
    "    \n",
    "    \n",
    "#     lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_1)] + [int(average_26_1)] + [df2.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_2)] + [int(average_26_2)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "\n",
    "    test_sameQP_df1_1 = pd.concat([test_sameQP_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    test_sameQP_df1_2= pd.concat([test_sameQP_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    test_sameQP_df1_3 = pd.concat([test_sameQP_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    # label_columnsの値を取得\n",
    "    test_sameQP_df2 = pd.concat([test_sameQP_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    test_sameQP_final_QP = extract_finalQP(test_csv_sameQP_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得\n",
    "    mae_d1, mae_d1_positive = calculate_mae(test_csv_sameQP_pkl_list[0])\n",
    "    _, mae_d2_positive = calculate_mae(test_csv_sameQP_pkl_list[1])\n",
    "\n",
    "    # mae1_columnsの値を取得\n",
    "    test_sameQP_df3 = pd.concat([test_sameQP_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    test_sameQP_df4 = pd.concat([test_sameQP_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    test_sameQP_df5 = pd.concat([test_sameQP_df5, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    test_sameQP_df6 = pd.concat([test_sameQP_df6, pd.DataFrame({\"FINAL_QP\": [test_sameQP_final_QP]})], ignore_index=True)\n",
    "\n",
    "\n",
    "# インデックスをリセット\n",
    "test_sameQP_df1_1.reset_index(drop=True, inplace=True)\n",
    "test_sameQP_df1_2.reset_index(drop=True, inplace=True)\n",
    "test_sameQP_df1_3.reset_index(drop=True, inplace=True)\n",
    "test_sameQP_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "test_sameQP = pd.concat([test_sameQP_df1_1, test_sameQP_df1_2, test_sameQP_df1_3, test_sameQP_df3, test_sameQP_df4], axis=1)\n",
    "test_sameQP_onlyGhost = pd.concat([test_sameQP_df3, test_sameQP_df4], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを初期化\n",
    "\n",
    "test_largeQP2_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "test_largeQP2_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "test_largeQP2_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "test_largeQP2_df2 = pd.DataFrame(columns=label_columns)\n",
    "test_largeQP2_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "test_largeQP2_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "test_largeQP2_df5 = pd.DataFrame(columns=mae_columns)\n",
    "test_largeQP2_df6 = pd.DataFrame(columns=final_qp_columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path1, path2, path3, path4 in test_csv_largeQP2_list:\n",
    "    label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "    test_csv_largeQP2_pkl_list = [path2, path4]\n",
    "    df1 = pd.read_csv(path1)\n",
    "    df2 = pd.read_csv(path3)\n",
    "        \n",
    "    pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "    # lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "    lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "    \n",
    "#     lu_values_10_1 = [df1.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "#     lu_values_10_2 = [df2.loc[i, \"luminance_counts\"] for i in [9,10,11]]\n",
    "    \n",
    "#     lu_values_26_1 = [df1.loc[i, \"luminance_counts\"] for i in [25,26,27]] \n",
    "#     lu_values_26_2 = [df2.loc[i, \"luminance_counts\"] for i in [25,26,27]]\n",
    "\n",
    "#     average_10_1 = np.mean(lu_values_10_1)\n",
    "#     average_10_2 = np.mean(lu_values_10_2)\n",
    "#     average_26_1 = np.mean(lu_values_26_1)\n",
    "#     average_26_2 = np.mean(lu_values_26_2)\n",
    "    \n",
    "    \n",
    "#     lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_1)] + [int(average_26_1)] + [df2.loc[i, \"luminance_counts\"] for i in [0,1]] + [int(average_10_2)] + [int(average_26_2)]\n",
    "    \n",
    "    \n",
    "    ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "\n",
    "    test_largeQP2_df1_1 = pd.concat([test_largeQP2_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "    test_largeQP2_df1_2= pd.concat([test_largeQP2_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "    test_largeQP2_df1_3 = pd.concat([test_largeQP2_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "\n",
    "    # label_columnsの値を取得\n",
    "    test_largeQP2_df2 = pd.concat([test_largeQP2_df2, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "    test_largeQP2_final_QP = extract_finalQP(test_csv_largeQP2_pkl_list[0])\n",
    "\n",
    "    # MAEの値を取得    \n",
    "    mae_d1, mae_d1_positive = calculate_mae(test_csv_largeQP2_pkl_list[0])\n",
    "    _, mae_d2_positive = calculate_mae(test_csv_largeQP2_pkl_list[1])\n",
    "    \n",
    "    # mae1_columnsの値を取得\n",
    "    test_largeQP2_df3 = pd.concat([test_largeQP2_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae2_columnsの値を取得\n",
    "    test_largeQP2_df4 = pd.concat([test_largeQP2_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2_positive[i]] for i in range(52)})], ignore_index=True)\n",
    "\n",
    "    # mae_columnsの値を取得\n",
    "    test_largeQP2_df5 = pd.concat([test_largeQP2_df5, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "\n",
    "    # final_qp_columnsの値を取得\n",
    "    test_largeQP2_df6 = pd.concat([test_largeQP2_df6, pd.DataFrame({\"FINAL_QP\": [test_largeQP2_final_QP]})], ignore_index=True)\n",
    "\n",
    "\n",
    "# インデックスをリセット\n",
    "test_largeQP2_df1_1.reset_index(drop=True, inplace=True)\n",
    "test_largeQP2_df1_2.reset_index(drop=True, inplace=True)\n",
    "test_largeQP2_df1_3.reset_index(drop=True, inplace=True)\n",
    "test_largeQP2_df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# データフレームを結合\n",
    "test_largeQP2 = pd.concat([test_largeQP2_df1_1, test_largeQP2_df1_2, test_largeQP2_df1_3, test_largeQP2_df3, test_largeQP2_df4], axis=1)\n",
    "test_largeQP2_onlyGhost = pd.concat([test_largeQP2_df3, test_largeQP2_df4], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_test_largeQP1: 600\n",
      "Length of X_test_largeQP1_onlyGhost: 600\n",
      "Length of Y_test_largeQP1: 600\n",
      "Length of MAE_largeQP1: 600\n",
      "Length of FINAL_QP_largeQP1: 600\n",
      "-------------------------------------------\n",
      "Length of X_test_sameQP: 600\n",
      "Length of X_test_sameQP_onlyGhost: 600\n",
      "Length of Y_test_sameQP: 600\n",
      "Length of MAE_sameQP: 600\n",
      "Length of FINAL_QP_sameQP: 600\n",
      "-------------------------------------------\n",
      "Length of X_test_largeQP2: 600\n",
      "Length of X_test_largeQP2_onlyGhost: 600\n",
      "Length of Y_test_largeQP2: 600\n",
      "Length of MAE_largeQP2: 600\n",
      "Length of FINAL_QP_largeQP2: 600\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# スケーラーを使って結合したデータをスケーリング\n",
    "X_test_largeQP1 = scaler.fit_transform(test_largeQP1)\n",
    "X_test_largeQP1_onlyGhost = scaler.fit_transform(test_largeQP1_onlyGhost)\n",
    "\n",
    "X_test_sameQP = scaler.fit_transform(test_sameQP)\n",
    "X_test_sameQP_onlyGhost = scaler.fit_transform(test_sameQP_onlyGhost)\n",
    "\n",
    "X_test_largeQP2 = scaler.fit_transform(test_largeQP2)\n",
    "X_test_largeQP2_onlyGhost = scaler.fit_transform(test_largeQP2_onlyGhost)\n",
    "\n",
    "# pandasをndarrayに変換\n",
    "MAE_largeQP1 = test_df5.values\n",
    "FINAL_QP_largeQP1 = test_df6.values\n",
    "\n",
    "MAE_sameQP = test_sameQP_df5.values\n",
    "FINAL_QP_sameQP = test_sameQP_df6.values\n",
    "\n",
    "MAE_largeQP2 = test_largeQP2_df5.values\n",
    "FINAL_QP_largeQP2 = test_largeQP2_df6.values\n",
    "\n",
    "# ラベルの準備\n",
    "Y_test_largeQP1 = test_df2['LABEL'].astype(int)\n",
    "Y_test_sameQP = test_sameQP_df2['LABEL'].astype(int)\n",
    "Y_test_largeQP2 = test_largeQP2_df2['LABEL'].astype(int)\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    ('largeQP1', X_test_largeQP1, X_test_largeQP1_onlyGhost, MAE_largeQP1, FINAL_QP_largeQP1, Y_test_largeQP1),\n",
    "    ('sameQP', X_test_sameQP, X_test_sameQP_onlyGhost , MAE_sameQP, FINAL_QP_sameQP, Y_test_sameQP),\n",
    "    ('largeQP2', X_test_largeQP2, X_test_largeQP2_onlyGhost, MAE_largeQP2, FINAL_QP_largeQP2, Y_test_largeQP2)\n",
    "]\n",
    "\n",
    "for name, X, X_onlyGhost, MAE, FINAL_QP, Y in datasets:    \n",
    "    # 出力\n",
    "    print(f'Length of X_test_{name}: {len(X)}')\n",
    "    print(f'Length of X_test_{name}_onlyGhost: {len(X_onlyGhost)}')\n",
    "    print(f'Length of Y_test_{name}: {len(Y)}')\n",
    "    print(f'Length of MAE_{name}: {len(MAE)}')\n",
    "    print(f'Length of FINAL_QP_{name}: {len(FINAL_QP)}')\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8872    0.7600    0.8187       300\n",
      "           1     0.7901    0.9033    0.8429       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8386    0.8317    0.8308       600\n",
      "weighted avg     0.8386    0.8317    0.8308       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9661    0.7600    0.8507       300\n",
      "           1     0.8022    0.9733    0.8795       300\n",
      "\n",
      "    accuracy                         0.8667       600\n",
      "   macro avg     0.8841    0.8667    0.8651       600\n",
      "weighted avg     0.8841    0.8667    0.8651       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5795    0.7900    0.6685       300\n",
      "           1     0.6702    0.4267    0.5214       300\n",
      "\n",
      "    accuracy                         0.6083       600\n",
      "   macro avg     0.6248    0.6083    0.5950       600\n",
      "weighted avg     0.6248    0.6083    0.5950       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8075    0.5033    0.6201       300\n",
      "           1     0.6392    0.8800    0.7405       300\n",
      "\n",
      "    accuracy                         0.6917       600\n",
      "   macro avg     0.7234    0.6917    0.6803       600\n",
      "weighted avg     0.7234    0.6917    0.6803       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5775    0.6833    0.6260       300\n",
      "           1     0.6122    0.5000    0.5505       300\n",
      "\n",
      "    accuracy                         0.5917       600\n",
      "   macro avg     0.5949    0.5917    0.5882       600\n",
      "weighted avg     0.5949    0.5917    0.5882       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.4200    0.5551       300\n",
      "           1     0.6099    0.9067    0.7292       300\n",
      "\n",
      "    accuracy                         0.6633       600\n",
      "   macro avg     0.7140    0.6633    0.6421       600\n",
      "weighted avg     0.7140    0.6633    0.6421       600\n",
      "\n",
      "<Fold-2>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9387    0.6633    0.7773       300\n",
      "           1     0.7397    0.9567    0.8343       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8392    0.8100    0.8058       600\n",
      "weighted avg     0.8392    0.8100    0.8058       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9650    0.6433    0.7720       300\n",
      "           1     0.7325    0.9767    0.8371       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8488    0.8100    0.8046       600\n",
      "weighted avg     0.8488    0.8100    0.8046       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6279    0.7200    0.6708       300\n",
      "           1     0.6719    0.5733    0.6187       300\n",
      "\n",
      "    accuracy                         0.6467       600\n",
      "   macro avg     0.6499    0.6467    0.6448       600\n",
      "weighted avg     0.6499    0.6467    0.6448       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8098    0.4967    0.6157       300\n",
      "           1     0.6370    0.8833    0.7402       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.7234    0.6900    0.6780       600\n",
      "weighted avg     0.7234    0.6900    0.6780       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5890    0.5733    0.5811       300\n",
      "           1     0.5844    0.6000    0.5921       300\n",
      "\n",
      "    accuracy                         0.5867       600\n",
      "   macro avg     0.5867    0.5867    0.5866       600\n",
      "weighted avg     0.5867    0.5867    0.5866       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8141    0.4233    0.5570       300\n",
      "           1     0.6104    0.9033    0.7285       300\n",
      "\n",
      "    accuracy                         0.6633       600\n",
      "   macro avg     0.7122    0.6633    0.6428       600\n",
      "weighted avg     0.7122    0.6633    0.6428       600\n",
      "\n",
      "<Fold-3>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9361    0.6833    0.7900       300\n",
      "           1     0.7507    0.9533    0.8399       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8434    0.8183    0.8150       600\n",
      "weighted avg     0.8434    0.8183    0.8150       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9726    0.7100    0.8208       300\n",
      "           1     0.7717    0.9800    0.8634       300\n",
      "\n",
      "    accuracy                         0.8450       600\n",
      "   macro avg     0.8721    0.8450    0.8421       600\n",
      "weighted avg     0.8721    0.8450    0.8421       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5983    0.7200    0.6536       300\n",
      "           1     0.6485    0.5167    0.5751       300\n",
      "\n",
      "    accuracy                         0.6183       600\n",
      "   macro avg     0.6234    0.6183    0.6143       600\n",
      "weighted avg     0.6234    0.6183    0.6143       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7989    0.5033    0.6176       300\n",
      "           1     0.6375    0.8733    0.7370       300\n",
      "\n",
      "    accuracy                         0.6883       600\n",
      "   macro avg     0.7182    0.6883    0.6773       600\n",
      "weighted avg     0.7182    0.6883    0.6773       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5798    0.6300    0.6038       300\n",
      "           1     0.5949    0.5433    0.5679       300\n",
      "\n",
      "    accuracy                         0.5867       600\n",
      "   macro avg     0.5873    0.5867    0.5859       600\n",
      "weighted avg     0.5873    0.5867    0.5859       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8483    0.4100    0.5528       300\n",
      "           1     0.6110    0.9267    0.7364       300\n",
      "\n",
      "    accuracy                         0.6683       600\n",
      "   macro avg     0.7296    0.6683    0.6446       600\n",
      "weighted avg     0.7296    0.6683    0.6446       600\n",
      "\n",
      "<Fold-4>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9298    0.7500    0.8303       300\n",
      "           1     0.7905    0.9433    0.8602       300\n",
      "\n",
      "    accuracy                         0.8467       600\n",
      "   macro avg     0.8601    0.8467    0.8452       600\n",
      "weighted avg     0.8601    0.8467    0.8452       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9555    0.7867    0.8629       300\n",
      "           1     0.8187    0.9633    0.8851       300\n",
      "\n",
      "    accuracy                         0.8750       600\n",
      "   macro avg     0.8871    0.8750    0.8740       600\n",
      "weighted avg     0.8871    0.8750    0.8740       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6121    0.7733    0.6834       300\n",
      "           1     0.6923    0.5100    0.5873       300\n",
      "\n",
      "    accuracy                         0.6417       600\n",
      "   macro avg     0.6522    0.6417    0.6353       600\n",
      "weighted avg     0.6522    0.6417    0.6353       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7638    0.5067    0.6092       300\n",
      "           1     0.6309    0.8433    0.7218       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6974    0.6750    0.6655       600\n",
      "weighted avg     0.6974    0.6750    0.6655       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5698    0.6667    0.6144       300\n",
      "           1     0.5984    0.4967    0.5428       300\n",
      "\n",
      "    accuracy                         0.5817       600\n",
      "   macro avg     0.5841    0.5817    0.5786       600\n",
      "weighted avg     0.5841    0.5817    0.5786       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8278    0.4167    0.5543       300\n",
      "           1     0.6102    0.9133    0.7316       300\n",
      "\n",
      "    accuracy                         0.6650       600\n",
      "   macro avg     0.7190    0.6650    0.6430       600\n",
      "weighted avg     0.7190    0.6650    0.6430       600\n",
      "\n",
      "<Fold-5>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9581    0.6867    0.8000       300\n",
      "           1     0.7558    0.9700    0.8496       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8570    0.8283    0.8248       600\n",
      "weighted avg     0.8570    0.8283    0.8248       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9569    0.7400    0.8346       300\n",
      "           1     0.7880    0.9667    0.8683       300\n",
      "\n",
      "    accuracy                         0.8533       600\n",
      "   macro avg     0.8725    0.8533    0.8514       600\n",
      "weighted avg     0.8725    0.8533    0.8514       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6254    0.7233    0.6708       300\n",
      "           1     0.6719    0.5667    0.6148       300\n",
      "\n",
      "    accuracy                         0.6450       600\n",
      "   macro avg     0.6486    0.6450    0.6428       600\n",
      "weighted avg     0.6486    0.6450    0.6428       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7610    0.5200    0.6178       300\n",
      "           1     0.6354    0.8367    0.7223       300\n",
      "\n",
      "    accuracy                         0.6783       600\n",
      "   macro avg     0.6982    0.6783    0.6701       600\n",
      "weighted avg     0.6982    0.6783    0.6701       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6127    0.5800    0.5959       300\n",
      "           1     0.6013    0.6333    0.6169       300\n",
      "\n",
      "    accuracy                         0.6067       600\n",
      "   macro avg     0.6070    0.6067    0.6064       600\n",
      "weighted avg     0.6070    0.6067    0.6064       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.4200    0.5551       300\n",
      "           1     0.6099    0.9067    0.7292       300\n",
      "\n",
      "    accuracy                         0.6633       600\n",
      "   macro avg     0.7140    0.6633    0.6421       600\n",
      "weighted avg     0.7140    0.6633    0.6421       600\n",
      "\n",
      "<Fold-6>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9406    0.6867    0.7938       300\n",
      "           1     0.7533    0.9567    0.8429       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8470    0.8217    0.8184       600\n",
      "weighted avg     0.8470    0.8217    0.8184       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9612    0.7433    0.8383       300\n",
      "           1     0.7908    0.9700    0.8713       300\n",
      "\n",
      "    accuracy                         0.8567       600\n",
      "   macro avg     0.8760    0.8567    0.8548       600\n",
      "weighted avg     0.8760    0.8567    0.8548       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6333    0.6967    0.6635       300\n",
      "           1     0.6630    0.5967    0.6281       300\n",
      "\n",
      "    accuracy                         0.6467       600\n",
      "   macro avg     0.6481    0.6467    0.6458       600\n",
      "weighted avg     0.6481    0.6467    0.6458       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8101    0.4833    0.6054       300\n",
      "           1     0.6318    0.8867    0.7379       300\n",
      "\n",
      "    accuracy                         0.6850       600\n",
      "   macro avg     0.7209    0.6850    0.6716       600\n",
      "weighted avg     0.7209    0.6850    0.6716       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5986    0.5767    0.5874       300\n",
      "           1     0.5916    0.6133    0.6023       300\n",
      "\n",
      "    accuracy                         0.5950       600\n",
      "   macro avg     0.5951    0.5950    0.5949       600\n",
      "weighted avg     0.5951    0.5950    0.5949       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8519    0.3833    0.5287       300\n",
      "           1     0.6022    0.9333    0.7320       300\n",
      "\n",
      "    accuracy                         0.6583       600\n",
      "   macro avg     0.7270    0.6583    0.6304       600\n",
      "weighted avg     0.7270    0.6583    0.6304       600\n",
      "\n",
      "<Fold-7>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9528    0.6733    0.7891       300\n",
      "           1     0.7474    0.9667    0.8430       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8501    0.8200    0.8160       600\n",
      "weighted avg     0.8501    0.8200    0.8160       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9571    0.6700    0.7882       300\n",
      "           1     0.7462    0.9700    0.8435       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8516    0.8200    0.8159       600\n",
      "weighted avg     0.8516    0.8200    0.8159       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6356    0.7267    0.6781       300\n",
      "           1     0.6809    0.5833    0.6284       300\n",
      "\n",
      "    accuracy                         0.6550       600\n",
      "   macro avg     0.6583    0.6550    0.6532       600\n",
      "weighted avg     0.6583    0.6550    0.6532       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8232    0.4967    0.6195       300\n",
      "           1     0.6396    0.8933    0.7455       300\n",
      "\n",
      "    accuracy                         0.6950       600\n",
      "   macro avg     0.7314    0.6950    0.6825       600\n",
      "weighted avg     0.7314    0.6950    0.6825       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5895    0.5600    0.5744       300\n",
      "           1     0.5810    0.6100    0.5951       300\n",
      "\n",
      "    accuracy                         0.5850       600\n",
      "   macro avg     0.5852    0.5850    0.5847       600\n",
      "weighted avg     0.5852    0.5850    0.5847       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8355    0.4233    0.5619       300\n",
      "           1     0.6138    0.9167    0.7353       300\n",
      "\n",
      "    accuracy                         0.6700       600\n",
      "   macro avg     0.7247    0.6700    0.6486       600\n",
      "weighted avg     0.7247    0.6700    0.6486       600\n",
      "\n",
      "<Fold-8>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8496    0.7533    0.7986       300\n",
      "           1     0.7784    0.8667    0.8202       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8140    0.8100    0.8094       600\n",
      "weighted avg     0.8140    0.8100    0.8094       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9494    0.7500    0.8380       300\n",
      "           1     0.7934    0.9600    0.8688       300\n",
      "\n",
      "    accuracy                         0.8550       600\n",
      "   macro avg     0.8714    0.8550    0.8534       600\n",
      "weighted avg     0.8714    0.8550    0.8534       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5649    0.7400    0.6407       300\n",
      "           1     0.6232    0.4300    0.5089       300\n",
      "\n",
      "    accuracy                         0.5850       600\n",
      "   macro avg     0.5940    0.5850    0.5748       600\n",
      "weighted avg     0.5940    0.5850    0.5748       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7740    0.5367    0.6339       300\n",
      "           1     0.6454    0.8433    0.7312       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.7097    0.6900    0.6825       600\n",
      "weighted avg     0.7097    0.6900    0.6825       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5867    0.6767    0.6285       300\n",
      "           1     0.6181    0.5233    0.5668       300\n",
      "\n",
      "    accuracy                         0.6000       600\n",
      "   macro avg     0.6024    0.6000    0.5976       600\n",
      "weighted avg     0.6024    0.6000    0.5976       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7988    0.4367    0.5647       300\n",
      "           1     0.6124    0.8900    0.7255       300\n",
      "\n",
      "    accuracy                         0.6633       600\n",
      "   macro avg     0.7056    0.6633    0.6451       600\n",
      "weighted avg     0.7056    0.6633    0.6451       600\n",
      "\n",
      "<Fold-9>\n",
      "\n",
      "4800 600\n",
      "0.88\n",
      "0.5166666666666667\n",
      "0.5816666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8510    0.7233    0.7820       300\n",
      "           1     0.7594    0.8733    0.8124       300\n",
      "\n",
      "    accuracy                         0.7983       600\n",
      "   macro avg     0.8052    0.7983    0.7972       600\n",
      "weighted avg     0.8052    0.7983    0.7972       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9600    0.7200    0.8229       300\n",
      "           1     0.7760    0.9700    0.8622       300\n",
      "\n",
      "    accuracy                         0.8450       600\n",
      "   macro avg     0.8680    0.8450    0.8425       600\n",
      "weighted avg     0.8680    0.8450    0.8425       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5740    0.7500    0.6503       300\n",
      "           1     0.6394    0.4433    0.5236       300\n",
      "\n",
      "    accuracy                         0.5967       600\n",
      "   macro avg     0.6067    0.5967    0.5870       600\n",
      "weighted avg     0.6067    0.5967    0.5870       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7938    0.5133    0.6235       300\n",
      "           1     0.6404    0.8667    0.7365       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.7171    0.6900    0.6800       600\n",
      "weighted avg     0.7171    0.6900    0.6800       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5856    0.6500    0.6161       300\n",
      "           1     0.6067    0.5400    0.5714       300\n",
      "\n",
      "    accuracy                         0.5950       600\n",
      "   macro avg     0.5962    0.5950    0.5938       600\n",
      "weighted avg     0.5962    0.5950    0.5938       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8609    0.4333    0.5765       300\n",
      "           1     0.6214    0.9300    0.7450       300\n",
      "\n",
      "    accuracy                         0.6817       600\n",
      "   macro avg     0.7412    0.6817    0.6607       600\n",
      "weighted avg     0.7412    0.6817    0.6607       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 1000, 2000, 3000]}\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "kfold = StratifiedKFold(n_splits=9, shuffle=True)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF','RBF_largeQP1','C_RBF_sameQP','RBF_sameQP','C_RBF_largeQP2','RBF_largeQP2',\n",
    "                                'C_LINEAR','LINEAR_largeQP1','C_LINEAR_sameQP','LINEAR_sameQP','C_LINEAR_largeQP2','LINEAR_largeQP2',\n",
    "                                \n",
    "                                'C_RBF_ghost','RBF_ghost_largeQP1','C_RBF_sameQP_ghost','RBF_ghost_sameQP','C_RBF_largeQP2_ghost','RBF_ghost_largeQP2',\n",
    "                                'C_LINEAR_ghost','LINEAR_ghost_largeQP1','C_LINEAR_sameQP_ghost','LINEAR_ghost_sameQP','C_LINEAR_largeQP2_ghost','LINEAR_ghost_largeQP2',\n",
    "                                \n",
    "                                'Threshold', 'largeQP1_old', 'Threshold_sameQP', 'sameQP_old', 'Threshold_largeQP2', 'largeQP2_old'])\n",
    "\n",
    "\n",
    "# 訓練データ\n",
    "O_X_train, O_X_train_onlyGhost = X_train, X_train_onlyGhost\n",
    "O_Y_train = Y_train\n",
    "\n",
    "# テストデータ\n",
    "O_X_test_largeQP1, O_X_test_largeQP1_onlyGhost = X_test_largeQP1, X_test_largeQP1_onlyGhost\n",
    "O_X_test_sameQP, O_X_test_sameQP_onlyGhost = X_test_sameQP, X_test_sameQP_onlyGhost\n",
    "O_X_test_largeQP2, O_X_test_largeQP2_onlyGhost = X_test_largeQP2, X_test_largeQP2_onlyGhost\n",
    "\n",
    "O_Y_test_largeQP1 = Y_test_largeQP1\n",
    "O_Y_test_sameQP = Y_test_sameQP\n",
    "O_Y_test_largeQP2 = Y_test_largeQP2\n",
    "\n",
    "# 閾値用テストデータ\n",
    "O_test_old, O_test_final_QP = MAE_largeQP1, FINAL_QP_largeQP1\n",
    "O_test_sameQP_old, O_test_sameQP_final_QP = MAE_sameQP, FINAL_QP_sameQP\n",
    "O_test_largeQP_old, O_test_largeQP_final_QP = MAE_largeQP2, FINAL_QP_largeQP2\n",
    "\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(O_X_train, O_Y_train)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print()\n",
    "    \n",
    "    print(len(train_ids), len(test_ids))\n",
    "    \n",
    "    results_old = []\n",
    "    \n",
    "    # 全体を訓練ラベルと検証ラベルに分割\n",
    "    X_train, X_val = O_X_train[train_ids], O_X_train[test_ids]\n",
    "    X_train_onlyGhost, X_val_onlyGhost = O_X_train_onlyGhost[train_ids], O_X_train_onlyGhost[test_ids]\n",
    "    \n",
    "    Y_train, Y_val = O_Y_train[train_ids], O_Y_train[test_ids]\n",
    "    \n",
    "    \n",
    "    # テストデータ・ラベルの処理（RBFとLINEAR）\n",
    "    X_test_largeQP1, X_test_sameQP, X_test_largeQP2 = O_X_test_largeQP1, O_X_test_sameQP, O_X_test_largeQP2\n",
    "    Y_test_largeQP1, Y_test_sameQP, Y_test_largeQP2 = O_Y_test_largeQP1, O_Y_test_sameQP, O_Y_test_largeQP2\n",
    "    \n",
    "    X_test_largeQP1_onlyGhost, X_test_sameQP_onlyGhost, X_test_largeQP2_onlyGhost = O_X_test_largeQP1_onlyGhost, O_X_test_sameQP_onlyGhost, O_X_test_largeQP2_onlyGhost\n",
    "    \n",
    "    \n",
    "    # テストデータ・ラベルの処理（閾値）\n",
    "    X_test_old, test_QP = O_test_old, O_test_final_QP\n",
    "    X_test_sameQP_old, test_sameQP = O_test_sameQP_old, O_test_sameQP_final_QP\n",
    "    X_test_largeQP_old, test_largeQP = O_test_largeQP_old, O_test_largeQP_final_QP\n",
    "    \n",
    "    # 訓練・検証データ（ラベル）を訓練データ（ラベル）と検証データ（ラベル）に分割\n",
    "    # X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=540, random_state=42)\n",
    "    # X_train_onlyGhost, X_val_onlyGhost, Y_train, Y_val = train_test_split(X_train_val_onlyGhost, Y_train_val, test_size=540, random_state=42)\n",
    "    \n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    sameQP_best_threshold = 0\n",
    "    sameQP_best_accuracy = 0\n",
    "    sameQP_best_predicted_labels = []\n",
    "    sameQP_best_ground_truth_labels = []\n",
    "    \n",
    "    largeQP_best_threshold = 0\n",
    "    largeQP_best_accuracy = 0\n",
    "    largeQP_best_predicted_labels = []\n",
    "    largeQP_best_ground_truth_labels = []\n",
    "    \n",
    "    \n",
    "#     for threshold in np.arange(0.00001,1.1,0.00001):\n",
    "#         test_old = [is_double_compressed(X_test_old[i], test_QP[i], threshold) for i in range(600)]\n",
    "        \n",
    "#         predicted_labels = [int(is_double) for is_double in test_old]\n",
    "#         ground_truth_labels = [label for label in Y_test_largeQP1]\n",
    "#         accuracy = sum(1 for true_label, pred_label in zip(ground_truth_labels, predicted_labels) if true_label == pred_label) / len(ground_truth_labels)\n",
    "        \n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_threshold = threshold\n",
    "#             best_predicted_labels = predicted_labels\n",
    "#             best_ground_truth_labels = ground_truth_labels\n",
    "            \n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.01, 0.01):\n",
    "        test_old = np.array([is_double_compressed(X_test_old[i], test_QP[i], threshold) for i in range(600)])\n",
    "        predicted_labels = test_old.astype(int)\n",
    "        ground_truth_labels = np.array(Y_test_largeQP1)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "            \n",
    "            \n",
    "#     for threshold in np.arange(0.0001,1.0001,0.0001):\n",
    "#         test_sameQP_old = [is_double_compressed(X_test_sameQP_old[i], test_sameQP[i], threshold) for i in range(600)]\n",
    "        \n",
    "#         same_predicted_labels = [int(is_double) for is_double in test_sameQP_old]\n",
    "#         same_ground_truth_labels = [label for label in Y_test_sameQP]\n",
    "#         same_accuracy = sum(1 for true_label, pred_label in zip(same_ground_truth_labels, same_predicted_labels) if true_label == pred_label) / len(same_ground_truth_labels)\n",
    "        \n",
    "#         if same_accuracy > sameQP_best_accuracy:\n",
    "#             sameQP_best_accuracy = same_accuracy\n",
    "#             sameQP_best_threshold = threshold\n",
    "#             sameQP_best_predicted_labels = same_predicted_labels\n",
    "#             sameQP_best_ground_truth_labels = same_ground_truth_labels\n",
    "            \n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.01, 0.01):\n",
    "        test_sameQP_old = np.array([is_double_compressed(X_test_sameQP_old[i], test_sameQP[i], threshold) for i in range(600)])\n",
    "        same_predicted_labels = test_sameQP_old.astype(int)\n",
    "        same_ground_truth_labels = np.array(Y_test_sameQP)\n",
    "        same_accuracy = np.sum(same_ground_truth_labels == same_predicted_labels) / len(same_ground_truth_labels)\n",
    "    \n",
    "        if same_accuracy > sameQP_best_accuracy:\n",
    "            sameQP_best_accuracy = same_accuracy\n",
    "            sameQP_best_threshold = threshold\n",
    "            sameQP_best_predicted_labels = same_predicted_labels\n",
    "            sameQP_best_ground_truth_labels = same_ground_truth_labels\n",
    "            \n",
    "            \n",
    "#     for threshold in np.arange(0.0001,1.0001,0.0001):\n",
    "#         test_largeQP_old = [is_double_compressed(X_test_largeQP_old[i], test_largeQP[i], threshold) for i in range(600)]\n",
    "        \n",
    "#         large_predicted_labels = [int(is_double) for is_double in test_largeQP_old]\n",
    "#         large_ground_truth_labels = [label for label in Y_test_largeQP2]\n",
    "#         large_accuracy = sum(1 for true_label, pred_label in zip(large_ground_truth_labels, large_predicted_labels) if true_label == pred_label) / len(large_ground_truth_labels)\n",
    "        \n",
    "#         if large_accuracy > largeQP_best_accuracy:\n",
    "#             largeQP_best_accuracy = large_accuracy\n",
    "#             largeQP_best_threshold = threshold\n",
    "#             largeQP_best_predicted_labels = large_predicted_labels\n",
    "#             largeQP_best_ground_truth_labels = large_ground_truth_labels\n",
    "            \n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.01, 0.01):\n",
    "        test_largeQP_old = np.array([is_double_compressed(X_test_largeQP_old[i], test_largeQP[i], threshold) for i in range(600)])\n",
    "        large_predicted_labels = test_largeQP_old.astype(int)\n",
    "        large_ground_truth_labels = np.array(Y_test_largeQP2)\n",
    "        large_accuracy = np.sum(large_ground_truth_labels == large_predicted_labels) / len(large_ground_truth_labels)\n",
    "    \n",
    "        if large_accuracy > largeQP_best_accuracy:\n",
    "            largeQP_best_accuracy = large_accuracy\n",
    "            largeQP_best_threshold = threshold\n",
    "            largeQP_best_predicted_labels = large_predicted_labels\n",
    "            largeQP_best_ground_truth_labels = large_ground_truth_labels\n",
    "    \n",
    "    \n",
    "    print(best_accuracy)\n",
    "    print(sameQP_best_accuracy)\n",
    "    print(largeQP_best_accuracy)\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF, best_gamma_RBF = 0, None, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR, best_gamma_LINEAR = 0, None, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "    \n",
    "    for C_value in C_values['C']:\n",
    "        \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_onlyGhost, Y_train)\n",
    "\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_onlyGhost))\n",
    "\n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_onlyGhost))\n",
    "\n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "\n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "            \n",
    "    \n",
    "    # テストデータで評価\n",
    "    predictions_RBF = best_svm_model_RBF.predict(X_test_largeQP1)\n",
    "    # predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    accuracy_RBF = accuracy_score(Y_test_largeQP1, predictions_RBF)\n",
    "    report_RBF = classification_report(Y_test_largeQP1, predictions_RBF, digits=4, zero_division=1)\n",
    "    print(f'report_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    predictions_LINEAR = best_svm_model_LINEAR.predict(X_test_largeQP1)\n",
    "    # predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    accuracy_LINEAR = accuracy_score(Y_test_largeQP1, predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(Y_test_largeQP1, predictions_LINEAR, digits=4, zero_division=1)\n",
    "    print(f'report_LINEAR:\\n{report_LINEAR}')\n",
    "    \n",
    "    same_predictions_RBF = best_svm_model_RBF.predict(X_test_sameQP)\n",
    "    # same_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_sameQP)\n",
    "    same_accuracy_RBF = accuracy_score(Y_test_sameQP, same_predictions_RBF)\n",
    "    same_report_RBF = classification_report(Y_test_sameQP, same_predictions_RBF, digits=4, zero_division=1)\n",
    "    print(f'same_report_RBF:\\n{same_report_RBF}')\n",
    "    \n",
    "    same_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test_sameQP)\n",
    "    # same_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_sameQP)\n",
    "    same_accuracy_LINEAR = accuracy_score(Y_test_sameQP, same_predictions_LINEAR)\n",
    "    same_report_LINEAR = classification_report(Y_test_sameQP, same_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    print(f'same_report_LINEAR:\\n{same_report_LINEAR}')\n",
    "    \n",
    "    large_predictions_RBF = best_svm_model_RBF.predict(X_test_largeQP2)\n",
    "    # large_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_largeQP)\n",
    "    large_accuracy_RBF = accuracy_score(Y_test_largeQP2, large_predictions_RBF)\n",
    "    large_report_RBF = classification_report(Y_test_largeQP2, large_predictions_RBF, digits=4, zero_division=1)\n",
    "    print(f'large_report_RBF:\\n{large_report_RBF}')\n",
    "    \n",
    "    large_predictions_LINEAR = best_svm_model_LINEAR.predict(X_test_largeQP2)\n",
    "    # large_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_largeQP)\n",
    "    large_accuracy_LINEAR = accuracy_score(Y_test_largeQP2, large_predictions_LINEAR)\n",
    "    large_report_LINEAR = classification_report(Y_test_largeQP2, large_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    print(f'large_report_LINEAR:\\n{large_report_LINEAR}')\n",
    "    \n",
    "    \n",
    "    # テストデータで評価\n",
    "    predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_largeQP1_onlyGhost)\n",
    "    # predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_onlyGhost)\n",
    "    accuracy_onlyGhost_RBF = accuracy_score(Y_test_largeQP1, predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(Y_test_largeQP1, predictions_onlyGhost_RBF)\n",
    "    # print(f'report_onlyGhost_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    same_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_sameQP_onlyGhost)\n",
    "    # same_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_sameQP_onlyGhost)\n",
    "    same_accuracy_onlyGhost_RBF = accuracy_score(Y_test_sameQP, same_predictions_onlyGhost_RBF)\n",
    "    same_report_onlyGhost_RBF = classification_report(Y_test_sameQP, same_predictions_onlyGhost_RBF)\n",
    "    # print(f'same_report_onlyGhost_RBF:\\n{same_report_onlyGhost_RBF}')\n",
    "    \n",
    "    large_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(X_test_largeQP2_onlyGhost)\n",
    "    # large_predictions_prob_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.decision_function(X_test_largeQP_onlyGhost)\n",
    "    large_accuracy_onlyGhost_RBF = accuracy_score(Y_test_largeQP2, large_predictions_onlyGhost_RBF)\n",
    "    large_report_onlyGhost_RBF = classification_report(Y_test_largeQP2, large_predictions_onlyGhost_RBF)\n",
    "    # print(f'large_report_onlyGhost_RBF:\\n{large_report_onlyGhost_RBF}')\n",
    "    \n",
    "    predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_largeQP1_onlyGhost)\n",
    "    # predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_onlyGhost)\n",
    "    accuracy_onlyGhost_LINEAR = accuracy_score(Y_test_largeQP1, predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(Y_test_largeQP1, predictions_onlyGhost_LINEAR)\n",
    "    # print(f'report_onlyGhost_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    same_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_sameQP_onlyGhost)\n",
    "    # same_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_sameQP_onlyGhost)\n",
    "    same_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test_sameQP, same_predictions_onlyGhost_LINEAR)\n",
    "    same_report_onlyGhost_LINEAR = classification_report(Y_test_sameQP, same_predictions_onlyGhost_LINEAR)\n",
    "    # print(f'same_report_onlyGhost_LINEAR:\\n{same_report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    large_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(X_test_largeQP2_onlyGhost)\n",
    "    # large_predictions_prob_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.decision_function(X_test_largeQP2_onlyGhost)\n",
    "    large_accuracy_onlyGhost_LINEAR = accuracy_score(Y_test_largeQP2, large_predictions_onlyGhost_LINEAR)\n",
    "    large_report_onlyGhost_LINEAR = classification_report(Y_test_largeQP2, large_predictions_onlyGhost_LINEAR)\n",
    "    # print(f'large_report_onlyGhost_LINEAR:\\n{large_report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    # print(f'test_old:\\n{test_old}')\n",
    "    \n",
    "    test_sameQP_old = classification_report(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    # print(f'test_sameQP_old:\\n{test_sameQP_old}')\n",
    "    \n",
    "    test_largeQP_old = classification_report(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    # print(f'test_largeQP_old:\\n{test_largeQP_old}')\n",
    "    \n",
    "        \n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row ={'C_RBF': best_c_value_RBF,'RBF_largeQP1':accuracy_RBF,\n",
    "                  'C_RBF_sameQP': best_c_value_RBF,'RBF_sameQP':same_accuracy_RBF,\n",
    "                  'C_RBF_largeQP2': best_c_value_RBF,'RBF_largeQP2':large_accuracy_RBF,\n",
    "\n",
    "                  'C_LINEAR': best_c_value_LINEAR,'LINEAR_largeQP1':accuracy_LINEAR,\n",
    "                  'C_LINEAR_sameQP': best_c_value_LINEAR,'LINEAR_sameQP':same_accuracy_LINEAR,\n",
    "                  'C_LINEAR_largeQP2': best_c_value_LINEAR,'LINEAR_largeQP2':large_accuracy_LINEAR,\n",
    "                 \n",
    "                  'C_RBF_ghost': best_c_value_onlyGhost_RBF,'RBF_ghost_largeQP1':accuracy_onlyGhost_RBF,\n",
    "                  'C_RBF_sameQP_ghost': best_c_value_onlyGhost_RBF,'RBF_ghost_sameQP':same_accuracy_onlyGhost_RBF,\n",
    "                  'C_RBF_largeQP_ghost': best_c_value_onlyGhost_RBF,'RBF_ghost_largeQP2':large_accuracy_onlyGhost_RBF,\n",
    "                  \n",
    "                  'C_LINEAR_ghost': best_c_value_onlyGhost_LINEAR,'LINEAR_ghost_largeQP1':accuracy_onlyGhost_LINEAR,\n",
    "                  'C_LINEAR_sameQP_ghost': best_c_value_onlyGhost_LINEAR,'LINEAR_ghost_sameQP':same_accuracy_onlyGhost_LINEAR,\n",
    "                  'C_LINEAR_largeQP_ghost': best_c_value_onlyGhost_LINEAR,'LINEAR_ghost_largeQP2':large_accuracy_onlyGhost_LINEAR,\n",
    "\n",
    "                  'Threshold':best_threshold, 'largeQP1_old':best_accuracy, \n",
    "                  'Threshold_sameQP':sameQP_best_threshold, 'sameQP_old':sameQP_best_accuracy, \n",
    "                  'Threshold_largeQP2':largeQP_best_threshold, 'largeQP2_old':largeQP_best_accuracy}\n",
    "    \n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n",
    "# 結果を表示\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Score RBF_largeQP1 with RBF: 0.8206\n",
      "Standard Deviation of Test Score RBF_largeQP1 with RBF: 0.0141\n",
      "Maximum Test Score RBF_largeQP1 with RBF: 0.8467\n",
      "Minimum Test Score RBF_largeQP1 with RBF: 0.7983\n",
      "------------------------------------------------------\n",
      "Average Test Score LINEAR_largeQP1 with LINEAR: 0.8474\n",
      "Standard Deviation of Test Score LINEAR_largeQP1 with LINEAR: 0.0208\n",
      "Maximum Test Score LINEAR_largeQP1 with LINEAR: 0.875\n",
      "Minimum Test Score LINEAR_largeQP1 with LINEAR: 0.81\n",
      "------------------------------------------------------\n",
      "Average Test Score RBF_ghost_largeQP1 with RBF: 0.7289\n",
      "Standard Deviation of Test Score RBF_ghost_largeQP1 with RBF: 0.0319\n",
      "Maximum Test Score RBF_ghost_largeQP1 with RBF: 0.77\n",
      "Minimum Test Score RBF_ghost_largeQP1 with RBF: 0.68\n",
      "------------------------------------------------------\n",
      "Average Test Score LINEAR_ghost_largeQP1 with LINEAR: 0.8148\n",
      "Standard Deviation of Test Score LINEAR_ghost_largeQP1 with LINEAR: 0.013\n",
      "Maximum Test Score LINEAR_ghost_largeQP1 with LINEAR: 0.8317\n",
      "Minimum Test Score LINEAR_ghost_largeQP1 with LINEAR: 0.7967\n",
      "------------------------------------------------------\n",
      "Average Test Score largeQP1_old with old model: 0.88\n",
      "Standard Deviation of Test Score largeQP1_old with old model: 0.0\n",
      "Maximum Test Score largeQP1_old with old model: 0.88\n",
      "Minimum Test Score largeQP1_old with old model: 0.88\n",
      "------------------------------------------------------\n",
      "\n",
      "Average Test Score RBF_sameQP with RBF: 0.627\n",
      "Standard Deviation of Test Score RBF_sameQP with RBF: 0.0255\n",
      "Maximum Test Score RBF_sameQP with RBF: 0.655\n",
      "Minimum Test Score RBF_sameQP with RBF: 0.585\n",
      "------------------------------------------------------\n",
      "Average Test Score LINEAR_sameQP with LINEAR: 0.687\n",
      "Standard Deviation of Test Score LINEAR_sameQP with LINEAR: 0.0065\n",
      "Maximum Test Score LINEAR_sameQP with LINEAR: 0.695\n",
      "Minimum Test Score LINEAR_sameQP with LINEAR: 0.675\n",
      "------------------------------------------------------\n",
      "Average Test Score RBF_ghost_sameQP with RBF: 0.6919\n",
      "Standard Deviation of Test Score RBF_ghost_sameQP with RBF: 0.0183\n",
      "Maximum Test Score RBF_ghost_sameQP with RBF: 0.72\n",
      "Minimum Test Score RBF_ghost_sameQP with RBF: 0.6683\n",
      "------------------------------------------------------\n",
      "Average Test Score LINEAR_ghost_sameQP with LINEAR: 0.6787\n",
      "Standard Deviation of Test Score LINEAR_ghost_sameQP with LINEAR: 0.0041\n",
      "Maximum Test Score LINEAR_ghost_sameQP with LINEAR: 0.685\n",
      "Minimum Test Score LINEAR_ghost_sameQP with LINEAR: 0.6717\n",
      "------------------------------------------------------\n",
      "Average Test Score sameQP_old with old model: 0.5167\n",
      "Standard Deviation of Test Score sameQP_old with old model: 0.0\n",
      "Maximum Test Score sameQP_old with old model: 0.5167\n",
      "Minimum Test Score sameQP_old with old model: 0.5167\n",
      "------------------------------------------------------\n",
      "\n",
      "Average Test Score RBF_largeQP2 with RBF: 0.592\n",
      "Standard Deviation of Test Score RBF_largeQP2 with RBF: 0.008\n",
      "Maximum Test Score RBF_largeQP2 with RBF: 0.6067\n",
      "Minimum Test Score RBF_largeQP2 with RBF: 0.5817\n",
      "------------------------------------------------------\n",
      "Average Test Score LINEAR_largeQP2 with LINEAR: 0.6663\n",
      "Standard Deviation of Test Score LINEAR_largeQP2 with LINEAR: 0.0067\n",
      "Maximum Test Score LINEAR_largeQP2 with LINEAR: 0.6817\n",
      "Minimum Test Score LINEAR_largeQP2 with LINEAR: 0.6583\n",
      "------------------------------------------------------\n",
      "Average Test Score RBF_ghost_largeQP2 with RBF: 0.522\n",
      "Standard Deviation of Test Score RBF_ghost_largeQP2 with RBF: 0.0134\n",
      "Maximum Test Score RBF_ghost_largeQP2 with RBF: 0.54\n",
      "Minimum Test Score RBF_ghost_largeQP2 with RBF: 0.4933\n",
      "------------------------------------------------------\n",
      "Average Test Score LINEAR_ghost_largeQP2 with LINEAR: 0.5563\n",
      "Standard Deviation of Test Score LINEAR_ghost_largeQP2 with LINEAR: 0.0105\n",
      "Maximum Test Score LINEAR_ghost_largeQP2 with LINEAR: 0.5667\n",
      "Minimum Test Score LINEAR_ghost_largeQP2 with LINEAR: 0.5383\n",
      "------------------------------------------------------\n",
      "Average Test Score largeQP2_old with old model: 0.5817\n",
      "Standard Deviation of Test Score largeQP2_old with old model: 0.0\n",
      "Maximum Test Score largeQP2_old with old model: 0.5817\n",
      "Minimum Test Score largeQP2_old with old model: 0.5817\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_stats(column_name, label):\n",
    "    data = (results[column_name])\n",
    "    average = round(results[column_name].mean(), 4)\n",
    "    std_dev = round(results[column_name].std(), 4)\n",
    "    max_value = round(results[column_name].max(), 4)\n",
    "    min_value = round(results[column_name].min(), 4)\n",
    "\n",
    "    # print(f'Test Score {column_name} {label}: {data}')\n",
    "    print(f'Average Test Score {column_name} {label}: {average}')\n",
    "    print(f'Standard Deviation of Test Score {column_name} {label}: {std_dev}')\n",
    "    print(f'Maximum Test Score {column_name} {label}: {max_value}')\n",
    "    print(f'Minimum Test Score {column_name} {label}: {min_value}')\n",
    "    print('------------------------------------------------------')\n",
    "\n",
    "# 'Test_Score'列に関して統計情報を表示\n",
    "# print_stats('C_RBF', 'with RBF')\n",
    "print_stats('RBF_largeQP1', 'with RBF')\n",
    "# print_stats('C_LINEAR', 'with RBF')\n",
    "print_stats('LINEAR_largeQP1', 'with LINEAR')\n",
    "print_stats('RBF_ghost_largeQP1', 'with RBF')\n",
    "print_stats('LINEAR_ghost_largeQP1', 'with LINEAR')\n",
    "print_stats('largeQP1_old', 'with old model')\n",
    "print()\n",
    "# print_stats('C_RBF_sameQP', 'with RBF')\n",
    "print_stats('RBF_sameQP', 'with RBF')\n",
    "# print_stats('C_LINEAR_sameQP', 'with RBF')\n",
    "print_stats('LINEAR_sameQP', 'with LINEAR')\n",
    "print_stats('RBF_ghost_sameQP', 'with RBF')\n",
    "print_stats('LINEAR_ghost_sameQP', 'with LINEAR')\n",
    "print_stats('sameQP_old', 'with old model')\n",
    "print()\n",
    "# print_stats('C_RBF_largeQP2', 'with RBF')\n",
    "print_stats('RBF_largeQP2', 'with RBF')\n",
    "# print_stats('C_LINEAR_largeQP2', 'with RBF')\n",
    "print_stats('LINEAR_largeQP2', 'with LINEAR')\n",
    "print_stats('RBF_ghost_largeQP2', 'with RBF')\n",
    "print_stats('LINEAR_ghost_largeQP2', 'with LINEAR')\n",
    "print_stats('largeQP2_old', 'with old model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    60\n",
      "1    20\n",
      "2    40\n",
      "3    20\n",
      "4    10\n",
      "5    10\n",
      "6    20\n",
      "7    70\n",
      "8    80\n",
      "Name: C_RBF, dtype: object\n",
      "0    60\n",
      "1    20\n",
      "2    40\n",
      "3    20\n",
      "4    10\n",
      "5    10\n",
      "6    20\n",
      "7    70\n",
      "8    80\n",
      "Name: C_RBF_sameQP, dtype: object\n",
      "0    60\n",
      "1    20\n",
      "2    40\n",
      "3    20\n",
      "4    10\n",
      "5    10\n",
      "6    20\n",
      "7    70\n",
      "8    80\n",
      "Name: C_RBF_largeQP2, dtype: object\n",
      "\n",
      "0     40\n",
      "1     20\n",
      "2     60\n",
      "3     80\n",
      "4    100\n",
      "5    200\n",
      "6     20\n",
      "7     80\n",
      "8     50\n",
      "Name: C_LINEAR, dtype: object\n",
      "0     40\n",
      "1     20\n",
      "2     60\n",
      "3     80\n",
      "4    100\n",
      "5    200\n",
      "6     20\n",
      "7     80\n",
      "8     50\n",
      "Name: C_LINEAR_sameQP, dtype: object\n",
      "0     40\n",
      "1     20\n",
      "2     60\n",
      "3     80\n",
      "4    100\n",
      "5    200\n",
      "6     20\n",
      "7     80\n",
      "8     50\n",
      "Name: C_LINEAR_largeQP2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF'])\n",
    "print(results['C_RBF_sameQP'])\n",
    "print(results['C_RBF_largeQP2'])\n",
    "print()\n",
    "print(results['C_LINEAR'])\n",
    "print(results['C_LINEAR_sameQP'])\n",
    "print(results['C_LINEAR_largeQP2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
