{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['99', '160', '293', '9', '215', '126', '42', '138', '243', '145', '171', '45', '268', '186', '21', '220', '63', '47', '296', '261', '127', '112', '252', '108', '230', '103', '82', '120', '28', '69'], ['211', '155', '248', '166', '144', '178', '180', '32', '58', '24', '219', '38', '151', '16', '125', '275', '119', '133', '26', '37', '36', '208', '189', '94', '92', '273', '173', '121', '4', '241'], ['182', '55', '84', '78', '250', '256', '10', '300', '80', '48', '265', '237', '281', '165', '232', '31', '40', '124', '188', '168', '199', '17', '148', '62', '132', '33', '70', '235', '295', '52'], ['284', '270', '22', '116', '194', '269', '209', '244', '271', '59', '288', '287', '246', '294', '88', '111', '90', '7', '1', '89', '123', '152', '202', '266', '185', '212', '131', '113', '147', '201'], ['71', '157', '224', '291', '254', '60', '282', '15', '162', '283', '200', '81', '110', '176', '169', '218', '79', '18', '222', '197', '242', '223', '204', '85', '34', '75', '107', '238', '12', '35'], ['11', '156', '44', '175', '134', '122', '106', '286', '142', '118', '105', '97', '277', '73', '109', '216', '49', '20', '102', '150', '153', '203', '164', '25', '53', '217', '67', '161', '136', '196'], ['149', '191', '195', '129', '93', '146', '61', '43', '77', '231', '183', '104', '114', '177', '262', '214', '50', '5', '227', '184', '172', '29', '56', '3', '249', '285', '143', '181', '264', '115'], ['27', '299', '193', '139', '226', '76', '276', '74', '255', '233', '279', '298', '130', '54', '251', '13', '91', '240', '234', '229', '66', '141', '239', '117', '260', '225', '154', '263', '247', '14'], ['68', '267', '41', '280', '174', '278', '39', '96', '101', '98', '207', '100', '205', '221', '259', '135', '140', '83', '163', '258', '46', '87', '8', '19', '72', '228', '6', '257', '167', '170'], ['190', '272', '64', '297', '206', '30', '213', '23', '274', '2', '187', '210', '179', '198', '51', '253', '236', '65', '289', '95', '292', '86', '245', '57', '159', '158', '192', '137', '290', '128']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print(len(single_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD8': 36, 'QPD5': 35, 'QPD18': 34, 'QPD25': 34, 'QPD3': 33, 'QPD20': 33, 'QPD13': 32, 'QPD30': 32, 'QPD1': 31, 'QPD10': 31, 'QPD11': 31, 'QPD15': 30, 'QPD40': 30, 'QPD6': 29, 'QPD16': 29, 'QPD19': 29, 'QPD35': 29, 'QPD45': 29, 'QPD4': 28, 'QPD9': 28, 'QPD12': 28, 'QPD14': 28, 'QPD22': 28, 'QPD24': 28, 'QPD27': 28, 'QPD29': 28, 'QPD34': 28, 'QPD21': 27, 'QPD23': 27, 'QPD26': 27})\n",
      "\n",
      "double images train by QP1 < QP2:  100\n",
      "\n",
      "double images test by QP1 < QP2:  300\n"
     ]
    }
   ],
   "source": [
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv10))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP1_csv1, second_largeQP1_csv2, second_largeQP1_csv3,\n",
    "    second_largeQP1_csv4, second_largeQP1_csv5, second_largeQP1_csv6,\n",
    "    second_largeQP1_csv7, second_largeQP1_csv8, second_largeQP1_csv9\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1\": [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"],\n",
    "    \"QPD3\": [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"],\n",
    "    \"QPD4\": [\"_1stQP20_2ndQP16_\"],\n",
    "    \"QPD5\": [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"],\n",
    "    \"QPD6\": [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"],\n",
    "    \"QPD8\": [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"],\n",
    "    \"QPD9\": [\"_1stQP25_2ndQP16_\"],\n",
    "    \"QPD10\": [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"],\n",
    "    \"QPD11\": [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"],\n",
    "    \"QPD12\": [\"_1stQP32_2ndQP20_\"],\n",
    "    \"QPD13\": [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"],\n",
    "    \"QPD14\": [\"_1stQP30_2ndQP16_\"],\n",
    "    \"QPD15\": [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"],\n",
    "    \"QPD16\": [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"],\n",
    "    \"QPD18\": [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"],\n",
    "    \"QPD19\": [\"_1stQP35_2ndQP16_\"],\n",
    "    \"QPD20\": [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"],\n",
    "    \"QPD21\": [\"_1stQP45_2ndQP24_\"],\n",
    "    \"QPD22\": [\"_1stQP32_2ndQP10_\"],\n",
    "    \"QPD23\": [\"_1stQP50_2ndQP27_\"],\n",
    "    \"QPD24\": [\"_1stQP40_2ndQP16_\"],\n",
    "    \"QPD25\": [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"],\n",
    "    \"QPD26\": [\"_1stQP50_2ndQP24_\"],\n",
    "    \"QPD27\": [\"_1stQP32_2ndQP5_\"],\n",
    "    \"QPD29\": [\"_1stQP45_2ndQP16_\"],\n",
    "    \"QPD30\": [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"],\n",
    "    \"QPD34\": [\"_1stQP50_2ndQP16_\"],\n",
    "    \"QPD35\": [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"],\n",
    "    \"QPD40\": [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"],\n",
    "    \"QPD45\": [\"_1stQP50_2ndQP5_\"]\n",
    "}\n",
    "\n",
    "# Priority QPD lists\n",
    "priority_qpd = {\"QPD1\", \"QPD3\", \"QPD4\", \"QPD5\", \"QPD6\", \"QPD8\", \"QPD9\", \"QPD10\"}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 3 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 3:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 3:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "    \n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)  \n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = selected_data[0]\n",
    "second_largeQP1_csv2 = selected_data[1]\n",
    "second_largeQP1_csv3 = selected_data[2]\n",
    "second_largeQP1_csv4 = selected_data[3]\n",
    "second_largeQP1_csv5 = selected_data[4]\n",
    "second_largeQP1_csv6 = selected_data[5]\n",
    "second_largeQP1_csv7 = selected_data[6]\n",
    "second_largeQP1_csv8 = selected_data[7]\n",
    "second_largeQP1_csv9 = selected_data[8]\n",
    "print('\\ndouble images train by QP1 < QP2: ', len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 300)\n",
    "print('\\ndouble images test by QP1 < QP2: ', len(second_largeQP1_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution: Counter({'QPD39S': 90, 'QPD27S': 90, 'QPD42S': 90, 'QPD5S': 90, 'QPD45S': 90, 'QPD10S': 90, 'QPD16S': 90, 'QPD20S': 90, 'QPD24S': 90, 'QPD32S': 90})\n",
      "\n",
      "double images sameQP:  100\n",
      "\n",
      "double images test by QP1 < QP2:  300\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_sameQP_csv1, second_sameQP_csv2, second_sameQP_csv3,\n",
    "    second_sameQP_csv4, second_sameQP_csv5, second_sameQP_csv6,\n",
    "    second_sameQP_csv7, second_sameQP_csv8, second_sameQP_csv9,\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD5S\": [\"_1stQP5_2ndQP5\"],\n",
    "    \"QPD10S\": [\"_1stQP10_2ndQP10\"],\n",
    "    \"QPD16S\": [\"_1stQP16_2ndQP16\"],\n",
    "    \"QPD20S\": [\"_1stQP20_2ndQP20\"],\n",
    "    \"QPD24S\": [\"_1stQP24_2ndQP24\"],\n",
    "    \"QPD27S\": [\"_1stQP27_2ndQP27\"],\n",
    "    \"QPD32S\": [\"_1stQP32_2ndQP32\"],\n",
    "    \"QPD39S\": [\"_1stQP39_2ndQP39\"],\n",
    "    \"QPD42S\": [\"_1stQP42_2ndQP42\"],\n",
    "    \"QPD45S\": [\"_1stQP45_2ndQP45\"],\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        while len(selected_from_dataset) < 100 and indices:\n",
    "            idx = indices.pop()\n",
    "            item = dataset[idx]\n",
    "            item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            if qpd_lists:\n",
    "                if all(selected_counts[qpd] < 100 / len(QPD) for qpd in qpd_lists):\n",
    "                    selected_from_dataset.append(item)\n",
    "                    for qpd in qpd_lists:\n",
    "                        selected_counts[qpd] += 1\n",
    "        \n",
    "        selected_data.append(selected_from_dataset)\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "\n",
    "# Print the distribution of QPD lists in the selected data (optional)\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_sameQP_csv1 = selected_data[0]\n",
    "second_sameQP_csv2 = selected_data[1]\n",
    "second_sameQP_csv3 = selected_data[2]\n",
    "second_sameQP_csv4 = selected_data[3]\n",
    "second_sameQP_csv5 = selected_data[4]\n",
    "second_sameQP_csv6 = selected_data[5]\n",
    "second_sameQP_csv7 = selected_data[6]\n",
    "second_sameQP_csv8 = selected_data[7]\n",
    "second_sameQP_csv9 = selected_data[8]\n",
    "print('\\ndouble images sameQP: ', len(second_sameQP_csv1))\n",
    "\n",
    "print('\\ndouble images test by QP1 < QP2: ', len(second_sameQP_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD2M': 45, 'QPD7M': 45, 'QPD10M': 45, 'QPD12M': 45, 'QPD17M': 43, 'QPD5M': 42, 'QPD9M': 42, 'QPD14M': 40, 'QPD4M': 39, 'QPD6M': 39, 'QPD22M': 39, 'QPD1M': 37, 'QPD24M': 37, 'QPD27M': 37, 'QPD30M': 37, 'QPD13M': 36, 'QPD15M': 36, 'QPD19M': 36, 'QPD20M': 36, 'QPD25M': 36, 'QPD29M': 36, 'QPD32M': 36, 'QPD35M': 36})\n",
      "\n",
      "double images largeQP2:  100\n",
      "\n",
      "double images test by QP1 > QP2:  300\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP2_csv1, second_largeQP2_csv2, second_largeQP2_csv3,\n",
    "    second_largeQP2_csv4, second_largeQP2_csv5, second_largeQP2_csv6,\n",
    "    second_largeQP2_csv7, second_largeQP2_csv8, second_largeQP2_csv9,\n",
    "]\n",
    "\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1M\": [\"_1stQP15_2ndQP16\"],\n",
    "    \"QPD2M\": [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"],\n",
    "    \"QPD4M\": [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"],\n",
    "    \"QPD5M\": [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"],\n",
    "    \"QPD6M\": [\"_1stQP10_2ndQP16\"],\n",
    "    \"QPD7M\": [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"],\n",
    "    \"QPD9M\": [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"],\n",
    "    \"QPD10M\": [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"],\n",
    "    \"QPD12M\": [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"],\n",
    "    \"QPD13M\": [\"_1stQP32_2ndQP45\"],\n",
    "    \"QPD14M\": [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"],\n",
    "    \"QPD15M\": [\"_1stQP30_2ndQP45\"],\n",
    "    \"QPD17M\": [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"],\n",
    "    \"QPD19M\": [\"_1stQP20_2ndQP39\"],\n",
    "    \"QPD20M\": [\"_1stQP25_2ndQP45\"],\n",
    "    \"QPD22M\": [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"],\n",
    "    \"QPD24M\": [\"_1stQP15_2ndQP39\"],\n",
    "    \"QPD25M\": [\"_1stQP20_2ndQP45\"],\n",
    "    \"QPD27M\": [\"_1stQP15_2ndQP42\"],\n",
    "    \"QPD29M\": [\"_1stQP10_2ndQP39\"],\n",
    "    \"QPD30M\": [\"_1stQP15_2ndQP45\"],\n",
    "    \"QPD32M\": [\"_1stQP10_2ndQP42\"],\n",
    "    \"QPD35M\": [\"_1stQP10_2ndQP45\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 4 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 4:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 4:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "second_largeQP2_csv1 = selected_data[0]\n",
    "second_largeQP2_csv2 = selected_data[1]\n",
    "second_largeQP2_csv3 = selected_data[2]\n",
    "second_largeQP2_csv4 = selected_data[3]\n",
    "second_largeQP2_csv5 = selected_data[4]\n",
    "second_largeQP2_csv6 = selected_data[5]\n",
    "second_largeQP2_csv7 = selected_data[6]\n",
    "second_largeQP2_csv8 = selected_data[7]\n",
    "second_largeQP2_csv9 = selected_data[8]\n",
    "print('\\ndouble images largeQP2: ', len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 300)\n",
    "print('\\ndouble images test by QP1 > QP2: ', len(second_largeQP2_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n",
      "test_csv_largeQP1 600\n",
      "test_csv_sameQP 600\n",
      "test_csv_largeQP2 600\n"
     ]
    }
   ],
   "source": [
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "print(\"train_csv_list: \", len(train_csv_list9))\n",
    "\n",
    "test_csv_largeQP1 = single_csv10 + second_largeQP1_csv10\n",
    "test_csv_sameQP = single_csv10 + second_sameQP_csv10\n",
    "test_csv_largeQP2 = single_csv10 + second_largeQP2_csv10\n",
    "\n",
    "print(\"test_csv_largeQP1\", len(test_csv_largeQP1))\n",
    "print(\"test_csv_sameQP\", len(test_csv_sameQP))\n",
    "print(\"test_csv_largeQP2\", len(test_csv_largeQP2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "\n",
    "test_df1, test_df_onlyGhost1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_csv_largeQP1)\n",
    "test_df2, test_df_onlyGhost2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_csv_sameQP)\n",
    "test_df3, test_df_onlyGhost3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_csv_largeQP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10      0   1088  12800  22228  23884      0    960  12864  21992  24184  13836   8593  1206    862   1193   1464   1480   1415  13766   9694  1358    886   1161   1408   1526   1387  14848  14404   5636   6920   1976  16216  14772  14592   5740   7128   1944  15824  0.000191   0.00186   0.000175  0.129296  0.071628\n",
      "1         16      0  25984  17200   9920   6896      0  25792  17392   9892   6924  14829   9567  2120    411   1236   1963   1187   1764  14910  10280  2233    470   1100   1990   1298   1864  10084  19912   3792   3736   1128  21348   9936  19868   3708   3368   1188  21932  0.000031  0.000946   0.000515  0.123426  0.061307\n",
      "2         20      0  33280  19760   4900   2060      0  33984  19344   4708   1964  14216   6714  3387    544   1513   1716   1249   2197  15144   7472  3190    663   1605   1624   1466   2231  13620   8836   4800   4196   1220  27328  11896  10004   4312   4176    968  28644  0.000299  0.001771   0.004631  0.194431  0.049937\n",
      "3         24      0  40576  14432   3832   1160      0  40640  14464   3804   1092  12622   2404  2272   2186   2153   1989   2459   2038  13378   2890  3009   2568   1927   2329   2189   2091  12240   2168   3296   2656    616  39024  11340   2848   4104   2824    728  38156  0.000038  0.005801   0.003876  0.239444  0.114531\n",
      "4         27      0  44800  11200   3080    920      0  45056  10960   3120    864  13518   2129  1889   2502   1980   1997   2734   1801  14033   2743  2608   2681   1662   2135   2752   1901   9168   1368   2660   1756    288  44760   8592   1400   2616   1980    304  45108  0.000089  0.005992   0.000576  0.193264  0.183531\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "595       45      0  35456  20880   3416    248      0  36032  21088   2644    236  21372  16221  1435    707    923    604   2881    172  22031  16666  1478    444    833    436   2918    148   2144    724   2040    532   1520  53040   2080    448   1104    352    816  55200  0.001817  0.002558   0.011873  0.158873  0.187309\n",
      "596       32      0  16896  12480  16844  13780      0  16832  12624  17004  13540  15951   8710  1318   1193    930    938   2314    994  16094   9227  1258   1175    888    964   2237    991  11048   6732   2384   2412   1088  36336  10680   6292   2280   2084   1008  37656  0.000064  0.000494   0.001245  0.159745  0.151773\n",
      "597       32      0  47232   7440   3576   1752      0  47360   7376   3596   1668  15234  18235  2554   2069    647   1722   4569   2005  15329  18490  2538   1934    625   1743   4715   1949   7688   2096   7280   3060   1064  38812   7312   1852   6752   2448   1128  40508  0.000043    0.0002   0.002561  0.234921  0.229807\n",
      "598       27      0  46976   5888   4144   2992      0  46976   5920   4116   2988  14462  17146  3406   2462    374   1816   3341   2466  14502  17290  3490   2519    464   1745   3494   2334   8492   3860   9152   5108   1080  32308   8144   3512   9128   5096   1004  33116  0.000003  0.000411   0.000613  0.236267   0.22416\n",
      "599       42      0  49920   8656   1380     44      0  51008   7744   1192     56  20023   6231  1238   1952   1516   2000   6556   1193  17307   5496  2198   1468   1968   1888   8498   1282   2612    328   1660    532    552  54316   1980    264    712    436    228  56380  0.001314  0.021379   0.012783   0.20853  0.184052\n",
      "\n",
      "[600 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP,\n",
    "                            X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "X_test_list1 = []\n",
    "X_test_onlyGhost_list1 = []\n",
    "MAE_list_t1 = []\n",
    "FINAL_QP_list_t1 = []\n",
    "Y_test_list1 = []\n",
    "\n",
    "X_test_list2 = []\n",
    "X_test_onlyGhost_list2 = []\n",
    "MAE_list_t2 = []\n",
    "FINAL_QP_list_t2 = []\n",
    "Y_test_list2 = []\n",
    "\n",
    "X_test_list3 = []\n",
    "X_test_onlyGhost_list3 = []\n",
    "MAE_list_t3 = []\n",
    "FINAL_QP_list_t3 = []\n",
    "Y_test_list3 = []\n",
    "\n",
    "\n",
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "\n",
    "append_results_to_lists(test_df1, test_df_onlyGhost1, LABEL_t1, MAE_t1, FINAL_QP_t1, X_test_list1, X_test_onlyGhost_list1, MAE_list_t1, FINAL_QP_list_t1, Y_test_list1)\n",
    "append_results_to_lists(test_df2, test_df_onlyGhost2, LABEL_t2, MAE_t2, FINAL_QP_t2, X_test_list2, X_test_onlyGhost_list2, MAE_list_t2, FINAL_QP_list_t2, Y_test_list2)\n",
    "append_results_to_lists(test_df3, test_df_onlyGhost3, LABEL_t3, MAE_t3, FINAL_QP_t3, X_test_list3, X_test_onlyGhost_list3, MAE_list_t3, FINAL_QP_list_t3, Y_test_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各リストの長さを確認\n",
    "# print(len(X_train_list))\n",
    "# print((X_train_onlyGhost_list[0])[0])\n",
    "# print((MAE_list[0])[0])\n",
    "# print((FINAL_QP_list[0])[0])\n",
    "# print((Y_train_list[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 8]\n",
      "Test indices: [7]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9164    0.9500    0.9329       300\n",
      "           1     0.9481    0.9133    0.9304       300\n",
      "\n",
      "    accuracy                         0.9317       600\n",
      "   macro avg     0.9322    0.9317    0.9316       600\n",
      "weighted avg     0.9322    0.9317    0.9316       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9443    0.9033    0.9233       300\n",
      "           1     0.9073    0.9467    0.9266       300\n",
      "\n",
      "    accuracy                         0.9250       600\n",
      "   macro avg     0.9258    0.9250    0.9250       600\n",
      "weighted avg     0.9258    0.9250    0.9250       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8980    0.9100    0.9040       300\n",
      "           1     0.9088    0.8967    0.9027       300\n",
      "\n",
      "    accuracy                         0.9033       600\n",
      "   macro avg     0.9034    0.9033    0.9033       600\n",
      "weighted avg     0.9034    0.9033    0.9033       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9273    0.8500    0.8870       300\n",
      "           1     0.8615    0.9333    0.8960       300\n",
      "\n",
      "    accuracy                         0.8917       600\n",
      "   macro avg     0.8944    0.8917    0.8915       600\n",
      "weighted avg     0.8944    0.8917    0.8915       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6319    0.7667    0.6928       300\n",
      "           1     0.7034    0.5533    0.6194       300\n",
      "\n",
      "    accuracy                         0.6600       600\n",
      "   macro avg     0.6676    0.6600    0.6561       600\n",
      "weighted avg     0.6676    0.6600    0.6561       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6138    0.6833    0.6467       300\n",
      "           1     0.6429    0.5700    0.6042       300\n",
      "\n",
      "    accuracy                         0.6267       600\n",
      "   macro avg     0.6283    0.6267    0.6255       600\n",
      "weighted avg     0.6283    0.6267    0.6255       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8]\n",
      "Test indices: [1]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9100    0.9433    0.9264       300\n",
      "           1     0.9412    0.9067    0.9236       300\n",
      "\n",
      "    accuracy                         0.9250       600\n",
      "   macro avg     0.9256    0.9250    0.9250       600\n",
      "weighted avg     0.9256    0.9250    0.9250       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8847    0.9467    0.9147       300\n",
      "           1     0.9427    0.8767    0.9085       300\n",
      "\n",
      "    accuracy                         0.9117       600\n",
      "   macro avg     0.9137    0.9117    0.9116       600\n",
      "weighted avg     0.9137    0.9117    0.9116       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8774    0.9067    0.8918       300\n",
      "           1     0.9034    0.8733    0.8881       300\n",
      "\n",
      "    accuracy                         0.8900       600\n",
      "   macro avg     0.8904    0.8900    0.8900       600\n",
      "weighted avg     0.8904    0.8900    0.8900       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8480    0.8367    0.8423       300\n",
      "           1     0.8388    0.8500    0.8444       300\n",
      "\n",
      "    accuracy                         0.8433       600\n",
      "   macro avg     0.8434    0.8433    0.8433       600\n",
      "weighted avg     0.8434    0.8433    0.8433       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6456    0.7833    0.7078       300\n",
      "           1     0.7246    0.5700    0.6381       300\n",
      "\n",
      "    accuracy                         0.6767       600\n",
      "   macro avg     0.6851    0.6767    0.6729       600\n",
      "weighted avg     0.6851    0.6767    0.6729       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6280    0.7767    0.6945       300\n",
      "           1     0.7074    0.5400    0.6125       300\n",
      "\n",
      "    accuracy                         0.6583       600\n",
      "   macro avg     0.6677    0.6583    0.6535       600\n",
      "weighted avg     0.6677    0.6583    0.6535       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8]\n",
      "Test indices: [5]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9111    0.9567    0.9333       300\n",
      "           1     0.9544    0.9067    0.9299       300\n",
      "\n",
      "    accuracy                         0.9317       600\n",
      "   macro avg     0.9327    0.9317    0.9316       600\n",
      "weighted avg     0.9327    0.9317    0.9316       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9283    0.9500    0.9390       300\n",
      "           1     0.9488    0.9267    0.9376       300\n",
      "\n",
      "    accuracy                         0.9383       600\n",
      "   macro avg     0.9386    0.9383    0.9383       600\n",
      "weighted avg     0.9386    0.9383    0.9383       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9147    0.8933    0.9039       300\n",
      "           1     0.8958    0.9167    0.9061       300\n",
      "\n",
      "    accuracy                         0.9050       600\n",
      "   macro avg     0.9052    0.9050    0.9050       600\n",
      "weighted avg     0.9052    0.9050    0.9050       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9255    0.8700    0.8969       300\n",
      "           1     0.8774    0.9300    0.9029       300\n",
      "\n",
      "    accuracy                         0.9000       600\n",
      "   macro avg     0.9014    0.9000    0.8999       600\n",
      "weighted avg     0.9014    0.9000    0.8999       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6361    0.8100    0.7126       300\n",
      "           1     0.7385    0.5367    0.6216       300\n",
      "\n",
      "    accuracy                         0.6733       600\n",
      "   macro avg     0.6873    0.6733    0.6671       600\n",
      "weighted avg     0.6873    0.6733    0.6671       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6292    0.7467    0.6829       300\n",
      "           1     0.6885    0.5600    0.6176       300\n",
      "\n",
      "    accuracy                         0.6533       600\n",
      "   macro avg     0.6589    0.6533    0.6503       600\n",
      "weighted avg     0.6589    0.6533    0.6503       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8]\n",
      "Test indices: [0]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9026    0.9267    0.9145       300\n",
      "           1     0.9247    0.9000    0.9122       300\n",
      "\n",
      "    accuracy                         0.9133       600\n",
      "   macro avg     0.9136    0.9133    0.9133       600\n",
      "weighted avg     0.9136    0.9133    0.9133       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9509    0.9033    0.9265       300\n",
      "           1     0.9079    0.9533    0.9301       300\n",
      "\n",
      "    accuracy                         0.9283       600\n",
      "   macro avg     0.9294    0.9283    0.9283       600\n",
      "weighted avg     0.9294    0.9283    0.9283       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8933    0.8801       300\n",
      "           1     0.8900    0.8633    0.8765       300\n",
      "\n",
      "    accuracy                         0.8783       600\n",
      "   macro avg     0.8787    0.8783    0.8783       600\n",
      "weighted avg     0.8787    0.8783    0.8783       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9453    0.8067    0.8705       300\n",
      "           1     0.8314    0.9533    0.8882       300\n",
      "\n",
      "    accuracy                         0.8800       600\n",
      "   macro avg     0.8884    0.8800    0.8794       600\n",
      "weighted avg     0.8884    0.8800    0.8794       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6479    0.7667    0.7023       300\n",
      "           1     0.7143    0.5833    0.6422       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6811    0.6750    0.6722       600\n",
      "weighted avg     0.6811    0.6750    0.6722       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6327    0.7233    0.6750       300\n",
      "           1     0.6770    0.5800    0.6248       300\n",
      "\n",
      "    accuracy                         0.6517       600\n",
      "   macro avg     0.6548    0.6517    0.6499       600\n",
      "weighted avg     0.6548    0.6517    0.6499       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [8]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9079    0.9200    0.9139       300\n",
      "           1     0.9189    0.9067    0.9128       300\n",
      "\n",
      "    accuracy                         0.9133       600\n",
      "   macro avg     0.9134    0.9133    0.9133       600\n",
      "weighted avg     0.9134    0.9133    0.9133       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9392    0.9267    0.9329       300\n",
      "           1     0.9276    0.9400    0.9338       300\n",
      "\n",
      "    accuracy                         0.9333       600\n",
      "   macro avg     0.9334    0.9333    0.9333       600\n",
      "weighted avg     0.9334    0.9333    0.9333       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8513    0.8967    0.8734       300\n",
      "           1     0.8908    0.8433    0.8664       300\n",
      "\n",
      "    accuracy                         0.8700       600\n",
      "   macro avg     0.8711    0.8700    0.8699       600\n",
      "weighted avg     0.8711    0.8700    0.8699       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9336    0.8900    0.9113       300\n",
      "           1     0.8949    0.9367    0.9153       300\n",
      "\n",
      "    accuracy                         0.9133       600\n",
      "   macro avg     0.9142    0.9133    0.9133       600\n",
      "weighted avg     0.9142    0.9133    0.9133       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6639    0.8100    0.7297       300\n",
      "           1     0.7564    0.5900    0.6629       300\n",
      "\n",
      "    accuracy                         0.7000       600\n",
      "   macro avg     0.7102    0.7000    0.6963       600\n",
      "weighted avg     0.7102    0.7000    0.6963       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6344    0.7867    0.7024       300\n",
      "           1     0.7193    0.5467    0.6212       300\n",
      "\n",
      "    accuracy                         0.6667       600\n",
      "   macro avg     0.6769    0.6667    0.6618       600\n",
      "weighted avg     0.6769    0.6667    0.6618       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8]\n",
      "Test indices: [2]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9172    0.9233    0.9203       300\n",
      "           1     0.9228    0.9167    0.9197       300\n",
      "\n",
      "    accuracy                         0.9200       600\n",
      "   macro avg     0.9200    0.9200    0.9200       600\n",
      "weighted avg     0.9200    0.9200    0.9200       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9454    0.9233    0.9342       300\n",
      "           1     0.9251    0.9467    0.9357       300\n",
      "\n",
      "    accuracy                         0.9350       600\n",
      "   macro avg     0.9352    0.9350    0.9350       600\n",
      "weighted avg     0.9352    0.9350    0.9350       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8738    0.9000    0.8867       300\n",
      "           1     0.8969    0.8700    0.8832       300\n",
      "\n",
      "    accuracy                         0.8850       600\n",
      "   macro avg     0.8853    0.8850    0.8850       600\n",
      "weighted avg     0.8853    0.8850    0.8850       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9236    0.8867    0.9048       300\n",
      "           1     0.8910    0.9267    0.9085       300\n",
      "\n",
      "    accuracy                         0.9067       600\n",
      "   macro avg     0.9073    0.9067    0.9066       600\n",
      "weighted avg     0.9073    0.9067    0.9066       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6555    0.7800    0.7123       300\n",
      "           1     0.7284    0.5900    0.6519       300\n",
      "\n",
      "    accuracy                         0.6850       600\n",
      "   macro avg     0.6919    0.6850    0.6821       600\n",
      "weighted avg     0.6919    0.6850    0.6821       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6188    0.7467    0.6767       300\n",
      "           1     0.6807    0.5400    0.6022       300\n",
      "\n",
      "    accuracy                         0.6433       600\n",
      "   macro avg     0.6497    0.6433    0.6395       600\n",
      "weighted avg     0.6497    0.6433    0.6395       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 5 6 7 8]\n",
      "Test indices: [4]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9269    0.9300    0.9285       300\n",
      "           1     0.9298    0.9267    0.9282       300\n",
      "\n",
      "    accuracy                         0.9283       600\n",
      "   macro avg     0.9283    0.9283    0.9283       600\n",
      "weighted avg     0.9283    0.9283    0.9283       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9488    0.9267    0.9376       300\n",
      "           1     0.9283    0.9500    0.9390       300\n",
      "\n",
      "    accuracy                         0.9383       600\n",
      "   macro avg     0.9386    0.9383    0.9383       600\n",
      "weighted avg     0.9386    0.9383    0.9383       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8947    0.9067    0.9007       300\n",
      "           1     0.9054    0.8933    0.8993       300\n",
      "\n",
      "    accuracy                         0.9000       600\n",
      "   macro avg     0.9001    0.9000    0.9000       600\n",
      "weighted avg     0.9001    0.9000    0.9000       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9326    0.8767    0.9038       300\n",
      "           1     0.8836    0.9367    0.9094       300\n",
      "\n",
      "    accuracy                         0.9067       600\n",
      "   macro avg     0.9081    0.9067    0.9066       600\n",
      "weighted avg     0.9081    0.9067    0.9066       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6332    0.7767    0.6976       300\n",
      "           1     0.7112    0.5500    0.6203       300\n",
      "\n",
      "    accuracy                         0.6633       600\n",
      "   macro avg     0.6722    0.6633    0.6590       600\n",
      "weighted avg     0.6722    0.6633    0.6590       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6204    0.7300    0.6708       300\n",
      "           1     0.6721    0.5533    0.6069       300\n",
      "\n",
      "    accuracy                         0.6417       600\n",
      "   macro avg     0.6462    0.6417    0.6388       600\n",
      "weighted avg     0.6462    0.6417    0.6388       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 4 5 6 7 8]\n",
      "Test indices: [3]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9128    0.9067    0.9097       300\n",
      "           1     0.9073    0.9133    0.9103       300\n",
      "\n",
      "    accuracy                         0.9100       600\n",
      "   macro avg     0.9100    0.9100    0.9100       600\n",
      "weighted avg     0.9100    0.9100    0.9100       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9365    0.9333    0.9349       300\n",
      "           1     0.9336    0.9367    0.9351       300\n",
      "\n",
      "    accuracy                         0.9350       600\n",
      "   macro avg     0.9350    0.9350    0.9350       600\n",
      "weighted avg     0.9350    0.9350    0.9350       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8863    0.8833    0.8848       300\n",
      "           1     0.8837    0.8867    0.8852       300\n",
      "\n",
      "    accuracy                         0.8850       600\n",
      "   macro avg     0.8850    0.8850    0.8850       600\n",
      "weighted avg     0.8850    0.8850    0.8850       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9189    0.9067    0.9128       300\n",
      "           1     0.9079    0.9200    0.9139       300\n",
      "\n",
      "    accuracy                         0.9133       600\n",
      "   macro avg     0.9134    0.9133    0.9133       600\n",
      "weighted avg     0.9134    0.9133    0.9133       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6462    0.7733    0.7041       300\n",
      "           1     0.7178    0.5767    0.6396       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6820    0.6750    0.6718       600\n",
      "weighted avg     0.6820    0.6750    0.6718       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6270    0.7900    0.6991       300\n",
      "           1     0.7162    0.5300    0.6092       300\n",
      "\n",
      "    accuracy                         0.6600       600\n",
      "   macro avg     0.6716    0.6600    0.6542       600\n",
      "weighted avg     0.6716    0.6600    0.6542       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 3 4 5 7 8]\n",
      "Test indices: [6]\n",
      "0.86\n",
      "0.505\n",
      "0.57\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9076    0.9500    0.9283       300\n",
      "           1     0.9476    0.9033    0.9249       300\n",
      "\n",
      "    accuracy                         0.9267       600\n",
      "   macro avg     0.9276    0.9267    0.9266       600\n",
      "weighted avg     0.9276    0.9267    0.9266       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8885    0.9567    0.9213       300\n",
      "           1     0.9531    0.8800    0.9151       300\n",
      "\n",
      "    accuracy                         0.9183       600\n",
      "   macro avg     0.9208    0.9183    0.9182       600\n",
      "weighted avg     0.9208    0.9183    0.9182       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9010    0.9100    0.9055       300\n",
      "           1     0.9091    0.9000    0.9045       300\n",
      "\n",
      "    accuracy                         0.9050       600\n",
      "   macro avg     0.9050    0.9050    0.9050       600\n",
      "weighted avg     0.9050    0.9050    0.9050       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8190    0.8600    0.8390       300\n",
      "           1     0.8526    0.8100    0.8308       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8358    0.8350    0.8349       600\n",
      "weighted avg     0.8358    0.8350    0.8349       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6403    0.7833    0.7046       300\n",
      "           1     0.7210    0.5600    0.6304       300\n",
      "\n",
      "    accuracy                         0.6717       600\n",
      "   macro avg     0.6807    0.6717    0.6675       600\n",
      "weighted avg     0.6807    0.6717    0.6675       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6234    0.8000    0.7007       300\n",
      "           1     0.7209    0.5167    0.6019       300\n",
      "\n",
      "    accuracy                         0.6583       600\n",
      "   macro avg     0.6722    0.6583    0.6513       600\n",
      "weighted avg     0.6722    0.6583    0.6513       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.8500    0.8586       300\n",
      "           1     0.8529    0.8700    0.8614       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8601    0.8600    0.8600       600\n",
      "weighted avg     0.8601    0.8600    0.8600       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5061    0.4167    0.4570       300\n",
      "           1     0.5042    0.5933    0.5452       300\n",
      "\n",
      "    accuracy                         0.5050       600\n",
      "   macro avg     0.5052    0.5050    0.5011       600\n",
      "weighted avg     0.5052    0.5050    0.5011       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6567    0.2933    0.4055       300\n",
      "           1     0.5451    0.8467    0.6632       300\n",
      "\n",
      "    accuracy                         0.5700       600\n",
      "   macro avg     0.6009    0.5700    0.5344       600\n",
      "weighted avg     0.6009    0.5700    0.5344       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "kfold = KFold(n_splits=9, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# results = pd.DataFrame(columns=['C_RBF_LQP1','Score_RBF_LQP1', 'tnr_rbf_lqp1', 'tpr_rbf_lqp1',\n",
    "#                                 'C_RBF_SQP','Score_RBF_SQP', 'tnr_rbf_sqp', 'tpr_rbf_sqp',\n",
    "#                                 'C_RBF_LQP2','Score_RBF_LQP2', 'tnr_rbf_lqp2', 'tpr_rbf_lqp2',\n",
    "#                                 'C_LINEAR_LQP1','Score_LINEAR_LQP1', 'tnr_linear_lqp1', 'tpr_linear_lqp1',\n",
    "#                                 'C_LINEAR_SQP','Score_LINEAR_SQP', 'tnr_linear_sqp', 'tpr_linear_sqp',\n",
    "#                                 'C_LINEAR_LQP2','Score_LINEAR_LQP2', 'tnr_linear_lqp2', 'tpr_linear_lqp2',\n",
    "                                \n",
    "#                                 'C_RBF_OG_LQP1','Score_RBF_OG_LQP1', 'tnr_og_rbf_lqp1', 'tpr_og_rbf_lqp1',\n",
    "#                                 'C_RBF_OG_SQP','Score_RBF_OG_SQP', 'tnr_og_rbf_sqp', 'tpr_og_rbf_sqp',\n",
    "#                                 'C_RBF_OG_LQP2','Score_RBF_OG_LQP2', 'tnr_og_rbf_lqp2', 'tpr_og_rbf_lqp2',\n",
    "#                                 'C_LINEAR_OG_LQP1','Score_LINEAR_OG_LQP1', 'tnr_og_linear_lqp1', 'tpr_og_linear_lqp1',\n",
    "#                                 'C_LINEAR_OG_SQP','Score_LINEAR_OG_SQP', 'tnr_og_linear_sqp', 'tpr_og_linear_sqp',\n",
    "#                                 'C_LINEAR_OG_LQP2','Score_LINEAR_OG_LQP2', 'tnr_og_linear_lqp2', 'tpr_og_linear_lqp2',\n",
    "                                \n",
    "#                                 'Threshold_LQP1', 'LQP1_old', 'tnr_old_lqp1', 'tpr_old_lqp1',\n",
    "#                                 'Threshold_SQP', 'SQP_old', 'tnr_old_sqp', 'tpr_old_sqp',\n",
    "#                                 'Threshold_LQP2', 'LQP2_old', 'tnr_old_lqp2', 'tpr_old_lqp2'])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['C_RBF_LQP1','Score_RBF_LQP1', 'tnr_rbf_lqp1', 'tpr_rbf_lqp1',\n",
    "                                'C_RBF_SQP','Score_RBF_SQP', 'tnr_rbf_sqp', 'tpr_rbf_sqp',\n",
    "                                'C_RBF_LQP2','Score_RBF_LQP2', 'tnr_rbf_lqp2', 'tpr_rbf_lqp2',\n",
    "                                'C_LINEAR_LQP1','Score_LINEAR_LQP1', 'tnr_linear_lqp1', 'tpr_linear_lqp1',\n",
    "                                'C_LINEAR_SQP','Score_LINEAR_SQP', 'tnr_linear_sqp', 'tpr_linear_sqp',\n",
    "                                'C_LINEAR_LQP2','Score_LINEAR_LQP2', 'tnr_linear_lqp2', 'tpr_linear_lqp2',\n",
    "                                                        \n",
    "                                'Threshold_LQP1', 'LQP1_old', 'tnr_old_lqp1', 'tpr_old_lqp1',\n",
    "                                'Threshold_SQP', 'SQP_old', 'tnr_old_sqp', 'tpr_old_sqp',\n",
    "                                'Threshold_LQP2', 'LQP2_old', 'tnr_old_lqp2', 'tpr_old_lqp2'])\n",
    "\n",
    "    \n",
    "X_index = np.arange(9)  # インデックスとして0から8までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    X_train_OG = [item for data in train_data_OG for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    X_val_OG = [item for data in val_data_OG for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    # print(len(Y_train))\n",
    "    # print(len(Y_val))\n",
    "    \n",
    "    test_data1 = [item for data in X_test_list1 for item in data]\n",
    "    test_data_OG1 = [item for data in X_test_onlyGhost_list1 for item in data]\n",
    "    test_label1 = [item for data in Y_test_list1 for item in data]\n",
    "    MAE_data1 = [item for data in MAE_list_t1 for item in data]\n",
    "    FINAL_QP_data1 = [item for data in FINAL_QP_list_t1 for item in data]\n",
    "    \n",
    "    test_data2 = [item for data in X_test_list2 for item in data]\n",
    "    test_data_OG2 = [item for data in X_test_onlyGhost_list2 for item in data]\n",
    "    test_label2 = [item for data in Y_test_list2 for item in data]\n",
    "    MAE_data2 = [item for data in MAE_list_t2 for item in data]\n",
    "    FINAL_QP_data2 = [item for data in FINAL_QP_list_t2 for item in data]\n",
    "    \n",
    "    test_data3 = [item for data in X_test_list3 for item in data]\n",
    "    test_data_OG3 = [item for data in X_test_onlyGhost_list3 for item in data]\n",
    "    test_label3 = [item for data in Y_test_list3 for item in data]\n",
    "    MAE_data3 = [item for data in MAE_list_t3 for item in data]\n",
    "    FINAL_QP_data3 = [item for data in FINAL_QP_list_t3 for item in data]\n",
    "    \n",
    "    # print(len(MAE_data1))\n",
    "    # print(len(MAE_data2))\n",
    "    # print(len(MAE_data3))\n",
    "    \n",
    "                                                                                   \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    sameQP_best_threshold = 0\n",
    "    sameQP_best_accuracy = 0\n",
    "    sameQP_best_predicted_labels = []\n",
    "    sameQP_best_ground_truth_labels = []\n",
    "    \n",
    "    largeQP_best_threshold = 0\n",
    "    largeQP_best_accuracy = 0\n",
    "    largeQP_best_predicted_labels = []\n",
    "    largeQP_best_ground_truth_labels = []\n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_old = np.array([is_double_compressed(MAE_data1[i], FINAL_QP_data1[i], threshold) for i in range(600)])\n",
    "        predicted_labels = test_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label1)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_sameQP_old = np.array([is_double_compressed(MAE_data2[i], FINAL_QP_data2[i], threshold) for i in range(600)])\n",
    "        same_predicted_labels = test_sameQP_old.astype(int)\n",
    "        same_ground_truth_labels = np.array(test_label2)\n",
    "        same_accuracy = np.sum(same_ground_truth_labels == same_predicted_labels) / len(same_ground_truth_labels)\n",
    "    \n",
    "        if same_accuracy > sameQP_best_accuracy:\n",
    "            sameQP_best_accuracy = same_accuracy\n",
    "            sameQP_best_threshold = threshold\n",
    "            sameQP_best_predicted_labels = same_predicted_labels\n",
    "            sameQP_best_ground_truth_labels = same_ground_truth_labels\n",
    "                        \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_largeQP_old = np.array([is_double_compressed(MAE_data3[i], FINAL_QP_data3[i], threshold) for i in range(600)])\n",
    "        large_predicted_labels = test_largeQP_old.astype(int)\n",
    "        large_ground_truth_labels = np.array(test_label3)\n",
    "        large_accuracy = np.sum(large_ground_truth_labels == large_predicted_labels) / len(large_ground_truth_labels)\n",
    "    \n",
    "        if large_accuracy > largeQP_best_accuracy:\n",
    "            largeQP_best_accuracy = large_accuracy\n",
    "            largeQP_best_threshold = threshold\n",
    "            largeQP_best_predicted_labels = large_predicted_labels\n",
    "            largeQP_best_ground_truth_labels = large_ground_truth_labels       \n",
    "            \n",
    "            \n",
    "    print(best_accuracy)\n",
    "    print(sameQP_best_accuracy)\n",
    "    print(largeQP_best_accuracy)\n",
    "            \n",
    "            \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "\n",
    "    # テストデータで評価    \n",
    "    predictions_RBF = best_svm_model_RBF.predict(test_data1)\n",
    "    # predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    accuracy_RBF = accuracy_score(test_label1, predictions_RBF)\n",
    "    report_RBF = classification_report(test_label1, predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label1, predictions_RBF)\n",
    "    tnr_rbf_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'report_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    predictions_LINEAR = best_svm_model_LINEAR.predict(test_data1)\n",
    "    # predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    accuracy_LINEAR = accuracy_score(test_label1, predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label1, predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label1, predictions_LINEAR)\n",
    "    tnr_linear_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'report_LINEAR:\\n{report_LINEAR}')\n",
    "    \n",
    "    same_predictions_RBF = best_svm_model_RBF.predict(test_data2)\n",
    "    # same_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_sameQP)\n",
    "    same_accuracy_RBF = accuracy_score(test_label2, same_predictions_RBF)\n",
    "    same_report_RBF = classification_report(test_label2, same_predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label2, same_predictions_RBF)\n",
    "    tnr_rbf_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'same_report_RBF:\\n{same_report_RBF}')\n",
    "    \n",
    "    same_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data2)\n",
    "    # same_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_sameQP)\n",
    "    same_accuracy_LINEAR = accuracy_score(test_label2, same_predictions_LINEAR)\n",
    "    same_report_LINEAR = classification_report(test_label2, same_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label2, same_predictions_LINEAR)\n",
    "    tnr_linear_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'same_report_LINEAR:\\n{same_report_LINEAR}')\n",
    "    \n",
    "    large_predictions_RBF = best_svm_model_RBF.predict(test_data3)\n",
    "    # large_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_largeQP)\n",
    "    large_accuracy_RBF = accuracy_score(test_label3, large_predictions_RBF)\n",
    "    large_report_RBF = classification_report(test_label3, large_predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label3, large_predictions_RBF)\n",
    "    tnr_rbf_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'large_report_RBF:\\n{large_report_RBF}')\n",
    "    \n",
    "    large_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data3)\n",
    "    # large_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_largeQP)\n",
    "    large_accuracy_LINEAR = accuracy_score(test_label3, large_predictions_LINEAR)\n",
    "    large_report_LINEAR = classification_report(test_label3, large_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label3, large_predictions_LINEAR)\n",
    "    tnr_linear_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'large_report_LINEAR:\\n{large_report_LINEAR}')\n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_old}')\n",
    "    \n",
    "    test_sameQP_old = classification_report(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels)\n",
    "    tnr_old_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_sameQP_old}')\n",
    "    \n",
    "    test_largeQP_old = classification_report(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels)\n",
    "    tnr_old_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_largeQP_old}')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row ={'C_RBF_LQP1':best_c_value_RBF,'Score_RBF_LQP1': accuracy_RBF, 'tnr_rbf_lqp1':tnr_rbf_lqp1, 'tpr_rbf_lqp1':tpr_rbf_lqp1,\n",
    "                'C_RBF_SQP': best_c_value_RBF, 'Score_RBF_SQP': same_accuracy_RBF, 'tnr_rbf_sqp':tnr_rbf_sqp, 'tpr_rbf_sqp':tpr_rbf_sqp,\n",
    "                'C_RBF_LQP2': best_c_value_RBF,'Score_RBF_LQP2': large_accuracy_RBF, 'tnr_rbf_lqp2':tnr_rbf_lqp2, 'tpr_rbf_lqp2':tpr_rbf_lqp2,\n",
    "                                    \n",
    "                'C_LINEAR_LQP1': best_c_value_LINEAR,'Score_LINEAR_LQP1':accuracy_LINEAR, 'tnr_linear_lqp1':tnr_linear_lqp1, 'tpr_linear_lqp1':tpr_linear_lqp1,\n",
    "                'C_LINEAR_SQP': best_c_value_LINEAR,'Score_LINEAR_SQP':same_accuracy_LINEAR, 'tnr_linear_sqp':tnr_linear_sqp, 'tpr_linear_sqp':tpr_linear_sqp,\n",
    "                'C_LINEAR_LQP2': best_c_value_LINEAR,'Score_LINEAR_LQP2':large_accuracy_LINEAR, 'tnr_linear_lqp2':tnr_linear_lqp2, 'tpr_linear_lqp2':tpr_linear_lqp2,\n",
    "                                                        \n",
    "                'Threshold_LQP1':best_threshold, 'LQP1_old':best_accuracy, 'tnr_old_lqp1':tnr_old_lqp1, 'tpr_old_lqp1':tpr_old_lqp1,\n",
    "                'Threshold_SQP':sameQP_best_threshold, 'SQP_old':sameQP_best_accuracy, 'tnr_old_sqp':tnr_old_sqp, 'tpr_old_sqp':tpr_old_sqp,\n",
    "                'Threshold_LQP2':largeQP_best_threshold, 'LQP2_old':largeQP_best_accuracy, 'tnr_old_lqp2':tnr_old_lqp2, 'tpr_old_lqp2':tpr_old_lqp2}\n",
    "\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0     RBF_LQP1        93.41        91.04               92.22                0.83           93.17           91.00\n",
      "1      RBF_SQP        90.00        88.26               89.13                1.27           90.50           87.00\n",
      "2     RBF_LQP2        78.33        56.78               67.56                1.17           70.00           66.00\n",
      "3  LINEAR_LQP1        93.00        92.85               92.93                0.93           93.83           91.17\n",
      "4   LINEAR_SQP        86.48        91.07               88.78                2.96           91.33           83.50\n",
      "5  LINEAR_LQP2        75.37        54.85               65.11                1.21           66.67           62.67\n",
      "6     OLD_LQP1        85.00        87.00               86.00                0.00           86.00           86.00\n",
      "7      OLD_SQP        41.67        59.33               50.50                0.00           50.50           50.50\n",
      "8     OLD_LQP2        29.33        84.67               57.00                0.00           57.00           57.00\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF_LQP1', 'RBF_SQP', 'RBF_LQP2', 'LINEAR_LQP1', 'LINEAR_SQP', 'LINEAR_LQP2', 'OLD_LQP1', 'OLD_SQP', 'OLD_LQP2'],\n",
    "    'Average TNR': [round(results['tnr_rbf_lqp1'].mean() * 100, 2), round(results['tnr_rbf_sqp'].mean() * 100, 2), round(results['tnr_rbf_lqp2'].mean() * 100, 2), round(results['tnr_linear_lqp1'].mean() * 100, 2), round(results['tnr_linear_sqp'].mean() * 100, 2), round(results['tnr_linear_lqp2'].mean() * 100, 2),round(results['tnr_old_lqp1'].mean() * 100, 2), round(results['tnr_old_sqp'].mean() * 100, 2), round(results['tnr_old_lqp2'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf_lqp1'].mean() * 100, 2), round(results['tpr_rbf_sqp'].mean() * 100, 2), round(results['tpr_rbf_lqp2'].mean() * 100, 2), round(results['tpr_linear_lqp1'].mean() * 100, 2), round(results['tpr_linear_sqp'].mean() * 100, 2), round(results['tpr_linear_lqp2'].mean() * 100, 2),round(results['tpr_old_lqp1'].mean() * 100, 2), round(results['tpr_old_sqp'].mean() * 100, 2), round(results['tpr_old_lqp2'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF_LQP1'].mean() * 100, 2), round(results['Score_RBF_SQP'].mean() * 100, 2), round(results['Score_RBF_LQP2'].mean() * 100, 2), round(results['Score_LINEAR_LQP1'].mean() * 100, 2), round(results['Score_LINEAR_SQP'].mean() * 100, 2), round(results['Score_LINEAR_LQP2'].mean() * 100, 2),round(results['LQP1_old'].mean() * 100, 2), round(results['SQP_old'].mean() * 100, 2), round(results['LQP2_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF_LQP1'].std() * 100, 2), round(results['Score_RBF_SQP'].std() * 100, 2), round(results['Score_RBF_LQP2'].std() * 100, 2), round(results['Score_LINEAR_LQP1'].std() * 100, 2), round(results['Score_LINEAR_SQP'].std() * 100, 2), round(results['Score_LINEAR_LQP2'].std() * 100, 2),round(results['LQP1_old'].std() * 100, 2), round(results['SQP_old'].std() * 100, 2), round(results['LQP2_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF_LQP1'].max() * 100, 2), round(results['Score_RBF_SQP'].max() * 100, 2), round(results['Score_RBF_LQP2'].max() * 100, 2), round(results['Score_LINEAR_LQP1'].max() * 100, 2), round(results['Score_LINEAR_SQP'].max() * 100, 2), round(results['Score_LINEAR_LQP2'].max() * 100, 2),round(results['LQP1_old'].max() * 100, 2), round(results['SQP_old'].max() * 100, 2), round(results['LQP2_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF_LQP1'].min() * 100, 2), round(results['Score_RBF_SQP'].min() * 100, 2), round(results['Score_RBF_LQP2'].min() * 100, 2), round(results['Score_LINEAR_LQP1'].min() * 100, 2), round(results['Score_LINEAR_SQP'].min() * 100, 2), round(results['Score_LINEAR_LQP2'].min() * 100, 2),round(results['LQP1_old'].min() * 100, 2), round(results['SQP_old'].min() * 100, 2), round(results['LQP2_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10\n",
      "1     10\n",
      "2     10\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6     10\n",
      "7    100\n",
      "8     10\n",
      "Name: C_RBF_LQP1, dtype: object\n",
      "0      10\n",
      "1       1\n",
      "2      10\n",
      "3    5000\n",
      "4    1000\n",
      "5      10\n",
      "6      10\n",
      "7    3000\n",
      "8       1\n",
      "Name: C_LINEAR_LQP1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF_LQP1'])\n",
    "print(results['C_LINEAR_LQP1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
