{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['129', '51', '282', '241', '128', '228', '27', '270', '84', '90', '230', '155', '220', '105', '299', '256', '167', '137', '97', '12', '38', '252', '8', '56', '277', '107', '171', '218', '157', '110'], ['198', '46', '25', '295', '275', '199', '278', '138', '77', '239', '289', '131', '69', '190', '188', '153', '162', '274', '9', '159', '205', '81', '266', '181', '235', '267', '285', '108', '236', '262'], ['201', '169', '72', '96', '3', '103', '115', '292', '119', '35', '41', '71', '142', '28', '10', '217', '124', '40', '19', '144', '227', '139', '168', '221', '268', '94', '176', '166', '180', '57'], ['130', '196', '50', '65', '183', '102', '88', '161', '212', '175', '47', '44', '207', '154', '186', '280', '179', '141', '177', '85', '82', '15', '203', '244', '146', '7', '60', '68', '200', '223'], ['281', '36', '219', '59', '273', '232', '48', '163', '39', '294', '210', '52', '173', '257', '192', '284', '193', '288', '109', '248', '174', '61', '2', '132', '225', '194', '126', '123', '93', '260'], ['253', '99', '70', '222', '43', '286', '86', '31', '240', '114', '6', '147', '170', '145', '79', '75', '206', '297', '111', '11', '116', '74', '23', '258', '83', '293', '73', '45', '58', '104'], ['34', '20', '209', '78', '237', '95', '216', '135', '4', '296', '255', '1', '134', '32', '265', '91', '224', '164', '37', '42', '229', '214', '120', '16', '62', '22', '233', '195', '98', '150'], ['80', '197', '250', '226', '291', '156', '118', '251', '148', '272', '276', '187', '133', '202', '158', '279', '24', '263', '189', '185', '101', '76', '247', '287', '249', '283', '246', '152', '26', '204'], ['53', '54', '112', '29', '14', '113', '243', '254', '238', '117', '165', '106', '49', '17', '242', '245', '21', '100', '261', '290', '271', '30', '215', '269', '149', '125', '143', '64', '151', '87'], ['63', '5', '191', '178', '298', '89', '259', '231', '208', '264', '160', '13', '140', '18', '121', '66', '182', '184', '136', '300', '211', '92', '213', '122', '127', '67', '55', '33', '172', '234']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print(len(single_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD8': 36, 'QPD5': 35, 'QPD10': 34, 'QPD3': 33, 'QPD25': 33, 'QPD30': 33, 'QPD1': 32, 'QPD15': 32, 'QPD35': 32, 'QPD13': 31, 'QPD40': 31, 'QPD6': 30, 'QPD16': 30, 'QPD18': 30, 'QPD4': 29, 'QPD11': 29, 'QPD14': 29, 'QPD19': 29, 'QPD20': 29, 'QPD24': 29, 'QPD29': 29, 'QPD22': 28, 'QPD34': 28, 'QPD9': 27, 'QPD12': 27, 'QPD21': 27, 'QPD23': 27, 'QPD26': 27, 'QPD27': 27, 'QPD45': 27})\n",
      "\n",
      "double images train by QP1 < QP2:  100\n",
      "\n",
      "double images test by QP1 < QP2:  300\n"
     ]
    }
   ],
   "source": [
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv10))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP1_csv1, second_largeQP1_csv2, second_largeQP1_csv3,\n",
    "    second_largeQP1_csv4, second_largeQP1_csv5, second_largeQP1_csv6,\n",
    "    second_largeQP1_csv7, second_largeQP1_csv8, second_largeQP1_csv9\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1\": [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"],\n",
    "    \"QPD3\": [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"],\n",
    "    \"QPD4\": [\"_1stQP20_2ndQP16_\"],\n",
    "    \"QPD5\": [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"],\n",
    "    \"QPD6\": [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"],\n",
    "    \"QPD8\": [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"],\n",
    "    \"QPD9\": [\"_1stQP25_2ndQP16_\"],\n",
    "    \"QPD10\": [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"],\n",
    "    \"QPD11\": [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"],\n",
    "    \"QPD12\": [\"_1stQP32_2ndQP20_\"],\n",
    "    \"QPD13\": [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"],\n",
    "    \"QPD14\": [\"_1stQP30_2ndQP16_\"],\n",
    "    \"QPD15\": [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"],\n",
    "    \"QPD16\": [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"],\n",
    "    \"QPD18\": [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"],\n",
    "    \"QPD19\": [\"_1stQP35_2ndQP16_\"],\n",
    "    \"QPD20\": [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"],\n",
    "    \"QPD21\": [\"_1stQP45_2ndQP24_\"],\n",
    "    \"QPD22\": [\"_1stQP32_2ndQP10_\"],\n",
    "    \"QPD23\": [\"_1stQP50_2ndQP27_\"],\n",
    "    \"QPD24\": [\"_1stQP40_2ndQP16_\"],\n",
    "    \"QPD25\": [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"],\n",
    "    \"QPD26\": [\"_1stQP50_2ndQP24_\"],\n",
    "    \"QPD27\": [\"_1stQP32_2ndQP5_\"],\n",
    "    \"QPD29\": [\"_1stQP45_2ndQP16_\"],\n",
    "    \"QPD30\": [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"],\n",
    "    \"QPD34\": [\"_1stQP50_2ndQP16_\"],\n",
    "    \"QPD35\": [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"],\n",
    "    \"QPD40\": [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"],\n",
    "    \"QPD45\": [\"_1stQP50_2ndQP5_\"]\n",
    "}\n",
    "\n",
    "# Priority QPD lists\n",
    "priority_qpd = {\"QPD1\", \"QPD3\", \"QPD4\", \"QPD5\", \"QPD6\", \"QPD8\", \"QPD9\", \"QPD10\"}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 3 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 3:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 3:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "    \n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)  \n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = selected_data[0]\n",
    "second_largeQP1_csv2 = selected_data[1]\n",
    "second_largeQP1_csv3 = selected_data[2]\n",
    "second_largeQP1_csv4 = selected_data[3]\n",
    "second_largeQP1_csv5 = selected_data[4]\n",
    "second_largeQP1_csv6 = selected_data[5]\n",
    "second_largeQP1_csv7 = selected_data[6]\n",
    "second_largeQP1_csv8 = selected_data[7]\n",
    "second_largeQP1_csv9 = selected_data[8]\n",
    "print('\\ndouble images train by QP1 < QP2: ', len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 300)\n",
    "print('\\ndouble images test by QP1 < QP2: ', len(second_largeQP1_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution: Counter({'QPD45S': 90, 'QPD42S': 90, 'QPD24S': 90, 'QPD16S': 90, 'QPD27S': 90, 'QPD39S': 90, 'QPD32S': 90, 'QPD10S': 90, 'QPD20S': 90, 'QPD5S': 90})\n",
      "\n",
      "double images sameQP:  100\n",
      "\n",
      "double images test by QP1 < QP2:  300\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_sameQP_csv1, second_sameQP_csv2, second_sameQP_csv3,\n",
    "    second_sameQP_csv4, second_sameQP_csv5, second_sameQP_csv6,\n",
    "    second_sameQP_csv7, second_sameQP_csv8, second_sameQP_csv9,\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD5S\": [\"_1stQP5_2ndQP5\"],\n",
    "    \"QPD10S\": [\"_1stQP10_2ndQP10\"],\n",
    "    \"QPD16S\": [\"_1stQP16_2ndQP16\"],\n",
    "    \"QPD20S\": [\"_1stQP20_2ndQP20\"],\n",
    "    \"QPD24S\": [\"_1stQP24_2ndQP24\"],\n",
    "    \"QPD27S\": [\"_1stQP27_2ndQP27\"],\n",
    "    \"QPD32S\": [\"_1stQP32_2ndQP32\"],\n",
    "    \"QPD39S\": [\"_1stQP39_2ndQP39\"],\n",
    "    \"QPD42S\": [\"_1stQP42_2ndQP42\"],\n",
    "    \"QPD45S\": [\"_1stQP45_2ndQP45\"],\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        while len(selected_from_dataset) < 100 and indices:\n",
    "            idx = indices.pop()\n",
    "            item = dataset[idx]\n",
    "            item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            if qpd_lists:\n",
    "                if all(selected_counts[qpd] < 100 / len(QPD) for qpd in qpd_lists):\n",
    "                    selected_from_dataset.append(item)\n",
    "                    for qpd in qpd_lists:\n",
    "                        selected_counts[qpd] += 1\n",
    "        \n",
    "        selected_data.append(selected_from_dataset)\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "\n",
    "# Print the distribution of QPD lists in the selected data (optional)\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_sameQP_csv1 = selected_data[0]\n",
    "second_sameQP_csv2 = selected_data[1]\n",
    "second_sameQP_csv3 = selected_data[2]\n",
    "second_sameQP_csv4 = selected_data[3]\n",
    "second_sameQP_csv5 = selected_data[4]\n",
    "second_sameQP_csv6 = selected_data[5]\n",
    "second_sameQP_csv7 = selected_data[6]\n",
    "second_sameQP_csv8 = selected_data[7]\n",
    "second_sameQP_csv9 = selected_data[8]\n",
    "print('\\ndouble images sameQP: ', len(second_sameQP_csv1))\n",
    "\n",
    "print('\\ndouble images test by QP1 < QP2: ', len(second_sameQP_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD7M': 45, 'QPD10M': 44, 'QPD12M': 44, 'QPD2M': 43, 'QPD17M': 42, 'QPD4M': 41, 'QPD5M': 41, 'QPD9M': 41, 'QPD14M': 41, 'QPD19M': 38, 'QPD22M': 38, 'QPD24M': 38, 'QPD30M': 38, 'QPD35M': 38, 'QPD13M': 37, 'QPD25M': 37, 'QPD29M': 37, 'QPD32M': 37, 'QPD1M': 36, 'QPD6M': 36, 'QPD15M': 36, 'QPD20M': 36, 'QPD27M': 36})\n",
      "\n",
      "double images largeQP2:  100\n",
      "\n",
      "double images test by QP1 > QP2:  300\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP2_csv1, second_largeQP2_csv2, second_largeQP2_csv3,\n",
    "    second_largeQP2_csv4, second_largeQP2_csv5, second_largeQP2_csv6,\n",
    "    second_largeQP2_csv7, second_largeQP2_csv8, second_largeQP2_csv9,\n",
    "]\n",
    "\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1M\": [\"_1stQP15_2ndQP16\"],\n",
    "    \"QPD2M\": [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"],\n",
    "    \"QPD4M\": [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"],\n",
    "    \"QPD5M\": [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"],\n",
    "    \"QPD6M\": [\"_1stQP10_2ndQP16\"],\n",
    "    \"QPD7M\": [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"],\n",
    "    \"QPD9M\": [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"],\n",
    "    \"QPD10M\": [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"],\n",
    "    \"QPD12M\": [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"],\n",
    "    \"QPD13M\": [\"_1stQP32_2ndQP45\"],\n",
    "    \"QPD14M\": [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"],\n",
    "    \"QPD15M\": [\"_1stQP30_2ndQP45\"],\n",
    "    \"QPD17M\": [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"],\n",
    "    \"QPD19M\": [\"_1stQP20_2ndQP39\"],\n",
    "    \"QPD20M\": [\"_1stQP25_2ndQP45\"],\n",
    "    \"QPD22M\": [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"],\n",
    "    \"QPD24M\": [\"_1stQP15_2ndQP39\"],\n",
    "    \"QPD25M\": [\"_1stQP20_2ndQP45\"],\n",
    "    \"QPD27M\": [\"_1stQP15_2ndQP42\"],\n",
    "    \"QPD29M\": [\"_1stQP10_2ndQP39\"],\n",
    "    \"QPD30M\": [\"_1stQP15_2ndQP45\"],\n",
    "    \"QPD32M\": [\"_1stQP10_2ndQP42\"],\n",
    "    \"QPD35M\": [\"_1stQP10_2ndQP45\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 4 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 4:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 4:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "second_largeQP2_csv1 = selected_data[0]\n",
    "second_largeQP2_csv2 = selected_data[1]\n",
    "second_largeQP2_csv3 = selected_data[2]\n",
    "second_largeQP2_csv4 = selected_data[3]\n",
    "second_largeQP2_csv5 = selected_data[4]\n",
    "second_largeQP2_csv6 = selected_data[5]\n",
    "second_largeQP2_csv7 = selected_data[6]\n",
    "second_largeQP2_csv8 = selected_data[7]\n",
    "second_largeQP2_csv9 = selected_data[8]\n",
    "print('\\ndouble images largeQP2: ', len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 300)\n",
    "print('\\ndouble images test by QP1 > QP2: ', len(second_largeQP2_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n",
      "test_csv_largeQP1 600\n",
      "test_csv_sameQP 600\n",
      "test_csv_largeQP2 600\n"
     ]
    }
   ],
   "source": [
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "print(\"train_csv_list: \", len(train_csv_list9))\n",
    "\n",
    "test_csv_largeQP1 = single_csv10 + second_largeQP1_csv10\n",
    "test_csv_sameQP = single_csv10 + second_sameQP_csv10\n",
    "test_csv_largeQP2 = single_csv10 + second_largeQP2_csv10\n",
    "\n",
    "print(\"test_csv_largeQP1\", len(test_csv_largeQP1))\n",
    "print(\"test_csv_sameQP\", len(test_csv_sameQP))\n",
    "print(\"test_csv_largeQP2\", len(test_csv_largeQP2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "\n",
    "test_df1, test_df_onlyGhost1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_csv_largeQP1)\n",
    "test_df2, test_df_onlyGhost2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_csv_sameQP)\n",
    "test_df3, test_df_onlyGhost3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_csv_largeQP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10      0   4416  13248  19124  23212      0   4416  13120  19212  23252  11577  11963  1168    698    622   1562   1354   1128  11022  13580  1158    705    776   1573   1359   1078  13232  11752   7976  11232   3556  12252  13020  11708   8204  11648   3200  12220  0.000014   0.00363   0.000528  0.183391  0.100349\n",
      "1         16      0  21056  15104  14604   9236      0  20800  15296  14512   9392  11130  12324  1341    505    966   1656   1439   1265  10752  13789  1254    466    816   1572   1614   1194   8116   5140   4140   5796   2672  34136   7652   5172   4064   5580   2464  35068  0.000073  0.003521   0.000663  0.068719  0.039025\n",
      "2         20      0  24512  22144   8612   4732      0  24000  22592   8640   4768  11926   9391  1378    505    681   1489   1810   1571  11586  10747  1374    516    770   1790   1815   1636   5884   1964   2136   3776   1700  44540   5396   2452   2452   3968   1668  44064  0.000168  0.003163   0.001707  0.226101  0.174574\n",
      "3         24      0  36608  15008   5848   2536      0  36928  14816   5904   2352  10200   5548  1418    755   1245   2171   2813   1757  10125   6559  1619    946   1374   2339   3203   1869   5484   1004   1868   2856   1024  47764   5596   1644   1928   2896   1032  46904  0.000165  0.002859   0.002583  0.106464  0.139105\n",
      "4         27      0  39680  12896   5756   1668      0  39040  13824   5388   1748  10696   3068  1767   1351   1487   2355   4923   1511  10520   4089  1542   1214   1581   2210   5483   1651   2884    272    832    936    696  54380   3376    904   1012   1740    884  52084  0.000854   0.00679   0.010907  0.356016  0.280289\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "595       27      0  14656  12160  15836  17348      0  14720  12000  15584  17696  14232  10497  2692   4330   1100   1950   2850    597  14453  10510  2708   4388   1027   2094   2789    602  10496   3928   3788   2720   1724  37344   9848   3596   3696   2576   1564  38720  0.000111  0.000237   0.001225   0.27153  0.217581\n",
      "596       42      0  28864  17008  10568   3560      0  29632  17232   9872   3264  20754  12070   687   1194    235   3092   4310    806  21750  12498   707   1187    742   3028   4846    801   1088    340    896    492    360  56824    624    156    712    384    280  57844  0.000808  0.005417   0.004608  0.149144  0.165488\n",
      "597       27      0   5568   6832  20564  27036      0   5568   6800  19908  27724  12321  10189  2143   2519   2780    538   1239    558  12545  10469  2150   2328   2888    537   1193    568  12056   4904   5192   3320   1388  33140  11496   4744   4736   2980   1232  34812  0.000323  0.000436   0.001772  0.254912  0.218868\n",
      "598       39      0  42688  13840   3176    296      0  43008  13888   2820    284  15927   6085  1837   5272   1008   1513   8963    766  14225   6809  1972   4693   1165   1655  10503    608   1656    552    728    812    372  55880   1664    712    624    604    228  56168  0.000385  0.008006   0.001644  0.246604  0.236176\n",
      "599       42      0  32448  18752   6312   2488      0  32896  19312   5488   2304  18456  13931  1333   2636   2162    130   1476    676  18792  15429  1050   2457   1998    130   2388    388   4912   1608   2060    672   1208  49540   3748    864   1668    356    992  52372   0.00129  0.009193   0.011207  0.134524  0.136487\n",
      "\n",
      "[600 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP,\n",
    "                            X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "X_test_list1 = []\n",
    "X_test_onlyGhost_list1 = []\n",
    "MAE_list_t1 = []\n",
    "FINAL_QP_list_t1 = []\n",
    "Y_test_list1 = []\n",
    "\n",
    "X_test_list2 = []\n",
    "X_test_onlyGhost_list2 = []\n",
    "MAE_list_t2 = []\n",
    "FINAL_QP_list_t2 = []\n",
    "Y_test_list2 = []\n",
    "\n",
    "X_test_list3 = []\n",
    "X_test_onlyGhost_list3 = []\n",
    "MAE_list_t3 = []\n",
    "FINAL_QP_list_t3 = []\n",
    "Y_test_list3 = []\n",
    "\n",
    "\n",
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "\n",
    "append_results_to_lists(test_df1, test_df_onlyGhost1, LABEL_t1, MAE_t1, FINAL_QP_t1, X_test_list1, X_test_onlyGhost_list1, MAE_list_t1, FINAL_QP_list_t1, Y_test_list1)\n",
    "append_results_to_lists(test_df2, test_df_onlyGhost2, LABEL_t2, MAE_t2, FINAL_QP_t2, X_test_list2, X_test_onlyGhost_list2, MAE_list_t2, FINAL_QP_list_t2, Y_test_list2)\n",
    "append_results_to_lists(test_df3, test_df_onlyGhost3, LABEL_t3, MAE_t3, FINAL_QP_t3, X_test_list3, X_test_onlyGhost_list3, MAE_list_t3, FINAL_QP_list_t3, Y_test_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各リストの長さを確認\n",
    "# print(len(X_train_list))\n",
    "# print((X_train_onlyGhost_list[0])[0])\n",
    "# print((MAE_list[0])[0])\n",
    "# print((FINAL_QP_list[0])[0])\n",
    "# print((Y_train_list[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 8]\n",
      "Test indices: [7]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9531    0.6100    0.7439       300\n",
      "           1     0.7132    0.9700    0.8220       300\n",
      "\n",
      "    accuracy                         0.7900       600\n",
      "   macro avg     0.8332    0.7900    0.7830       600\n",
      "weighted avg     0.8332    0.7900    0.7830       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9607    0.5700    0.7155       300\n",
      "           1     0.6943    0.9767    0.8116       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.8275    0.7733    0.7636       600\n",
      "weighted avg     0.8275    0.7733    0.7636       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7946    0.8767    0.8336       300\n",
      "           1     0.8625    0.7733    0.8155       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8285    0.8250    0.8245       600\n",
      "weighted avg     0.8285    0.8250    0.8245       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7416    0.8800    0.8049       300\n",
      "           1     0.8525    0.6933    0.7647       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7970    0.7867    0.7848       600\n",
      "weighted avg     0.7970    0.7867    0.7848       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6538    0.7933    0.7169       300\n",
      "           1     0.7373    0.5800    0.6493       300\n",
      "\n",
      "    accuracy                         0.6867       600\n",
      "   macro avg     0.6956    0.6867    0.6831       600\n",
      "weighted avg     0.6956    0.6867    0.6831       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6508    0.8200    0.7257       300\n",
      "           1     0.7568    0.5600    0.6437       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.7038    0.6900    0.6847       600\n",
      "weighted avg     0.7038    0.6900    0.6847       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8]\n",
      "Test indices: [1]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9783    0.6000    0.7438       300\n",
      "           1     0.7115    0.9867    0.8268       300\n",
      "\n",
      "    accuracy                         0.7933       600\n",
      "   macro avg     0.8449    0.7933    0.7853       600\n",
      "weighted avg     0.8449    0.7933    0.7853       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9752    0.5233    0.6811       300\n",
      "           1     0.6743    0.9867    0.8011       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.8247    0.7550    0.7411       600\n",
      "weighted avg     0.8247    0.7550    0.7411       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7962    0.8333    0.8143       300\n",
      "           1     0.8252    0.7867    0.8055       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8107    0.8100    0.8099       600\n",
      "weighted avg     0.8107    0.8100    0.8099       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7275    0.8900    0.8006       300\n",
      "           1     0.8584    0.6667    0.7505       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7929    0.7783    0.7755       600\n",
      "weighted avg     0.7929    0.7783    0.7755       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6727    0.7467    0.7077       300\n",
      "           1     0.7154    0.6367    0.6737       300\n",
      "\n",
      "    accuracy                         0.6917       600\n",
      "   macro avg     0.6940    0.6917    0.6907       600\n",
      "weighted avg     0.6940    0.6917    0.6907       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6603    0.8033    0.7248       300\n",
      "           1     0.7489    0.5867    0.6579       300\n",
      "\n",
      "    accuracy                         0.6950       600\n",
      "   macro avg     0.7046    0.6950    0.6914       600\n",
      "weighted avg     0.7046    0.6950    0.6914       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8]\n",
      "Test indices: [5]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9733    0.6067    0.7474       300\n",
      "           1     0.7143    0.9833    0.8275       300\n",
      "\n",
      "    accuracy                         0.7950       600\n",
      "   macro avg     0.8438    0.7950    0.7875       600\n",
      "weighted avg     0.8438    0.7950    0.7875       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9583    0.5367    0.6880       300\n",
      "           1     0.6782    0.9767    0.8005       300\n",
      "\n",
      "    accuracy                         0.7567       600\n",
      "   macro avg     0.8183    0.7567    0.7443       600\n",
      "weighted avg     0.8183    0.7567    0.7443       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7472    0.8967    0.8152       300\n",
      "           1     0.8708    0.6967    0.7741       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8090    0.7967    0.7946       600\n",
      "weighted avg     0.8090    0.7967    0.7946       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6838    0.8867    0.7721       300\n",
      "           1     0.8389    0.5900    0.6928       300\n",
      "\n",
      "    accuracy                         0.7383       600\n",
      "   macro avg     0.7613    0.7383    0.7324       600\n",
      "weighted avg     0.7613    0.7383    0.7324       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6340    0.7967    0.7061       300\n",
      "           1     0.7265    0.5400    0.6195       300\n",
      "\n",
      "    accuracy                         0.6683       600\n",
      "   macro avg     0.6802    0.6683    0.6628       600\n",
      "weighted avg     0.6802    0.6683    0.6628       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6366    0.8467    0.7268       300\n",
      "           1     0.7711    0.5167    0.6188       300\n",
      "\n",
      "    accuracy                         0.6817       600\n",
      "   macro avg     0.7039    0.6817    0.6728       600\n",
      "weighted avg     0.7039    0.6817    0.6728       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8]\n",
      "Test indices: [0]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9565    0.5867    0.7273       300\n",
      "           1     0.7019    0.9733    0.8156       300\n",
      "\n",
      "    accuracy                         0.7800       600\n",
      "   macro avg     0.8292    0.7800    0.7715       600\n",
      "weighted avg     0.8292    0.7800    0.7715       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9639    0.5333    0.6867       300\n",
      "           1     0.6774    0.9800    0.8011       300\n",
      "\n",
      "    accuracy                         0.7567       600\n",
      "   macro avg     0.8206    0.7567    0.7439       600\n",
      "weighted avg     0.8206    0.7567    0.7439       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8667    0.8320       300\n",
      "           1     0.8545    0.7833    0.8174       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8273    0.8250    0.8247       600\n",
      "weighted avg     0.8273    0.8250    0.8247       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7402    0.8833    0.8055       300\n",
      "           1     0.8554    0.6900    0.7638       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7978    0.7867    0.7847       600\n",
      "weighted avg     0.7978    0.7867    0.7847       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6755    0.7633    0.7167       300\n",
      "           1     0.7280    0.6333    0.6774       300\n",
      "\n",
      "    accuracy                         0.6983       600\n",
      "   macro avg     0.7017    0.6983    0.6971       600\n",
      "weighted avg     0.7017    0.6983    0.6971       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6350    0.8233    0.7170       300\n",
      "           1     0.7488    0.5267    0.6184       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6919    0.6750    0.6677       600\n",
      "weighted avg     0.6919    0.6750    0.6677       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [8]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9793    0.6300    0.7667       300\n",
      "           1     0.7273    0.9867    0.8373       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8533    0.8083    0.8020       600\n",
      "weighted avg     0.8533    0.8083    0.8020       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9636    0.5300    0.6839       300\n",
      "           1     0.6759    0.9800    0.8000       300\n",
      "\n",
      "    accuracy                         0.7550       600\n",
      "   macro avg     0.8197    0.7550    0.7419       600\n",
      "weighted avg     0.8197    0.7550    0.7419       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7795    0.8367    0.8071       300\n",
      "           1     0.8237    0.7633    0.7924       300\n",
      "\n",
      "    accuracy                         0.8000       600\n",
      "   macro avg     0.8016    0.8000    0.7997       600\n",
      "weighted avg     0.8016    0.8000    0.7997       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7361    0.8833    0.8030       300\n",
      "           1     0.8542    0.6833    0.7593       300\n",
      "\n",
      "    accuracy                         0.7833       600\n",
      "   macro avg     0.7951    0.7833    0.7811       600\n",
      "weighted avg     0.7951    0.7833    0.7811       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6528    0.7333    0.6907       300\n",
      "           1     0.6958    0.6100    0.6501       300\n",
      "\n",
      "    accuracy                         0.6717       600\n",
      "   macro avg     0.6743    0.6717    0.6704       600\n",
      "weighted avg     0.6743    0.6717    0.6704       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6309    0.8433    0.7218       300\n",
      "           1     0.7638    0.5067    0.6092       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6974    0.6750    0.6655       600\n",
      "weighted avg     0.6974    0.6750    0.6655       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8]\n",
      "Test indices: [2]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9771    0.5700    0.7200       300\n",
      "           1     0.6965    0.9867    0.8166       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.8368    0.7783    0.7683       600\n",
      "weighted avg     0.8368    0.7783    0.7683       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9701    0.5400    0.6938       300\n",
      "           1     0.6813    0.9833    0.8049       300\n",
      "\n",
      "    accuracy                         0.7617       600\n",
      "   macro avg     0.8257    0.7617    0.7494       600\n",
      "weighted avg     0.8257    0.7617    0.7494       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7969    0.8500    0.8226       300\n",
      "           1     0.8393    0.7833    0.8103       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8181    0.8167    0.8165       600\n",
      "weighted avg     0.8181    0.8167    0.8165       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7239    0.9000    0.8024       300\n",
      "           1     0.8678    0.6567    0.7476       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7959    0.7783    0.7750       600\n",
      "weighted avg     0.7959    0.7783    0.7750       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6626    0.7267    0.6932       300\n",
      "           1     0.6974    0.6300    0.6620       300\n",
      "\n",
      "    accuracy                         0.6783       600\n",
      "   macro avg     0.6800    0.6783    0.6776       600\n",
      "weighted avg     0.6800    0.6783    0.6776       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6570    0.8300    0.7334       300\n",
      "           1     0.7692    0.5667    0.6526       300\n",
      "\n",
      "    accuracy                         0.6983       600\n",
      "   macro avg     0.7131    0.6983    0.6930       600\n",
      "weighted avg     0.7131    0.6983    0.6930       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 5 6 7 8]\n",
      "Test indices: [4]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9503    0.5733    0.7152       300\n",
      "           1     0.6945    0.9700    0.8095       300\n",
      "\n",
      "    accuracy                         0.7717       600\n",
      "   macro avg     0.8224    0.7717    0.7623       600\n",
      "weighted avg     0.8224    0.7717    0.7623       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9679    0.5033    0.6623       300\n",
      "           1     0.6644    0.9833    0.7930       300\n",
      "\n",
      "    accuracy                         0.7433       600\n",
      "   macro avg     0.8162    0.7433    0.7276       600\n",
      "weighted avg     0.8162    0.7433    0.7276       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7898    0.8767    0.8310       300\n",
      "           1     0.8614    0.7667    0.8113       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8256    0.8217    0.8211       600\n",
      "weighted avg     0.8256    0.8217    0.8211       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7579    0.8767    0.8130       300\n",
      "           1     0.8538    0.7200    0.7812       300\n",
      "\n",
      "    accuracy                         0.7983       600\n",
      "   macro avg     0.8058    0.7983    0.7971       600\n",
      "weighted avg     0.8058    0.7983    0.7971       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6601    0.7767    0.7136       300\n",
      "           1     0.7287    0.6000    0.6581       300\n",
      "\n",
      "    accuracy                         0.6883       600\n",
      "   macro avg     0.6944    0.6883    0.6859       600\n",
      "weighted avg     0.6944    0.6883    0.6859       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6575    0.7933    0.7190       300\n",
      "           1     0.7395    0.5867    0.6543       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.6985    0.6900    0.6867       600\n",
      "weighted avg     0.6985    0.6900    0.6867       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 4 5 6 7 8]\n",
      "Test indices: [3]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9585    0.6167    0.7505       300\n",
      "           1     0.7174    0.9733    0.8260       300\n",
      "\n",
      "    accuracy                         0.7950       600\n",
      "   macro avg     0.8380    0.7950    0.7883       600\n",
      "weighted avg     0.8380    0.7950    0.7883       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9699    0.5367    0.6910       300\n",
      "           1     0.6797    0.9833    0.8038       300\n",
      "\n",
      "    accuracy                         0.7600       600\n",
      "   macro avg     0.8248    0.7600    0.7474       600\n",
      "weighted avg     0.8248    0.7600    0.7474       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7964    0.8733    0.8331       300\n",
      "           1     0.8598    0.7767    0.8161       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8281    0.8250    0.8246       600\n",
      "weighted avg     0.8281    0.8250    0.8246       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7514    0.8767    0.8092       300\n",
      "           1     0.8520    0.7100    0.7745       300\n",
      "\n",
      "    accuracy                         0.7933       600\n",
      "   macro avg     0.8017    0.7933    0.7919       600\n",
      "weighted avg     0.8017    0.7933    0.7919       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6629    0.7733    0.7138       300\n",
      "           1     0.7280    0.6067    0.6618       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.6954    0.6900    0.6878       600\n",
      "weighted avg     0.6954    0.6900    0.6878       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6401    0.8300    0.7228       300\n",
      "           1     0.7583    0.5333    0.6262       300\n",
      "\n",
      "    accuracy                         0.6817       600\n",
      "   macro avg     0.6992    0.6817    0.6745       600\n",
      "weighted avg     0.6992    0.6817    0.6745       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 3 4 5 7 8]\n",
      "Test indices: [6]\n",
      "0.86\n",
      "0.5116666666666667\n",
      "0.5766666666666667\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9615    0.5833    0.7261       300\n",
      "           1     0.7010    0.9767    0.8162       300\n",
      "\n",
      "    accuracy                         0.7800       600\n",
      "   macro avg     0.8312    0.7800    0.7711       600\n",
      "weighted avg     0.8312    0.7800    0.7711       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9673    0.4933    0.6534       300\n",
      "           1     0.6600    0.9833    0.7898       300\n",
      "\n",
      "    accuracy                         0.7383       600\n",
      "   macro avg     0.8136    0.7383    0.7216       600\n",
      "weighted avg     0.8136    0.7383    0.7216       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7844    0.8733    0.8265       300\n",
      "           1     0.8571    0.7600    0.8057       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8208    0.8167    0.8161       600\n",
      "weighted avg     0.8208    0.8167    0.8161       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7341    0.8833    0.8018       300\n",
      "           1     0.8536    0.6800    0.7570       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7938    0.7817    0.7794       600\n",
      "weighted avg     0.7938    0.7817    0.7794       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6611    0.7933    0.7212       300\n",
      "           1     0.7417    0.5933    0.6593       300\n",
      "\n",
      "    accuracy                         0.6933       600\n",
      "   macro avg     0.7014    0.6933    0.6902       600\n",
      "weighted avg     0.7014    0.6933    0.6902       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6241    0.8633    0.7245       300\n",
      "           1     0.7784    0.4800    0.5938       300\n",
      "\n",
      "    accuracy                         0.6717       600\n",
      "   macro avg     0.7012    0.6717    0.6591       600\n",
      "weighted avg     0.7012    0.6717    0.6591       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.9133    0.8671       300\n",
      "           1     0.9030    0.8067    0.8521       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8641    0.8600    0.8596       600\n",
      "weighted avg     0.8641    0.8600    0.8596       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5176    0.3433    0.4128       300\n",
      "           1     0.5087    0.6800    0.5820       300\n",
      "\n",
      "    accuracy                         0.5117       600\n",
      "   macro avg     0.5132    0.5117    0.4974       600\n",
      "weighted avg     0.5132    0.5117    0.4974       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6825    0.2867    0.4038       300\n",
      "           1     0.5485    0.8667    0.6718       300\n",
      "\n",
      "    accuracy                         0.5767       600\n",
      "   macro avg     0.6155    0.5767    0.5378       600\n",
      "weighted avg     0.6155    0.5767    0.5378       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "kfold = KFold(n_splits=9, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# results = pd.DataFrame(columns=['C_RBF_LQP1','Score_RBF_LQP1', 'tnr_rbf_lqp1', 'tpr_rbf_lqp1',\n",
    "#                                 'C_RBF_SQP','Score_RBF_SQP', 'tnr_rbf_sqp', 'tpr_rbf_sqp',\n",
    "#                                 'C_RBF_LQP2','Score_RBF_LQP2', 'tnr_rbf_lqp2', 'tpr_rbf_lqp2',\n",
    "#                                 'C_LINEAR_LQP1','Score_LINEAR_LQP1', 'tnr_linear_lqp1', 'tpr_linear_lqp1',\n",
    "#                                 'C_LINEAR_SQP','Score_LINEAR_SQP', 'tnr_linear_sqp', 'tpr_linear_sqp',\n",
    "#                                 'C_LINEAR_LQP2','Score_LINEAR_LQP2', 'tnr_linear_lqp2', 'tpr_linear_lqp2',\n",
    "                                \n",
    "#                                 'C_RBF_OG_LQP1','Score_RBF_OG_LQP1', 'tnr_og_rbf_lqp1', 'tpr_og_rbf_lqp1',\n",
    "#                                 'C_RBF_OG_SQP','Score_RBF_OG_SQP', 'tnr_og_rbf_sqp', 'tpr_og_rbf_sqp',\n",
    "#                                 'C_RBF_OG_LQP2','Score_RBF_OG_LQP2', 'tnr_og_rbf_lqp2', 'tpr_og_rbf_lqp2',\n",
    "#                                 'C_LINEAR_OG_LQP1','Score_LINEAR_OG_LQP1', 'tnr_og_linear_lqp1', 'tpr_og_linear_lqp1',\n",
    "#                                 'C_LINEAR_OG_SQP','Score_LINEAR_OG_SQP', 'tnr_og_linear_sqp', 'tpr_og_linear_sqp',\n",
    "#                                 'C_LINEAR_OG_LQP2','Score_LINEAR_OG_LQP2', 'tnr_og_linear_lqp2', 'tpr_og_linear_lqp2',\n",
    "                                \n",
    "#                                 'Threshold_LQP1', 'LQP1_old', 'tnr_old_lqp1', 'tpr_old_lqp1',\n",
    "#                                 'Threshold_SQP', 'SQP_old', 'tnr_old_sqp', 'tpr_old_sqp',\n",
    "#                                 'Threshold_LQP2', 'LQP2_old', 'tnr_old_lqp2', 'tpr_old_lqp2'])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['C_RBF_LQP1','Score_RBF_LQP1', 'tnr_rbf_lqp1', 'tpr_rbf_lqp1',\n",
    "                                'C_RBF_SQP','Score_RBF_SQP', 'tnr_rbf_sqp', 'tpr_rbf_sqp',\n",
    "                                'C_RBF_LQP2','Score_RBF_LQP2', 'tnr_rbf_lqp2', 'tpr_rbf_lqp2',\n",
    "                                'C_LINEAR_LQP1','Score_LINEAR_LQP1', 'tnr_linear_lqp1', 'tpr_linear_lqp1',\n",
    "                                'C_LINEAR_SQP','Score_LINEAR_SQP', 'tnr_linear_sqp', 'tpr_linear_sqp',\n",
    "                                'C_LINEAR_LQP2','Score_LINEAR_LQP2', 'tnr_linear_lqp2', 'tpr_linear_lqp2',\n",
    "                                                        \n",
    "                                'Threshold_LQP1', 'LQP1_old', 'tnr_old_lqp1', 'tpr_old_lqp1',\n",
    "                                'Threshold_SQP', 'SQP_old', 'tnr_old_sqp', 'tpr_old_sqp',\n",
    "                                'Threshold_LQP2', 'LQP2_old', 'tnr_old_lqp2', 'tpr_old_lqp2'])\n",
    "\n",
    "    \n",
    "X_index = np.arange(9)  # インデックスとして0から8までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    X_train_OG = [item for data in train_data_OG for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    X_val_OG = [item for data in val_data_OG for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    # print(len(Y_train))\n",
    "    # print(len(Y_val))\n",
    "    \n",
    "    test_data1 = [item for data in X_test_list1 for item in data]\n",
    "    test_data_OG1 = [item for data in X_test_onlyGhost_list1 for item in data]\n",
    "    test_label1 = [item for data in Y_test_list1 for item in data]\n",
    "    MAE_data1 = [item for data in MAE_list_t1 for item in data]\n",
    "    FINAL_QP_data1 = [item for data in FINAL_QP_list_t1 for item in data]\n",
    "    \n",
    "    test_data2 = [item for data in X_test_list2 for item in data]\n",
    "    test_data_OG2 = [item for data in X_test_onlyGhost_list2 for item in data]\n",
    "    test_label2 = [item for data in Y_test_list2 for item in data]\n",
    "    MAE_data2 = [item for data in MAE_list_t2 for item in data]\n",
    "    FINAL_QP_data2 = [item for data in FINAL_QP_list_t2 for item in data]\n",
    "    \n",
    "    test_data3 = [item for data in X_test_list3 for item in data]\n",
    "    test_data_OG3 = [item for data in X_test_onlyGhost_list3 for item in data]\n",
    "    test_label3 = [item for data in Y_test_list3 for item in data]\n",
    "    MAE_data3 = [item for data in MAE_list_t3 for item in data]\n",
    "    FINAL_QP_data3 = [item for data in FINAL_QP_list_t3 for item in data]\n",
    "    \n",
    "    # print(len(MAE_data1))\n",
    "    # print(len(MAE_data2))\n",
    "    # print(len(MAE_data3))\n",
    "    \n",
    "                                                                                   \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    sameQP_best_threshold = 0\n",
    "    sameQP_best_accuracy = 0\n",
    "    sameQP_best_predicted_labels = []\n",
    "    sameQP_best_ground_truth_labels = []\n",
    "    \n",
    "    largeQP_best_threshold = 0\n",
    "    largeQP_best_accuracy = 0\n",
    "    largeQP_best_predicted_labels = []\n",
    "    largeQP_best_ground_truth_labels = []\n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_old = np.array([is_double_compressed(MAE_data1[i], FINAL_QP_data1[i], threshold) for i in range(600)])\n",
    "        predicted_labels = test_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label1)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_sameQP_old = np.array([is_double_compressed(MAE_data2[i], FINAL_QP_data2[i], threshold) for i in range(600)])\n",
    "        same_predicted_labels = test_sameQP_old.astype(int)\n",
    "        same_ground_truth_labels = np.array(test_label2)\n",
    "        same_accuracy = np.sum(same_ground_truth_labels == same_predicted_labels) / len(same_ground_truth_labels)\n",
    "    \n",
    "        if same_accuracy > sameQP_best_accuracy:\n",
    "            sameQP_best_accuracy = same_accuracy\n",
    "            sameQP_best_threshold = threshold\n",
    "            sameQP_best_predicted_labels = same_predicted_labels\n",
    "            sameQP_best_ground_truth_labels = same_ground_truth_labels\n",
    "                        \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_largeQP_old = np.array([is_double_compressed(MAE_data3[i], FINAL_QP_data3[i], threshold) for i in range(600)])\n",
    "        large_predicted_labels = test_largeQP_old.astype(int)\n",
    "        large_ground_truth_labels = np.array(test_label3)\n",
    "        large_accuracy = np.sum(large_ground_truth_labels == large_predicted_labels) / len(large_ground_truth_labels)\n",
    "    \n",
    "        if large_accuracy > largeQP_best_accuracy:\n",
    "            largeQP_best_accuracy = large_accuracy\n",
    "            largeQP_best_threshold = threshold\n",
    "            largeQP_best_predicted_labels = large_predicted_labels\n",
    "            largeQP_best_ground_truth_labels = large_ground_truth_labels       \n",
    "            \n",
    "            \n",
    "    print(best_accuracy)\n",
    "    print(sameQP_best_accuracy)\n",
    "    print(largeQP_best_accuracy)\n",
    "            \n",
    "            \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "\n",
    "    # テストデータで評価    \n",
    "    predictions_RBF = best_svm_model_RBF.predict(test_data1)\n",
    "    # predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    accuracy_RBF = accuracy_score(test_label1, predictions_RBF)\n",
    "    report_RBF = classification_report(test_label1, predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label1, predictions_RBF)\n",
    "    tnr_rbf_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'report_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    predictions_LINEAR = best_svm_model_LINEAR.predict(test_data1)\n",
    "    # predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    accuracy_LINEAR = accuracy_score(test_label1, predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label1, predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label1, predictions_LINEAR)\n",
    "    tnr_linear_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'report_LINEAR:\\n{report_LINEAR}')\n",
    "    \n",
    "    same_predictions_RBF = best_svm_model_RBF.predict(test_data2)\n",
    "    # same_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_sameQP)\n",
    "    same_accuracy_RBF = accuracy_score(test_label2, same_predictions_RBF)\n",
    "    same_report_RBF = classification_report(test_label2, same_predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label2, same_predictions_RBF)\n",
    "    tnr_rbf_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'same_report_RBF:\\n{same_report_RBF}')\n",
    "    \n",
    "    same_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data2)\n",
    "    # same_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_sameQP)\n",
    "    same_accuracy_LINEAR = accuracy_score(test_label2, same_predictions_LINEAR)\n",
    "    same_report_LINEAR = classification_report(test_label2, same_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label2, same_predictions_LINEAR)\n",
    "    tnr_linear_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'same_report_LINEAR:\\n{same_report_LINEAR}')\n",
    "    \n",
    "    large_predictions_RBF = best_svm_model_RBF.predict(test_data3)\n",
    "    # large_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_largeQP)\n",
    "    large_accuracy_RBF = accuracy_score(test_label3, large_predictions_RBF)\n",
    "    large_report_RBF = classification_report(test_label3, large_predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label3, large_predictions_RBF)\n",
    "    tnr_rbf_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'large_report_RBF:\\n{large_report_RBF}')\n",
    "    \n",
    "    large_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data3)\n",
    "    # large_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_largeQP)\n",
    "    large_accuracy_LINEAR = accuracy_score(test_label3, large_predictions_LINEAR)\n",
    "    large_report_LINEAR = classification_report(test_label3, large_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label3, large_predictions_LINEAR)\n",
    "    tnr_linear_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'large_report_LINEAR:\\n{large_report_LINEAR}')\n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_old}')\n",
    "    \n",
    "    test_sameQP_old = classification_report(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels)\n",
    "    tnr_old_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_sameQP_old}')\n",
    "    \n",
    "    test_largeQP_old = classification_report(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels)\n",
    "    tnr_old_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_largeQP_old}')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row ={'C_RBF_LQP1':best_c_value_RBF,'Score_RBF_LQP1': accuracy_RBF, 'tnr_rbf_lqp1':tnr_rbf_lqp1, 'tpr_rbf_lqp1':tpr_rbf_lqp1,\n",
    "                'C_RBF_SQP': best_c_value_RBF, 'Score_RBF_SQP': same_accuracy_RBF, 'tnr_rbf_sqp':tnr_rbf_sqp, 'tpr_rbf_sqp':tpr_rbf_sqp,\n",
    "                'C_RBF_LQP2': best_c_value_RBF,'Score_RBF_LQP2': large_accuracy_RBF, 'tnr_rbf_lqp2':tnr_rbf_lqp2, 'tpr_rbf_lqp2':tpr_rbf_lqp2,\n",
    "                                    \n",
    "                'C_LINEAR_LQP1': best_c_value_LINEAR,'Score_LINEAR_LQP1':accuracy_LINEAR, 'tnr_linear_lqp1':tnr_linear_lqp1, 'tpr_linear_lqp1':tpr_linear_lqp1,\n",
    "                'C_LINEAR_SQP': best_c_value_LINEAR,'Score_LINEAR_SQP':same_accuracy_LINEAR, 'tnr_linear_sqp':tnr_linear_sqp, 'tpr_linear_sqp':tpr_linear_sqp,\n",
    "                'C_LINEAR_LQP2': best_c_value_LINEAR,'Score_LINEAR_LQP2':large_accuracy_LINEAR, 'tnr_linear_lqp2':tnr_linear_lqp2, 'tpr_linear_lqp2':tpr_linear_lqp2,\n",
    "                                                        \n",
    "                'Threshold_LQP1':best_threshold, 'LQP1_old':best_accuracy, 'tnr_old_lqp1':tnr_old_lqp1, 'tpr_old_lqp1':tpr_old_lqp1,\n",
    "                'Threshold_SQP':sameQP_best_threshold, 'SQP_old':sameQP_best_accuracy, 'tnr_old_sqp':tnr_old_sqp, 'tpr_old_sqp':tpr_old_sqp,\n",
    "                'Threshold_LQP2':largeQP_best_threshold, 'LQP2_old':largeQP_best_accuracy, 'tnr_old_lqp2':tnr_old_lqp2, 'tpr_old_lqp2':tpr_old_lqp2}\n",
    "\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0     RBF_LQP1        59.74        97.85               78.80                1.14           80.83           77.17\n",
      "1      RBF_SQP        86.48        76.56               81.52                1.08           82.50           79.67\n",
      "2     RBF_LQP2        76.70        60.33               68.52                1.02           69.83           66.83\n",
      "3  LINEAR_LQP1        52.96        98.15               75.56                1.01           77.33           73.83\n",
      "4   LINEAR_SQP        88.44        67.67               78.06                1.72           79.83           73.83\n",
      "5  LINEAR_LQP2        82.81        54.04               68.43                0.95           69.83           67.17\n",
      "6     OLD_LQP1        91.33        80.67               86.00                0.00           86.00           86.00\n",
      "7      OLD_SQP        34.33        68.00               51.17                0.00           51.17           51.17\n",
      "8     OLD_LQP2        28.67        86.67               57.67                0.00           57.67           57.67\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF_LQP1', 'RBF_SQP', 'RBF_LQP2', 'LINEAR_LQP1', 'LINEAR_SQP', 'LINEAR_LQP2', 'OLD_LQP1', 'OLD_SQP', 'OLD_LQP2'],\n",
    "    'Average TNR': [round(results['tnr_rbf_lqp1'].mean() * 100, 2), round(results['tnr_rbf_sqp'].mean() * 100, 2), round(results['tnr_rbf_lqp2'].mean() * 100, 2), round(results['tnr_linear_lqp1'].mean() * 100, 2), round(results['tnr_linear_sqp'].mean() * 100, 2), round(results['tnr_linear_lqp2'].mean() * 100, 2),round(results['tnr_old_lqp1'].mean() * 100, 2), round(results['tnr_old_sqp'].mean() * 100, 2), round(results['tnr_old_lqp2'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf_lqp1'].mean() * 100, 2), round(results['tpr_rbf_sqp'].mean() * 100, 2), round(results['tpr_rbf_lqp2'].mean() * 100, 2), round(results['tpr_linear_lqp1'].mean() * 100, 2), round(results['tpr_linear_sqp'].mean() * 100, 2), round(results['tpr_linear_lqp2'].mean() * 100, 2),round(results['tpr_old_lqp1'].mean() * 100, 2), round(results['tpr_old_sqp'].mean() * 100, 2), round(results['tpr_old_lqp2'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF_LQP1'].mean() * 100, 2), round(results['Score_RBF_SQP'].mean() * 100, 2), round(results['Score_RBF_LQP2'].mean() * 100, 2), round(results['Score_LINEAR_LQP1'].mean() * 100, 2), round(results['Score_LINEAR_SQP'].mean() * 100, 2), round(results['Score_LINEAR_LQP2'].mean() * 100, 2),round(results['LQP1_old'].mean() * 100, 2), round(results['SQP_old'].mean() * 100, 2), round(results['LQP2_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF_LQP1'].std() * 100, 2), round(results['Score_RBF_SQP'].std() * 100, 2), round(results['Score_RBF_LQP2'].std() * 100, 2), round(results['Score_LINEAR_LQP1'].std() * 100, 2), round(results['Score_LINEAR_SQP'].std() * 100, 2), round(results['Score_LINEAR_LQP2'].std() * 100, 2),round(results['LQP1_old'].std() * 100, 2), round(results['SQP_old'].std() * 100, 2), round(results['LQP2_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF_LQP1'].max() * 100, 2), round(results['Score_RBF_SQP'].max() * 100, 2), round(results['Score_RBF_LQP2'].max() * 100, 2), round(results['Score_LINEAR_LQP1'].max() * 100, 2), round(results['Score_LINEAR_SQP'].max() * 100, 2), round(results['Score_LINEAR_LQP2'].max() * 100, 2),round(results['LQP1_old'].max() * 100, 2), round(results['SQP_old'].max() * 100, 2), round(results['LQP2_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF_LQP1'].min() * 100, 2), round(results['Score_RBF_SQP'].min() * 100, 2), round(results['Score_RBF_LQP2'].min() * 100, 2), round(results['Score_LINEAR_LQP1'].min() * 100, 2), round(results['Score_LINEAR_SQP'].min() * 100, 2), round(results['Score_LINEAR_LQP2'].min() * 100, 2),round(results['LQP1_old'].min() * 100, 2), round(results['SQP_old'].min() * 100, 2), round(results['LQP2_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10\n",
      "1    100\n",
      "2    100\n",
      "3     10\n",
      "4    100\n",
      "5    100\n",
      "6     10\n",
      "7     10\n",
      "8     10\n",
      "Name: C_RBF_LQP1, dtype: object\n",
      "0      10\n",
      "1      10\n",
      "2    4000\n",
      "3     100\n",
      "4    1000\n",
      "5      10\n",
      "6      10\n",
      "7     100\n",
      "8    1000\n",
      "Name: C_LINEAR_LQP1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF_LQP1'])\n",
    "print(results['C_LINEAR_LQP1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
