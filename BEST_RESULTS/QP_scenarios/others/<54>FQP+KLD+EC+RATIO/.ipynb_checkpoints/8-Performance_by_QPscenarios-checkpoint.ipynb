{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['7', '190', '290', '64', '126', '271', '6', '252', '17', '99', '58', '138', '260', '268', '52', '207', '284', '74', '270', '36', '278', '65', '300', '235', '139', '145', '258', '200', '16', '289'], ['132', '277', '18', '154', '142', '170', '188', '149', '177', '75', '150', '288', '29', '179', '256', '31', '233', '114', '194', '48', '205', '193', '237', '47', '33', '81', '166', '38', '259', '27'], ['120', '15', '281', '171', '291', '69', '287', '249', '123', '124', '155', '164', '266', '25', '70', '285', '41', '231', '55', '160', '12', '230', '183', '111', '71', '202', '107', '112', '238', '117'], ['98', '243', '261', '39', '214', '251', '224', '191', '269', '21', '76', '274', '109', '217', '94', '57', '280', '108', '157', '137', '62', '130', '292', '11', '141', '176', '167', '162', '185', '211'], ['119', '35', '83', '104', '79', '152', '151', '1', '110', '279', '236', '9', '53', '210', '244', '72', '50', '144', '201', '68', '178', '135', '66', '165', '127', '42', '239', '153', '37', '293'], ['257', '116', '189', '105', '262', '294', '102', '186', '254', '222', '84', '23', '14', '26', '133', '299', '80', '121', '91', '54', '219', '265', '93', '87', '240', '5', '195', '245', '136', '34'], ['273', '228', '221', '248', '45', '173', '43', '89', '131', '59', '276', '10', '227', '129', '184', '96', '128', '40', '122', '203', '95', '13', '158', '78', '2', '115', '180', '226', '156', '134'], ['161', '247', '264', '159', '296', '90', '267', '106', '216', '246', '209', '242', '212', '253', '229', '8', '218', '187', '4', '182', '241', '24', '46', '67', '223', '147', '215', '198', '297', '118'], ['204', '175', '51', '82', '172', '77', '19', '169', '234', '88', '174', '275', '73', '44', '225', '255', '103', '146', '220', '168', '208', '181', '20', '272', '143', '61', '163', '199', '86', '206'], ['283', '97', '28', '49', '263', '113', '101', '3', '250', '32', '286', '22', '140', '192', '85', '60', '125', '197', '100', '63', '56', '232', '92', '148', '282', '295', '213', '30', '196', '298']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print(len(single_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD1': 54, 'QPD3': 54, 'QPD5': 54, 'QPD6': 54, 'QPD8': 54, 'QPD10': 54, 'QPD9': 54, 'QPD4': 54, 'QPD25': 27, 'QPD30': 27, 'QPD15': 27, 'QPD14': 27, 'QPD45': 27, 'QPD20': 27, 'QPD40': 27, 'QPD27': 27, 'QPD13': 18, 'QPD12': 18, 'QPD22': 18, 'QPD24': 18, 'QPD16': 18, 'QPD23': 18, 'QPD29': 18, 'QPD34': 18, 'QPD35': 18, 'QPD11': 18, 'QPD18': 18, 'QPD19': 18, 'QPD26': 18, 'QPD21': 18})\n",
      "\n",
      "double images train by QP1 < QP2:  100\n",
      "\n",
      "double images test by QP1 < QP2:  300\n"
     ]
    }
   ],
   "source": [
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv10))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP1_csv1, second_largeQP1_csv2, second_largeQP1_csv3,\n",
    "    second_largeQP1_csv4, second_largeQP1_csv5, second_largeQP1_csv6,\n",
    "    second_largeQP1_csv7, second_largeQP1_csv8, second_largeQP1_csv9\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1\": [\"_1stQP25_2ndQP24_\", \"_1stQP40_2ndQP39_\"],\n",
    "    \"QPD3\": [\"_1stQP30_2ndQP27_\", \"_1stQP35_2ndQP32_\", \"_1stQP45_2ndQP42_\"],\n",
    "    \"QPD4\": [\"_1stQP20_2ndQP16_\"],\n",
    "    \"QPD5\": [\"_1stQP10_2ndQP5_\", \"_1stQP15_2ndQP10_\", \"_1stQP25_2ndQP20_\", \"_1stQP32_2ndQP27_\", \"_1stQP50_2ndQP45_\"],\n",
    "    \"QPD6\": [\"_1stQP30_2ndQP24_\", \"_1stQP45_2ndQP39_\"],\n",
    "    \"QPD8\": [\"_1stQP32_2ndQP24_\", \"_1stQP35_2ndQP27_\", \"_1stQP40_2ndQP32_\", \"_1stQP50_2ndQP42_\"],\n",
    "    \"QPD9\": [\"_1stQP25_2ndQP16_\"],\n",
    "    \"QPD10\": [\"_1stQP15_2ndQP5_\", \"_1stQP20_2ndQP10_\", \"_1stQP30_2ndQP20_\"],\n",
    "    \"QPD11\": [\"_1stQP35_2ndQP24_\", \"_1stQP50_2ndQP39_\"],\n",
    "    \"QPD12\": [\"_1stQP32_2ndQP20_\"],\n",
    "    \"QPD13\": [\"_1stQP40_2ndQP27_\", \"_1stQP45_2ndQP32_\"],\n",
    "    \"QPD14\": [\"_1stQP30_2ndQP16_\"],\n",
    "    \"QPD15\": [\"_1stQP20_2ndQP5_\", \"_1stQP25_2ndQP10_\", \"_1stQP35_2ndQP20_\"],\n",
    "    \"QPD16\": [\"_1stQP32_2ndQP16_\", \"_1stQP40_2ndQP24_\"],\n",
    "    \"QPD18\": [\"_1stQP45_2ndQP27_\", \"_1stQP50_2ndQP32_\"],\n",
    "    \"QPD19\": [\"_1stQP35_2ndQP16_\"],\n",
    "    \"QPD20\": [\"_1stQP25_2ndQP5_\", \"_1stQP30_2ndQP10_\", \"_1stQP40_2ndQP20_\"],\n",
    "    \"QPD21\": [\"_1stQP45_2ndQP24_\"],\n",
    "    \"QPD22\": [\"_1stQP32_2ndQP10_\"],\n",
    "    \"QPD23\": [\"_1stQP50_2ndQP27_\"],\n",
    "    \"QPD24\": [\"_1stQP40_2ndQP16_\"],\n",
    "    \"QPD25\": [\"_1stQP30_2ndQP5_\", \"_1stQP35_2ndQP10_\", \"_1stQP45_2ndQP20_\"],\n",
    "    \"QPD26\": [\"_1stQP50_2ndQP24_\"],\n",
    "    \"QPD27\": [\"_1stQP32_2ndQP5_\"],\n",
    "    \"QPD29\": [\"_1stQP45_2ndQP16_\"],\n",
    "    \"QPD30\": [\"_1stQP35_2ndQP5_\", \"_1stQP40_2ndQP10_\", \"_1stQP50_2ndQP20_\"],\n",
    "    \"QPD34\": [\"_1stQP50_2ndQP16_\"],\n",
    "    \"QPD35\": [\"_1stQP40_2ndQP5_\", \"_1stQP45_2ndQP10_\"],\n",
    "    \"QPD40\": [\"_1stQP45_2ndQP5_\", \"_1stQP50_2ndQP10_\"],\n",
    "    \"QPD45\": [\"_1stQP50_2ndQP5_\"]\n",
    "}\n",
    "\n",
    "# Priority QPD lists\n",
    "priority_qpd = {\"QPD1\", \"QPD3\", \"QPD4\", \"QPD5\", \"QPD6\", \"QPD8\", \"QPD9\", \"QPD10\"}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Initialize the selected data list\n",
    "selected_data = [[] for _ in range(len(datasets))]\n",
    "\n",
    "# Step 1: Select 6 items from each priority QPD from each dataset\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    selected_counts = defaultdict(int)\n",
    "    \n",
    "    for item in dataset:\n",
    "        item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        priority_check = any(qpd in priority_qpd for qpd in qpd_lists)\n",
    "        \n",
    "        if priority_check:\n",
    "            if any(selected_counts[qpd] < 6 for qpd in qpd_lists if qpd in priority_qpd):\n",
    "                selected_data[dataset_idx].append(item)\n",
    "                for qpd in qpd_lists:\n",
    "                    if qpd in priority_qpd:\n",
    "                        selected_counts[qpd] += 1\n",
    "                        \n",
    "        if all(selected_counts[qpd] >= 6 for qpd in priority_qpd):\n",
    "            break\n",
    "\n",
    "# Step 2: Select exactly 2 items from each non-priority QPD\n",
    "non_priority_qpd = set(QPD.keys()) - priority_qpd\n",
    "\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    current_selected = selected_data[dataset_idx]\n",
    "    non_priority_counts = defaultdict(int)\n",
    "    remaining_qpd_items = {qpd: [] for qpd in non_priority_qpd}\n",
    "    \n",
    "    for item in dataset:\n",
    "        item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        non_priority_check = any(qpd in non_priority_qpd for qpd in qpd_lists)\n",
    "        \n",
    "        if non_priority_check:\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in non_priority_qpd and non_priority_counts[qpd] < 2:\n",
    "                    remaining_qpd_items[qpd].append(item)\n",
    "                    non_priority_counts[qpd] += 1\n",
    "    \n",
    "    for qpd, items in remaining_qpd_items.items():\n",
    "        current_selected.extend(items[:2])\n",
    "    \n",
    "    selected_data[dataset_idx] = current_selected\n",
    "\n",
    "# Step 3: Select 8 more items from non-priority QPD to make 100 items\n",
    "for dataset_idx, dataset in enumerate(datasets):\n",
    "    random.shuffle(dataset)\n",
    "    current_selected = selected_data[dataset_idx]\n",
    "    extra_items_needed = 100 - len(current_selected)\n",
    "    \n",
    "    if extra_items_needed > 0:\n",
    "        remaining_qpd_items = {qpd: [] for qpd in non_priority_qpd}\n",
    "        non_priority_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            non_priority_check = any(qpd in non_priority_qpd for qpd in qpd_lists)\n",
    "            \n",
    "            if non_priority_check:\n",
    "                for qpd in qpd_lists:\n",
    "                    if qpd in non_priority_qpd and non_priority_counts[qpd] < 3:\n",
    "                        remaining_qpd_items[qpd].append(item)\n",
    "                        non_priority_counts[qpd] += 1\n",
    "        \n",
    "        extra_items_selected = 0\n",
    "        while extra_items_selected < extra_items_needed:\n",
    "            for qpd, items in remaining_qpd_items.items():\n",
    "                if items and extra_items_selected < extra_items_needed:\n",
    "                    current_selected.append(items.pop(0))\n",
    "                    extra_items_selected += 1\n",
    "    \n",
    "    selected_data[dataset_idx] = current_selected[:100]\n",
    "\n",
    "# Check the distribution of QPDs\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = selected_data[0]\n",
    "second_largeQP1_csv2 = selected_data[1]\n",
    "second_largeQP1_csv3 = selected_data[2]\n",
    "second_largeQP1_csv4 = selected_data[3]\n",
    "second_largeQP1_csv5 = selected_data[4]\n",
    "second_largeQP1_csv6 = selected_data[5]\n",
    "second_largeQP1_csv7 = selected_data[6]\n",
    "second_largeQP1_csv8 = selected_data[7]\n",
    "second_largeQP1_csv9 = selected_data[8]\n",
    "print('\\ndouble images train by QP1 < QP2: ', len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 300)\n",
    "print('\\ndouble images test by QP1 < QP2: ', len(second_largeQP1_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution: Counter({'QPD24S': 90, 'QPD5S': 90, 'QPD42S': 90, 'QPD32S': 90, 'QPD45S': 90, 'QPD10S': 90, 'QPD16S': 90, 'QPD27S': 90, 'QPD39S': 90, 'QPD20S': 90})\n",
      "\n",
      "double images sameQP:  100\n",
      "\n",
      "double images test by QP1 < QP2:  300\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_sameQP_csv1, second_sameQP_csv2, second_sameQP_csv3,\n",
    "    second_sameQP_csv4, second_sameQP_csv5, second_sameQP_csv6,\n",
    "    second_sameQP_csv7, second_sameQP_csv8, second_sameQP_csv9,\n",
    "]\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD5S\": [\"_1stQP5_2ndQP5\"],\n",
    "    \"QPD10S\": [\"_1stQP10_2ndQP10\"],\n",
    "    \"QPD16S\": [\"_1stQP16_2ndQP16\"],\n",
    "    \"QPD20S\": [\"_1stQP20_2ndQP20\"],\n",
    "    \"QPD24S\": [\"_1stQP24_2ndQP24\"],\n",
    "    \"QPD27S\": [\"_1stQP27_2ndQP27\"],\n",
    "    \"QPD32S\": [\"_1stQP32_2ndQP32\"],\n",
    "    \"QPD39S\": [\"_1stQP39_2ndQP39\"],\n",
    "    \"QPD42S\": [\"_1stQP42_2ndQP42\"],\n",
    "    \"QPD45S\": [\"_1stQP45_2ndQP45\"],\n",
    "}\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        while len(selected_from_dataset) < 100 and indices:\n",
    "            idx = indices.pop()\n",
    "            item = dataset[idx]\n",
    "            item_str = item[0]  # Assume the first element in the tuple is the string to check\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            if qpd_lists:\n",
    "                if all(selected_counts[qpd] < 100 / len(QPD) for qpd in qpd_lists):\n",
    "                    selected_from_dataset.append(item)\n",
    "                    for qpd in qpd_lists:\n",
    "                        selected_counts[qpd] += 1\n",
    "        \n",
    "        selected_data.append(selected_from_dataset)\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "\n",
    "# Print the distribution of QPD lists in the selected data (optional)\n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\", qpd_distribution)\n",
    "\n",
    "\n",
    "second_sameQP_csv1 = selected_data[0]\n",
    "second_sameQP_csv2 = selected_data[1]\n",
    "second_sameQP_csv3 = selected_data[2]\n",
    "second_sameQP_csv4 = selected_data[3]\n",
    "second_sameQP_csv5 = selected_data[4]\n",
    "second_sameQP_csv6 = selected_data[5]\n",
    "second_sameQP_csv7 = selected_data[6]\n",
    "second_sameQP_csv8 = selected_data[7]\n",
    "second_sameQP_csv9 = selected_data[8]\n",
    "print('\\ndouble images sameQP: ', len(second_sameQP_csv1))\n",
    "\n",
    "print('\\ndouble images test by QP1 < QP2: ', len(second_sameQP_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: 100 items\n",
      "List 2: 100 items\n",
      "List 3: 100 items\n",
      "List 4: 100 items\n",
      "List 5: 100 items\n",
      "List 6: 100 items\n",
      "List 7: 100 items\n",
      "List 8: 100 items\n",
      "List 9: 100 items\n",
      "\n",
      "QPD Distribution:\n",
      " Counter({'QPD2M': 45, 'QPD7M': 45, 'QPD10M': 45, 'QPD17M': 44, 'QPD9M': 42, 'QPD12M': 42, 'QPD4M': 39, 'QPD5M': 39, 'QPD14M': 39, 'QPD24M': 39, 'QPD30M': 39, 'QPD22M': 38, 'QPD27M': 38, 'QPD35M': 38, 'QPD13M': 37, 'QPD20M': 37, 'QPD29M': 37, 'QPD32M': 37, 'QPD1M': 36, 'QPD6M': 36, 'QPD15M': 36, 'QPD19M': 36, 'QPD25M': 36})\n",
      "\n",
      "double images largeQP2:  100\n",
      "\n",
      "double images test by QP1 > QP2:  300\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "# Define the datasets\n",
    "datasets = [\n",
    "    second_largeQP2_csv1, second_largeQP2_csv2, second_largeQP2_csv3,\n",
    "    second_largeQP2_csv4, second_largeQP2_csv5, second_largeQP2_csv6,\n",
    "    second_largeQP2_csv7, second_largeQP2_csv8, second_largeQP2_csv9,\n",
    "]\n",
    "\n",
    "\n",
    "# Define QPD lists\n",
    "QPD = {\n",
    "    \"QPD1M\": [\"_1stQP15_2ndQP16\"],\n",
    "    \"QPD2M\": [\"_1stQP25_2ndQP27\", \"_1stQP30_2ndQP32\", \"_1stQP40_2ndQP42\"],\n",
    "    \"QPD4M\": [\"_1stQP20_2ndQP24\", \"_1stQP35_2ndQP39\"],\n",
    "    \"QPD5M\": [\"_1stQP15_2ndQP20\", \"_1stQP40_2ndQP45\"],\n",
    "    \"QPD6M\": [\"_1stQP10_2ndQP16\"],\n",
    "    \"QPD7M\": [\"_1stQP20_2ndQP27\", \"_1stQP25_2ndQP32\", \"_1stQP32_2ndQP39\", \"_1stQP35_2ndQP42\"],\n",
    "    \"QPD9M\": [\"_1stQP15_2ndQP24\", \"_1stQP30_2ndQP39\"],\n",
    "    \"QPD10M\": [\"_1stQP10_2ndQP20\", \"_1stQP32_2ndQP42\", \"_1stQP35_2ndQP45\"],\n",
    "    \"QPD12M\": [\"_1stQP15_2ndQP27\", \"_1stQP20_2ndQP32\", \"_1stQP30_2ndQP42\"],\n",
    "    \"QPD13M\": [\"_1stQP32_2ndQP45\"],\n",
    "    \"QPD14M\": [\"_1stQP10_2ndQP24\", \"_1stQP25_2ndQP39\"],\n",
    "    \"QPD15M\": [\"_1stQP30_2ndQP45\"],\n",
    "    \"QPD17M\": [\"_1stQP10_2ndQP27\", \"_1stQP15_2ndQP32\", \"_1stQP25_2ndQP42\"],\n",
    "    \"QPD19M\": [\"_1stQP20_2ndQP39\"],\n",
    "    \"QPD20M\": [\"_1stQP25_2ndQP45\"],\n",
    "    \"QPD22M\": [\"_1stQP10_2ndQP32\", \"_1stQP20_2ndQP42\"],\n",
    "    \"QPD24M\": [\"_1stQP15_2ndQP39\"],\n",
    "    \"QPD25M\": [\"_1stQP20_2ndQP45\"],\n",
    "    \"QPD27M\": [\"_1stQP15_2ndQP42\"],\n",
    "    \"QPD29M\": [\"_1stQP10_2ndQP39\"],\n",
    "    \"QPD30M\": [\"_1stQP15_2ndQP45\"],\n",
    "    \"QPD32M\": [\"_1stQP10_2ndQP42\"],\n",
    "    \"QPD35M\": [\"_1stQP10_2ndQP45\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a function to check which QPD lists a string belongs to\n",
    "def check_qpd_lists(s):\n",
    "    qpd_lists = []\n",
    "    for k, v in QPD.items():\n",
    "        if any(qp in s for qp in v):\n",
    "            qpd_lists.append(k)\n",
    "    return qpd_lists\n",
    "\n",
    "# Function to select 100 items from each dataset ensuring QPD distribution\n",
    "def select_items(datasets):\n",
    "    selected_data = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        random.shuffle(dataset)\n",
    "        selected_counts = defaultdict(int)\n",
    "        selected_from_dataset = []\n",
    "        \n",
    "        # Select 4 items from each QPD\n",
    "        for qpd in QPD.keys():\n",
    "            count = 0\n",
    "            for item in dataset:\n",
    "                if count >= 4:\n",
    "                    break\n",
    "                item_str = item[0]\n",
    "                if qpd in check_qpd_lists(item_str) and selected_counts[qpd] < 4:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    selected_counts[qpd] += 1\n",
    "                    count += 1\n",
    "                    dataset.remove(item)\n",
    "        \n",
    "        # Select additional 8 items from QPD to make 100 items\n",
    "        remaining_qpds = list(QPD.keys())\n",
    "        extra_items_needed = 100 - len(selected_from_dataset)\n",
    "        extra_counts = defaultdict(int)\n",
    "        \n",
    "        for item in dataset:\n",
    "            if extra_items_needed <= 0:\n",
    "                break\n",
    "            item_str = item[0]\n",
    "            qpd_lists = check_qpd_lists(item_str)\n",
    "            for qpd in qpd_lists:\n",
    "                if qpd in remaining_qpds and extra_counts[qpd] < 1:\n",
    "                    selected_from_dataset.append(item)\n",
    "                    extra_counts[qpd] += 1\n",
    "                    extra_items_needed -= 1\n",
    "                    dataset.remove(item)\n",
    "                    break\n",
    "        \n",
    "        selected_data.append(selected_from_dataset[:100])\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "# Select 100 items from each dataset\n",
    "selected_data = select_items(datasets)\n",
    "\n",
    "# Verify the selected data length and distribution\n",
    "for i, data_list in enumerate(selected_data):\n",
    "    print(f\"List {i+1}: {len(data_list)} items\")\n",
    "    \n",
    "qpd_distribution = Counter()\n",
    "for data_list in selected_data:\n",
    "    for item in data_list:\n",
    "        item_str = item[0]\n",
    "        qpd_lists = check_qpd_lists(item_str)\n",
    "        for qpd in qpd_lists:\n",
    "            qpd_distribution[qpd] += 1\n",
    "\n",
    "print(\"\\nQPD Distribution:\\n\", qpd_distribution)\n",
    "\n",
    "second_largeQP2_csv1 = selected_data[0]\n",
    "second_largeQP2_csv2 = selected_data[1]\n",
    "second_largeQP2_csv3 = selected_data[2]\n",
    "second_largeQP2_csv4 = selected_data[3]\n",
    "second_largeQP2_csv5 = selected_data[4]\n",
    "second_largeQP2_csv6 = selected_data[5]\n",
    "second_largeQP2_csv7 = selected_data[6]\n",
    "second_largeQP2_csv8 = selected_data[7]\n",
    "second_largeQP2_csv9 = selected_data[8]\n",
    "print('\\ndouble images largeQP2: ', len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 300)\n",
    "print('\\ndouble images test by QP1 > QP2: ', len(second_largeQP2_csv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n",
      "test_csv_largeQP1 600\n",
      "test_csv_sameQP 600\n",
      "test_csv_largeQP2 600\n"
     ]
    }
   ],
   "source": [
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "print(\"train_csv_list: \", len(train_csv_list9))\n",
    "\n",
    "test_csv_largeQP1 = single_csv10 + second_largeQP1_csv10\n",
    "test_csv_sameQP = single_csv10 + second_sameQP_csv10\n",
    "test_csv_largeQP2 = single_csv10 + second_largeQP2_csv10\n",
    "\n",
    "print(\"test_csv_largeQP1\", len(test_csv_largeQP1))\n",
    "print(\"test_csv_sameQP\", len(test_csv_sameQP))\n",
    "print(\"test_csv_largeQP2\", len(test_csv_largeQP2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "\n",
    "test_df1, test_df_onlyGhost1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_csv_largeQP1)\n",
    "test_df2, test_df_onlyGhost2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_csv_sameQP)\n",
    "test_df3, test_df_onlyGhost3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_csv_largeQP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10      0  13120  12880  15828  18172      0  12992  12960  15652  18396  17158   8389  1703   1262   2218    974   1141   1197  16747   9288  1763   1465   2175    941   1075   1210  10104  21556   7664   7592   3024  10060   9728  21648   7936   7780   2932   9976  0.000054  0.001856   0.000269  0.148662  0.079983\n",
      "1         16      0  48896   9424   1224    456      0  48896   9520   1180    404  15499  12850  2389   1250   2927   1461   1227   1791  15862  12925  2310   1280   3038   1422   1109   1712  10408  24724   3440   4116   1392  15920   9828  25300   3468   4040   1100  16264  0.000075  0.000397   0.001059  0.071083  0.049651\n",
      "2         20      0  47104  12704    160     32      0  47360  12464    148     28  17043   9844  2122   1793   2902    896   1312   1440  17777  10005  2155   1809   2870   1120   1200   1488  13756  15180   4044   4900   1312  20808  13104  15988   4196   5200   1248  20264  0.000062  0.000856   0.000953  0.258477  0.136565\n",
      "3         24      0  54016   5952     32      0      0  53888   6080     32      0  16924   5904  2608   1584   2480   1184   2128   1072  17276   5856  2736   1520   2384   1056   2064   1200  15448   6368   3572   4944   2176  27492  15592   6336   3940   4864   2256  27012  0.000025  0.000736   0.000414  0.158917  0.140671\n",
      "4         27      0  56192   3792     16      0      0  56128   3856     16      0  17892   5376  1504   1680   3120    800   2416   1440  17892   5584  1568   1728   3136    800   2352   1504  11776   3088   3840   2928   2672  35696  11824   3040   3808   2784   2608  35936   0.00001  0.000192   0.000097  0.136185  0.119765\n",
      "..       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...       ...       ...        ...       ...       ...\n",
      "595       45      0  46144  10080   3336    440      0  46528  10384   2748    340  20218  11676   208    579    878    233   6435     85  20910  11900   225    844    489    491   6017    213    896    412    688    432    252  57320    768    124    512    448    144  58004  0.001305  0.008092   0.004668  0.188323   0.19127\n",
      "596       45      0  26624  22368   9008   2000      0  27520  22224   8604   1652  17937  19474   162    426    190    714   4049    645  17894  20566   368    364    230    685   4197   1079   1700    724    480    788   1372  54936   1072    344    520    584    780  56700  0.000981  0.004144   0.009301  0.169768  0.189836\n",
      "597       42      0  24064  20576  10892   4468      0  26176  19808  10012   4004  17712  15866   872    998    269   1163   3899    415  17328  15053   950   1481    252   1099   5078    167   1820    572   1100    940    876  54692   1552    428    496    680    692  56152  0.002763  0.009659    0.00669  0.130372  0.151839\n",
      "598       39      0  36352  15232   6600   1816      0  35968  16048   6340   1644  18670  10626   961   1592    846    797   4272    241  20368  11760   573   1285    598    991   4673    369   1596    684    840    360    432  56088    764    376    372    220    212  58056  0.000618  0.007762   0.013637  0.180464  0.178932\n",
      "599       20      0   9088  14240  14788  21884      0   9024  14192  14528  22256   7612   6963  3088   3589   2249    866   1535    707   7897   7679  2997   3678   2292    894   1488    708   6808   3436   6160   2892    708  39996   6648   3656   5988   2788    604  40316  0.000096   0.00092   0.000379  0.032926  0.023844\n",
      "\n",
      "[600 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # スケーラーを使って結合したデータをスケーリング\n",
    "    X_train = scaler.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    # pandasをndarrayに変換\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "\n",
    "    # ラベルの準備\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train\n",
    "\n",
    "def append_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP,\n",
    "                            X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list):\n",
    "    X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train = process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP)\n",
    "    X_train_list.append(X_train)\n",
    "    X_train_onlyGhost_list.append(X_train_onlyGhost)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "X_test_list1 = []\n",
    "X_test_onlyGhost_list1 = []\n",
    "MAE_list_t1 = []\n",
    "FINAL_QP_list_t1 = []\n",
    "Y_test_list1 = []\n",
    "\n",
    "X_test_list2 = []\n",
    "X_test_onlyGhost_list2 = []\n",
    "MAE_list_t2 = []\n",
    "FINAL_QP_list_t2 = []\n",
    "Y_test_list2 = []\n",
    "\n",
    "X_test_list3 = []\n",
    "X_test_onlyGhost_list3 = []\n",
    "MAE_list_t3 = []\n",
    "FINAL_QP_list_t3 = []\n",
    "Y_test_list3 = []\n",
    "\n",
    "\n",
    "# データを処理してリストに追加\n",
    "append_results_to_lists(train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "append_results_to_lists(train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9, X_train_list, X_train_onlyGhost_list, MAE_list, FINAL_QP_list, Y_train_list)\n",
    "\n",
    "append_results_to_lists(test_df1, test_df_onlyGhost1, LABEL_t1, MAE_t1, FINAL_QP_t1, X_test_list1, X_test_onlyGhost_list1, MAE_list_t1, FINAL_QP_list_t1, Y_test_list1)\n",
    "append_results_to_lists(test_df2, test_df_onlyGhost2, LABEL_t2, MAE_t2, FINAL_QP_t2, X_test_list2, X_test_onlyGhost_list2, MAE_list_t2, FINAL_QP_list_t2, Y_test_list2)\n",
    "append_results_to_lists(test_df3, test_df_onlyGhost3, LABEL_t3, MAE_t3, FINAL_QP_t3, X_test_list3, X_test_onlyGhost_list3, MAE_list_t3, FINAL_QP_list_t3, Y_test_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各リストの長さを確認\n",
    "# print(len(X_train_list))\n",
    "# print((X_train_onlyGhost_list[0])[0])\n",
    "# print((MAE_list[0])[0])\n",
    "# print((FINAL_QP_list[0])[0])\n",
    "# print((Y_train_list[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 8]\n",
      "Test indices: [7]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9842    0.6233    0.7633       300\n",
      "           1     0.7244    0.9900    0.8366       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8543    0.8067    0.7999       600\n",
      "weighted avg     0.8543    0.8067    0.7999       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9831    0.5800    0.7296       300\n",
      "           1     0.7021    0.9900    0.8216       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.8426    0.7850    0.7756       600\n",
      "weighted avg     0.8426    0.7850    0.7756       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7444    0.8833    0.8079       300\n",
      "           1     0.8566    0.6967    0.7684       300\n",
      "\n",
      "    accuracy                         0.7900       600\n",
      "   macro avg     0.8005    0.7900    0.7882       600\n",
      "weighted avg     0.8005    0.7900    0.7882       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7656    0.8600    0.8100       300\n",
      "           1     0.8403    0.7367    0.7851       300\n",
      "\n",
      "    accuracy                         0.7983       600\n",
      "   macro avg     0.8029    0.7983    0.7976       600\n",
      "weighted avg     0.8029    0.7983    0.7976       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6627    0.7400    0.6992       300\n",
      "           1     0.7057    0.6233    0.6619       300\n",
      "\n",
      "    accuracy                         0.6817       600\n",
      "   macro avg     0.6842    0.6817    0.6806       600\n",
      "weighted avg     0.6842    0.6817    0.6806       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7647    0.4767    0.5873       300\n",
      "           1     0.6199    0.8533    0.7181       300\n",
      "\n",
      "    accuracy                         0.6650       600\n",
      "   macro avg     0.6923    0.6650    0.6527       600\n",
      "weighted avg     0.6923    0.6650    0.6527       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8]\n",
      "Test indices: [1]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9760    0.5433    0.6981       300\n",
      "           1     0.6836    0.9867    0.8076       300\n",
      "\n",
      "    accuracy                         0.7650       600\n",
      "   macro avg     0.8298    0.7650    0.7529       600\n",
      "weighted avg     0.8298    0.7650    0.7529       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9829    0.5733    0.7242       300\n",
      "           1     0.6988    0.9900    0.8193       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.8408    0.7817    0.7718       600\n",
      "weighted avg     0.8408    0.7817    0.7718       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8408    0.8100    0.8251       300\n",
      "           1     0.8167    0.8467    0.8314       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8288    0.8283    0.8283       600\n",
      "weighted avg     0.8288    0.8283    0.8283       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7715    0.8667    0.8163       300\n",
      "           1     0.8479    0.7433    0.7922       300\n",
      "\n",
      "    accuracy                         0.8050       600\n",
      "   macro avg     0.8097    0.8050    0.8043       600\n",
      "weighted avg     0.8097    0.8050    0.8043       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7215    0.5700    0.6369       300\n",
      "           1     0.6446    0.7800    0.7059       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6831    0.6750    0.6714       600\n",
      "weighted avg     0.6831    0.6750    0.6714       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7668    0.4933    0.6004       300\n",
      "           1     0.6265    0.8500    0.7214       300\n",
      "\n",
      "    accuracy                         0.6717       600\n",
      "   macro avg     0.6967    0.6717    0.6609       600\n",
      "weighted avg     0.6967    0.6717    0.6609       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8]\n",
      "Test indices: [5]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9831    0.5833    0.7322       300\n",
      "           1     0.7038    0.9900    0.8227       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.8435    0.7867    0.7775       600\n",
      "weighted avg     0.8435    0.7867    0.7775       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9746    0.6400    0.7726       300\n",
      "           1     0.7320    0.9833    0.8393       300\n",
      "\n",
      "    accuracy                         0.8117       600\n",
      "   macro avg     0.8533    0.8117    0.8059       600\n",
      "weighted avg     0.8533    0.8117    0.8059       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7870    0.8867    0.8339       300\n",
      "           1     0.8702    0.7600    0.8114       300\n",
      "\n",
      "    accuracy                         0.8233       600\n",
      "   macro avg     0.8286    0.8233    0.8226       600\n",
      "weighted avg     0.8286    0.8233    0.8226       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8123    0.8367    0.8243       300\n",
      "           1     0.8316    0.8067    0.8190       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8220    0.8217    0.8216       600\n",
      "weighted avg     0.8220    0.8217    0.8216       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6345    0.7233    0.6760       300\n",
      "           1     0.6783    0.5833    0.6272       300\n",
      "\n",
      "    accuracy                         0.6533       600\n",
      "   macro avg     0.6564    0.6533    0.6516       600\n",
      "weighted avg     0.6564    0.6533    0.6516       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7463    0.5000    0.5988       300\n",
      "           1     0.6241    0.8300    0.7124       300\n",
      "\n",
      "    accuracy                         0.6650       600\n",
      "   macro avg     0.6852    0.6650    0.6556       600\n",
      "weighted avg     0.6852    0.6650    0.6556       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8]\n",
      "Test indices: [0]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9814    0.5267    0.6855       300\n",
      "           1     0.6765    0.9900    0.8038       300\n",
      "\n",
      "    accuracy                         0.7583       600\n",
      "   macro avg     0.8290    0.7583    0.7446       600\n",
      "weighted avg     0.8290    0.7583    0.7446       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9783    0.6000    0.7438       300\n",
      "           1     0.7115    0.9867    0.8268       300\n",
      "\n",
      "    accuracy                         0.7933       600\n",
      "   macro avg     0.8449    0.7933    0.7853       600\n",
      "weighted avg     0.8449    0.7933    0.7853       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8241    0.8900    0.8558       300\n",
      "           1     0.8804    0.8100    0.8438       300\n",
      "\n",
      "    accuracy                         0.8500       600\n",
      "   macro avg     0.8523    0.8500    0.8498       600\n",
      "weighted avg     0.8523    0.8500    0.8498       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8267    0.8267    0.8267       300\n",
      "           1     0.8267    0.8267    0.8267       300\n",
      "\n",
      "    accuracy                         0.8267       600\n",
      "   macro avg     0.8267    0.8267    0.8267       600\n",
      "weighted avg     0.8267    0.8267    0.8267       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6615    0.7100    0.6849       300\n",
      "           1     0.6871    0.6367    0.6609       300\n",
      "\n",
      "    accuracy                         0.6733       600\n",
      "   macro avg     0.6743    0.6733    0.6729       600\n",
      "weighted avg     0.6743    0.6733    0.6729       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7673    0.5167    0.6175       300\n",
      "           1     0.6357    0.8433    0.7249       300\n",
      "\n",
      "    accuracy                         0.6800       600\n",
      "   macro avg     0.7015    0.6800    0.6712       600\n",
      "weighted avg     0.7015    0.6800    0.6712       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [8]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9609    0.5733    0.7182       300\n",
      "           1     0.6960    0.9767    0.8128       300\n",
      "\n",
      "    accuracy                         0.7750       600\n",
      "   macro avg     0.8284    0.7750    0.7655       600\n",
      "weighted avg     0.8284    0.7750    0.7655       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9843    0.6267    0.7658       300\n",
      "           1     0.7262    0.9900    0.8378       300\n",
      "\n",
      "    accuracy                         0.8083       600\n",
      "   macro avg     0.8552    0.8083    0.8018       600\n",
      "weighted avg     0.8552    0.8083    0.8018       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7903    0.8167    0.8033       300\n",
      "           1     0.8103    0.7833    0.7966       300\n",
      "\n",
      "    accuracy                         0.8000       600\n",
      "   macro avg     0.8003    0.8000    0.7999       600\n",
      "weighted avg     0.8003    0.8000    0.7999       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8006    0.8567    0.8277       300\n",
      "           1     0.8459    0.7867    0.8152       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8233    0.8217    0.8214       600\n",
      "weighted avg     0.8233    0.8217    0.8214       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7403    0.6367    0.6846       300\n",
      "           1     0.6813    0.7767    0.7259       300\n",
      "\n",
      "    accuracy                         0.7067       600\n",
      "   macro avg     0.7108    0.7067    0.7052       600\n",
      "weighted avg     0.7108    0.7067    0.7052       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7312    0.4533    0.5597       300\n",
      "           1     0.6039    0.8333    0.7003       300\n",
      "\n",
      "    accuracy                         0.6433       600\n",
      "   macro avg     0.6675    0.6433    0.6300       600\n",
      "weighted avg     0.6675    0.6433    0.6300       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8]\n",
      "Test indices: [2]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9767    0.5600    0.7119       300\n",
      "           1     0.6916    0.9867    0.8132       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.8342    0.7733    0.7625       600\n",
      "weighted avg     0.8342    0.7733    0.7625       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9775    0.5800    0.7280       300\n",
      "           1     0.7014    0.9867    0.8199       300\n",
      "\n",
      "    accuracy                         0.7833       600\n",
      "   macro avg     0.8395    0.7833    0.7740       600\n",
      "weighted avg     0.8395    0.7833    0.7740       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7802    0.8400    0.8090       300\n",
      "           1     0.8267    0.7633    0.7938       300\n",
      "\n",
      "    accuracy                         0.8017       600\n",
      "   macro avg     0.8035    0.8017    0.8014       600\n",
      "weighted avg     0.8035    0.8017    0.8014       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7605    0.8467    0.8013       300\n",
      "           1     0.8271    0.7333    0.7774       300\n",
      "\n",
      "    accuracy                         0.7900       600\n",
      "   macro avg     0.7938    0.7900    0.7893       600\n",
      "weighted avg     0.7938    0.7900    0.7893       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7209    0.6200    0.6667       300\n",
      "           1     0.6667    0.7600    0.7103       300\n",
      "\n",
      "    accuracy                         0.6900       600\n",
      "   macro avg     0.6938    0.6900    0.6885       600\n",
      "weighted avg     0.6938    0.6900    0.6885       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7571    0.4467    0.5618       300\n",
      "           1     0.6076    0.8567    0.7109       300\n",
      "\n",
      "    accuracy                         0.6517       600\n",
      "   macro avg     0.6823    0.6517    0.6364       600\n",
      "weighted avg     0.6823    0.6517    0.6364       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 5 6 7 8]\n",
      "Test indices: [4]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9842    0.6233    0.7633       300\n",
      "           1     0.7244    0.9900    0.8366       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8543    0.8067    0.7999       600\n",
      "weighted avg     0.8543    0.8067    0.7999       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9752    0.6567    0.7849       300\n",
      "           1     0.7412    0.9833    0.8453       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8582    0.8200    0.8151       600\n",
      "weighted avg     0.8582    0.8200    0.8151       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8137    0.8733    0.8424       300\n",
      "           1     0.8633    0.8000    0.8304       300\n",
      "\n",
      "    accuracy                         0.8367       600\n",
      "   macro avg     0.8385    0.8367    0.8364       600\n",
      "weighted avg     0.8385    0.8367    0.8364       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8119    0.8200    0.8159       300\n",
      "           1     0.8182    0.8100    0.8141       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8150    0.8150    0.8150       600\n",
      "weighted avg     0.8150    0.8150    0.8150       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6801    0.7300    0.7042       300\n",
      "           1     0.7086    0.6567    0.6817       300\n",
      "\n",
      "    accuracy                         0.6933       600\n",
      "   macro avg     0.6944    0.6933    0.6929       600\n",
      "weighted avg     0.6944    0.6933    0.6929       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7789    0.5167    0.6212       300\n",
      "           1     0.6384    0.8533    0.7304       300\n",
      "\n",
      "    accuracy                         0.6850       600\n",
      "   macro avg     0.7086    0.6850    0.6758       600\n",
      "weighted avg     0.7086    0.6850    0.6758       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 4 5 6 7 8]\n",
      "Test indices: [3]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9944    0.5900    0.7406       300\n",
      "           1     0.7085    0.9967    0.8283       300\n",
      "\n",
      "    accuracy                         0.7933       600\n",
      "   macro avg     0.8515    0.7933    0.7844       600\n",
      "weighted avg     0.8515    0.7933    0.7844       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9393    0.6700    0.7821       300\n",
      "           1     0.7435    0.9567    0.8367       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8414    0.8133    0.8094       600\n",
      "weighted avg     0.8414    0.8133    0.8094       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7940    0.8867    0.8378       300\n",
      "           1     0.8717    0.7700    0.8177       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8329    0.8283    0.8277       600\n",
      "weighted avg     0.8329    0.8283    0.8277       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6799    0.9133    0.7795       300\n",
      "           1     0.8680    0.5700    0.6881       300\n",
      "\n",
      "    accuracy                         0.7417       600\n",
      "   macro avg     0.7740    0.7417    0.7338       600\n",
      "weighted avg     0.7740    0.7417    0.7338       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6637    0.7500    0.7042       300\n",
      "           1     0.7126    0.6200    0.6631       300\n",
      "\n",
      "    accuracy                         0.6850       600\n",
      "   macro avg     0.6882    0.6850    0.6837       600\n",
      "weighted avg     0.6882    0.6850    0.6837       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6571    0.7600    0.7048       300\n",
      "           1     0.7154    0.6033    0.6546       300\n",
      "\n",
      "    accuracy                         0.6817       600\n",
      "   macro avg     0.6862    0.6817    0.6797       600\n",
      "weighted avg     0.6862    0.6817    0.6797       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 3 4 5 7 8]\n",
      "Test indices: [6]\n",
      "0.86\n",
      "0.515\n",
      "0.5883333333333334\n",
      "report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9745    0.6367    0.7702       300\n",
      "           1     0.7302    0.9833    0.8381       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8523    0.8100    0.8041       600\n",
      "weighted avg     0.8523    0.8100    0.8041       600\n",
      "\n",
      "report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9892    0.6100    0.7546       300\n",
      "           1     0.7181    0.9933    0.8336       300\n",
      "\n",
      "    accuracy                         0.8017       600\n",
      "   macro avg     0.8536    0.8017    0.7941       600\n",
      "weighted avg     0.8536    0.8017    0.7941       600\n",
      "\n",
      "same_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7884    0.9067    0.8434       300\n",
      "           1     0.8902    0.7567    0.8180       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8393    0.8317    0.8307       600\n",
      "weighted avg     0.8393    0.8317    0.8307       600\n",
      "\n",
      "same_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7522    0.8500    0.7981       300\n",
      "           1     0.8276    0.7200    0.7701       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7899    0.7850    0.7841       600\n",
      "weighted avg     0.7899    0.7850    0.7841       600\n",
      "\n",
      "large_report_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6479    0.7667    0.7023       300\n",
      "           1     0.7143    0.5833    0.6422       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6811    0.6750    0.6722       600\n",
      "weighted avg     0.6811    0.6750    0.6722       600\n",
      "\n",
      "large_report_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.4700    0.5779       300\n",
      "           1     0.6141    0.8433    0.7107       300\n",
      "\n",
      "    accuracy                         0.6567       600\n",
      "   macro avg     0.6820    0.6567    0.6443       600\n",
      "weighted avg     0.6820    0.6567    0.6443       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9300    0.8692       300\n",
      "           1     0.9186    0.7900    0.8495       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8672    0.8600    0.8593       600\n",
      "weighted avg     0.8672    0.8600    0.8593       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5169    0.4600    0.4868       300\n",
      "           1     0.5135    0.5700    0.5403       300\n",
      "\n",
      "    accuracy                         0.5150       600\n",
      "   macro avg     0.5152    0.5150    0.5135       600\n",
      "weighted avg     0.5152    0.5150    0.5135       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6803    0.3333    0.4474       300\n",
      "           1     0.5585    0.8433    0.6720       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6194    0.5883    0.5597       600\n",
      "weighted avg     0.6194    0.5883    0.5597       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "# C_values = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "kfold = KFold(n_splits=9, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# results = pd.DataFrame(columns=['C_RBF_LQP1','Score_RBF_LQP1', 'tnr_rbf_lqp1', 'tpr_rbf_lqp1',\n",
    "#                                 'C_RBF_SQP','Score_RBF_SQP', 'tnr_rbf_sqp', 'tpr_rbf_sqp',\n",
    "#                                 'C_RBF_LQP2','Score_RBF_LQP2', 'tnr_rbf_lqp2', 'tpr_rbf_lqp2',\n",
    "#                                 'C_LINEAR_LQP1','Score_LINEAR_LQP1', 'tnr_linear_lqp1', 'tpr_linear_lqp1',\n",
    "#                                 'C_LINEAR_SQP','Score_LINEAR_SQP', 'tnr_linear_sqp', 'tpr_linear_sqp',\n",
    "#                                 'C_LINEAR_LQP2','Score_LINEAR_LQP2', 'tnr_linear_lqp2', 'tpr_linear_lqp2',\n",
    "                                \n",
    "#                                 'C_RBF_OG_LQP1','Score_RBF_OG_LQP1', 'tnr_og_rbf_lqp1', 'tpr_og_rbf_lqp1',\n",
    "#                                 'C_RBF_OG_SQP','Score_RBF_OG_SQP', 'tnr_og_rbf_sqp', 'tpr_og_rbf_sqp',\n",
    "#                                 'C_RBF_OG_LQP2','Score_RBF_OG_LQP2', 'tnr_og_rbf_lqp2', 'tpr_og_rbf_lqp2',\n",
    "#                                 'C_LINEAR_OG_LQP1','Score_LINEAR_OG_LQP1', 'tnr_og_linear_lqp1', 'tpr_og_linear_lqp1',\n",
    "#                                 'C_LINEAR_OG_SQP','Score_LINEAR_OG_SQP', 'tnr_og_linear_sqp', 'tpr_og_linear_sqp',\n",
    "#                                 'C_LINEAR_OG_LQP2','Score_LINEAR_OG_LQP2', 'tnr_og_linear_lqp2', 'tpr_og_linear_lqp2',\n",
    "                                \n",
    "#                                 'Threshold_LQP1', 'LQP1_old', 'tnr_old_lqp1', 'tpr_old_lqp1',\n",
    "#                                 'Threshold_SQP', 'SQP_old', 'tnr_old_sqp', 'tpr_old_sqp',\n",
    "#                                 'Threshold_LQP2', 'LQP2_old', 'tnr_old_lqp2', 'tpr_old_lqp2'])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['C_RBF_LQP1','Score_RBF_LQP1', 'tnr_rbf_lqp1', 'tpr_rbf_lqp1',\n",
    "                                'C_RBF_SQP','Score_RBF_SQP', 'tnr_rbf_sqp', 'tpr_rbf_sqp',\n",
    "                                'C_RBF_LQP2','Score_RBF_LQP2', 'tnr_rbf_lqp2', 'tpr_rbf_lqp2',\n",
    "                                'C_LINEAR_LQP1','Score_LINEAR_LQP1', 'tnr_linear_lqp1', 'tpr_linear_lqp1',\n",
    "                                'C_LINEAR_SQP','Score_LINEAR_SQP', 'tnr_linear_sqp', 'tpr_linear_sqp',\n",
    "                                'C_LINEAR_LQP2','Score_LINEAR_LQP2', 'tnr_linear_lqp2', 'tpr_linear_lqp2',\n",
    "                                                        \n",
    "                                'Threshold_LQP1', 'LQP1_old', 'tnr_old_lqp1', 'tpr_old_lqp1',\n",
    "                                'Threshold_SQP', 'SQP_old', 'tnr_old_sqp', 'tpr_old_sqp',\n",
    "                                'Threshold_LQP2', 'LQP2_old', 'tnr_old_lqp2', 'tpr_old_lqp2'])\n",
    "\n",
    "    \n",
    "X_index = np.arange(9)  # インデックスとして0から8までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    X_train_OG = [item for data in train_data_OG for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    X_val_OG = [item for data in val_data_OG for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    # print(len(Y_train))\n",
    "    # print(len(Y_val))\n",
    "    \n",
    "    test_data1 = [item for data in X_test_list1 for item in data]\n",
    "    test_data_OG1 = [item for data in X_test_onlyGhost_list1 for item in data]\n",
    "    test_label1 = [item for data in Y_test_list1 for item in data]\n",
    "    MAE_data1 = [item for data in MAE_list_t1 for item in data]\n",
    "    FINAL_QP_data1 = [item for data in FINAL_QP_list_t1 for item in data]\n",
    "    \n",
    "    test_data2 = [item for data in X_test_list2 for item in data]\n",
    "    test_data_OG2 = [item for data in X_test_onlyGhost_list2 for item in data]\n",
    "    test_label2 = [item for data in Y_test_list2 for item in data]\n",
    "    MAE_data2 = [item for data in MAE_list_t2 for item in data]\n",
    "    FINAL_QP_data2 = [item for data in FINAL_QP_list_t2 for item in data]\n",
    "    \n",
    "    test_data3 = [item for data in X_test_list3 for item in data]\n",
    "    test_data_OG3 = [item for data in X_test_onlyGhost_list3 for item in data]\n",
    "    test_label3 = [item for data in Y_test_list3 for item in data]\n",
    "    MAE_data3 = [item for data in MAE_list_t3 for item in data]\n",
    "    FINAL_QP_data3 = [item for data in FINAL_QP_list_t3 for item in data]\n",
    "    \n",
    "    # print(len(MAE_data1))\n",
    "    # print(len(MAE_data2))\n",
    "    # print(len(MAE_data3))\n",
    "    \n",
    "                                                                                   \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "    \n",
    "    sameQP_best_threshold = 0\n",
    "    sameQP_best_accuracy = 0\n",
    "    sameQP_best_predicted_labels = []\n",
    "    sameQP_best_ground_truth_labels = []\n",
    "    \n",
    "    largeQP_best_threshold = 0\n",
    "    largeQP_best_accuracy = 0\n",
    "    largeQP_best_predicted_labels = []\n",
    "    largeQP_best_ground_truth_labels = []\n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_old = np.array([is_double_compressed(MAE_data1[i], FINAL_QP_data1[i], threshold) for i in range(600)])\n",
    "        predicted_labels = test_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label1)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "            \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_sameQP_old = np.array([is_double_compressed(MAE_data2[i], FINAL_QP_data2[i], threshold) for i in range(600)])\n",
    "        same_predicted_labels = test_sameQP_old.astype(int)\n",
    "        same_ground_truth_labels = np.array(test_label2)\n",
    "        same_accuracy = np.sum(same_ground_truth_labels == same_predicted_labels) / len(same_ground_truth_labels)\n",
    "    \n",
    "        if same_accuracy > sameQP_best_accuracy:\n",
    "            sameQP_best_accuracy = same_accuracy\n",
    "            sameQP_best_threshold = threshold\n",
    "            sameQP_best_predicted_labels = same_predicted_labels\n",
    "            sameQP_best_ground_truth_labels = same_ground_truth_labels\n",
    "                        \n",
    "    for threshold in np.arange(0.01, 1.00, 0.01):\n",
    "        test_largeQP_old = np.array([is_double_compressed(MAE_data3[i], FINAL_QP_data3[i], threshold) for i in range(600)])\n",
    "        large_predicted_labels = test_largeQP_old.astype(int)\n",
    "        large_ground_truth_labels = np.array(test_label3)\n",
    "        large_accuracy = np.sum(large_ground_truth_labels == large_predicted_labels) / len(large_ground_truth_labels)\n",
    "    \n",
    "        if large_accuracy > largeQP_best_accuracy:\n",
    "            largeQP_best_accuracy = large_accuracy\n",
    "            largeQP_best_threshold = threshold\n",
    "            largeQP_best_predicted_labels = large_predicted_labels\n",
    "            largeQP_best_ground_truth_labels = large_ground_truth_labels       \n",
    "            \n",
    "            \n",
    "    print(best_accuracy)\n",
    "    print(sameQP_best_accuracy)\n",
    "    print(largeQP_best_accuracy)\n",
    "            \n",
    "            \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        \n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "        \n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "\n",
    "    # テストデータで評価    \n",
    "    predictions_RBF = best_svm_model_RBF.predict(test_data1)\n",
    "    # predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test)\n",
    "    accuracy_RBF = accuracy_score(test_label1, predictions_RBF)\n",
    "    report_RBF = classification_report(test_label1, predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label1, predictions_RBF)\n",
    "    tnr_rbf_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'report_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    predictions_LINEAR = best_svm_model_LINEAR.predict(test_data1)\n",
    "    # predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test)\n",
    "    accuracy_LINEAR = accuracy_score(test_label1, predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label1, predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label1, predictions_LINEAR)\n",
    "    tnr_linear_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'report_LINEAR:\\n{report_LINEAR}')\n",
    "    \n",
    "    same_predictions_RBF = best_svm_model_RBF.predict(test_data2)\n",
    "    # same_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_sameQP)\n",
    "    same_accuracy_RBF = accuracy_score(test_label2, same_predictions_RBF)\n",
    "    same_report_RBF = classification_report(test_label2, same_predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label2, same_predictions_RBF)\n",
    "    tnr_rbf_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'same_report_RBF:\\n{same_report_RBF}')\n",
    "    \n",
    "    same_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data2)\n",
    "    # same_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_sameQP)\n",
    "    same_accuracy_LINEAR = accuracy_score(test_label2, same_predictions_LINEAR)\n",
    "    same_report_LINEAR = classification_report(test_label2, same_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label2, same_predictions_LINEAR)\n",
    "    tnr_linear_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'same_report_LINEAR:\\n{same_report_LINEAR}')\n",
    "    \n",
    "    large_predictions_RBF = best_svm_model_RBF.predict(test_data3)\n",
    "    # large_predictions_prob_RBF = best_svm_model_RBF.decision_function(X_test_largeQP)\n",
    "    large_accuracy_RBF = accuracy_score(test_label3, large_predictions_RBF)\n",
    "    large_report_RBF = classification_report(test_label3, large_predictions_RBF, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label3, large_predictions_RBF)\n",
    "    tnr_rbf_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'large_report_RBF:\\n{large_report_RBF}')\n",
    "    \n",
    "    large_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data3)\n",
    "    # large_predictions_prob_LINEAR = best_svm_model_LINEAR.decision_function(X_test_largeQP)\n",
    "    large_accuracy_LINEAR = accuracy_score(test_label3, large_predictions_LINEAR)\n",
    "    large_report_LINEAR = classification_report(test_label3, large_predictions_LINEAR, digits=4, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(test_label3, large_predictions_LINEAR)\n",
    "    tnr_linear_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'large_report_LINEAR:\\n{large_report_LINEAR}')\n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old_lqp1 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_lqp1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_old}')\n",
    "    \n",
    "    test_sameQP_old = classification_report(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(sameQP_best_ground_truth_labels, sameQP_best_predicted_labels)\n",
    "    tnr_old_sqp = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_sqp = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_sameQP_old}')\n",
    "    \n",
    "    test_largeQP_old = classification_report(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(largeQP_best_ground_truth_labels, largeQP_best_predicted_labels)\n",
    "    tnr_old_lqp2 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old_lqp2 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{test_largeQP_old}')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Test結果を保存\n",
    "    \n",
    "    result_row ={'C_RBF_LQP1':best_c_value_RBF,'Score_RBF_LQP1': accuracy_RBF, 'tnr_rbf_lqp1':tnr_rbf_lqp1, 'tpr_rbf_lqp1':tpr_rbf_lqp1,\n",
    "                'C_RBF_SQP': best_c_value_RBF, 'Score_RBF_SQP': same_accuracy_RBF, 'tnr_rbf_sqp':tnr_rbf_sqp, 'tpr_rbf_sqp':tpr_rbf_sqp,\n",
    "                'C_RBF_LQP2': best_c_value_RBF,'Score_RBF_LQP2': large_accuracy_RBF, 'tnr_rbf_lqp2':tnr_rbf_lqp2, 'tpr_rbf_lqp2':tpr_rbf_lqp2,\n",
    "                                    \n",
    "                'C_LINEAR_LQP1': best_c_value_LINEAR,'Score_LINEAR_LQP1':accuracy_LINEAR, 'tnr_linear_lqp1':tnr_linear_lqp1, 'tpr_linear_lqp1':tpr_linear_lqp1,\n",
    "                'C_LINEAR_SQP': best_c_value_LINEAR,'Score_LINEAR_SQP':same_accuracy_LINEAR, 'tnr_linear_sqp':tnr_linear_sqp, 'tpr_linear_sqp':tpr_linear_sqp,\n",
    "                'C_LINEAR_LQP2': best_c_value_LINEAR,'Score_LINEAR_LQP2':large_accuracy_LINEAR, 'tnr_linear_lqp2':tnr_linear_lqp2, 'tpr_linear_lqp2':tpr_linear_lqp2,\n",
    "                                                        \n",
    "                'Threshold_LQP1':best_threshold, 'LQP1_old':best_accuracy, 'tnr_old_lqp1':tnr_old_lqp1, 'tpr_old_lqp1':tpr_old_lqp1,\n",
    "                'Threshold_SQP':sameQP_best_threshold, 'SQP_old':sameQP_best_accuracy, 'tnr_old_sqp':tnr_old_sqp, 'tpr_old_sqp':tpr_old_sqp,\n",
    "                'Threshold_LQP2':largeQP_best_threshold, 'LQP2_old':largeQP_best_accuracy, 'tnr_old_lqp2':tnr_old_lqp2, 'tpr_old_lqp2':tpr_old_lqp2}\n",
    "\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0     RBF_LQP1        58.44        98.78               78.61                1.93           81.00           75.83\n",
      "1      RBF_SQP        86.59        77.63               82.11                1.96           85.00           79.00\n",
      "2     RBF_LQP2        69.41        66.89               68.15                1.50           70.67           65.33\n",
      "3  LINEAR_LQP1        61.52        98.44               79.98                1.44           82.00           78.17\n",
      "4   LINEAR_SQP        85.30        74.81               80.06                2.65           82.67           74.17\n",
      "5  LINEAR_LQP2        51.48        81.85               66.67                1.43           68.50           64.33\n",
      "6     OLD_LQP1        93.00        79.00               86.00                0.00           86.00           86.00\n",
      "7      OLD_SQP        46.00        57.00               51.50                0.00           51.50           51.50\n",
      "8     OLD_LQP2        33.33        84.33               58.83                0.00           58.83           58.83\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF_LQP1', 'RBF_SQP', 'RBF_LQP2', 'LINEAR_LQP1', 'LINEAR_SQP', 'LINEAR_LQP2', 'OLD_LQP1', 'OLD_SQP', 'OLD_LQP2'],\n",
    "    'Average TNR': [round(results['tnr_rbf_lqp1'].mean() * 100, 2), round(results['tnr_rbf_sqp'].mean() * 100, 2), round(results['tnr_rbf_lqp2'].mean() * 100, 2), round(results['tnr_linear_lqp1'].mean() * 100, 2), round(results['tnr_linear_sqp'].mean() * 100, 2), round(results['tnr_linear_lqp2'].mean() * 100, 2),round(results['tnr_old_lqp1'].mean() * 100, 2), round(results['tnr_old_sqp'].mean() * 100, 2), round(results['tnr_old_lqp2'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf_lqp1'].mean() * 100, 2), round(results['tpr_rbf_sqp'].mean() * 100, 2), round(results['tpr_rbf_lqp2'].mean() * 100, 2), round(results['tpr_linear_lqp1'].mean() * 100, 2), round(results['tpr_linear_sqp'].mean() * 100, 2), round(results['tpr_linear_lqp2'].mean() * 100, 2),round(results['tpr_old_lqp1'].mean() * 100, 2), round(results['tpr_old_sqp'].mean() * 100, 2), round(results['tpr_old_lqp2'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF_LQP1'].mean() * 100, 2), round(results['Score_RBF_SQP'].mean() * 100, 2), round(results['Score_RBF_LQP2'].mean() * 100, 2), round(results['Score_LINEAR_LQP1'].mean() * 100, 2), round(results['Score_LINEAR_SQP'].mean() * 100, 2), round(results['Score_LINEAR_LQP2'].mean() * 100, 2),round(results['LQP1_old'].mean() * 100, 2), round(results['SQP_old'].mean() * 100, 2), round(results['LQP2_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF_LQP1'].std() * 100, 2), round(results['Score_RBF_SQP'].std() * 100, 2), round(results['Score_RBF_LQP2'].std() * 100, 2), round(results['Score_LINEAR_LQP1'].std() * 100, 2), round(results['Score_LINEAR_SQP'].std() * 100, 2), round(results['Score_LINEAR_LQP2'].std() * 100, 2),round(results['LQP1_old'].std() * 100, 2), round(results['SQP_old'].std() * 100, 2), round(results['LQP2_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF_LQP1'].max() * 100, 2), round(results['Score_RBF_SQP'].max() * 100, 2), round(results['Score_RBF_LQP2'].max() * 100, 2), round(results['Score_LINEAR_LQP1'].max() * 100, 2), round(results['Score_LINEAR_SQP'].max() * 100, 2), round(results['Score_LINEAR_LQP2'].max() * 100, 2),round(results['LQP1_old'].max() * 100, 2), round(results['SQP_old'].max() * 100, 2), round(results['LQP2_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF_LQP1'].min() * 100, 2), round(results['Score_RBF_SQP'].min() * 100, 2), round(results['Score_RBF_LQP2'].min() * 100, 2), round(results['Score_LINEAR_LQP1'].min() * 100, 2), round(results['Score_LINEAR_SQP'].min() * 100, 2), round(results['Score_LINEAR_LQP2'].min() * 100, 2),round(results['LQP1_old'].min() * 100, 2), round(results['SQP_old'].min() * 100, 2), round(results['LQP2_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10\n",
      "1    100\n",
      "2     10\n",
      "3     10\n",
      "4    100\n",
      "5    100\n",
      "6     10\n",
      "7     10\n",
      "8     10\n",
      "Name: C_RBF_LQP1, dtype: object\n",
      "0    1000\n",
      "1    1000\n",
      "2      10\n",
      "3      10\n",
      "4    5000\n",
      "5    5000\n",
      "6      10\n",
      "7       1\n",
      "8    1000\n",
      "Name: C_LINEAR_LQP1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF_LQP1'])\n",
    "print(results['C_LINEAR_LQP1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
