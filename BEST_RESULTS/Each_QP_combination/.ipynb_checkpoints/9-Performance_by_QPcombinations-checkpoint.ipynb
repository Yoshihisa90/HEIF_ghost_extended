{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['198', '173', '157', '205', '47', '13', '183', '32', '133', '170', '235', '129', '185', '195', '164', '247', '118', '207', '231', '293', '192', '182', '257', '41', '130', '46', '204', '273', '296', '34'], ['49', '11', '160', '93', '37', '200', '248', '28', '61', '217', '88', '85', '227', '54', '137', '179', '171', '208', '151', '98', '289', '55', '188', '14', '59', '108', '125', '261', '300', '17'], ['169', '35', '128', '89', '48', '127', '236', '121', '278', '24', '15', '146', '10', '38', '149', '232', '33', '193', '138', '177', '23', '114', '36', '120', '294', '63', '197', '56', '82', '141'], ['101', '86', '6', '102', '99', '116', '229', '69', '239', '259', '281', '274', '299', '251', '199', '16', '4', '112', '210', '109', '95', '269', '209', '126', '175', '162', '266', '262', '224', '212'], ['74', '270', '297', '168', '76', '222', '237', '66', '264', '180', '242', '256', '186', '298', '84', '285', '111', '115', '191', '73', '150', '284', '196', '136', '83', '291', '107', '167', '214', '5'], ['172', '100', '81', '152', '234', '206', '43', '165', '96', '287', '282', '92', '30', '156', '53', '144', '22', '163', '52', '295', '153', '184', '79', '18', '1', '25', '226', '178', '202', '161'], ['288', '228', '290', '21', '72', '67', '181', '253', '104', '42', '2', '225', '213', '238', '123', '78', '26', '68', '27', '143', '70', '203', '140', '39', '65', '106', '155', '77', '272', '20'], ['90', '71', '148', '263', '260', '286', '190', '255', '51', '245', '211', '220', '50', '45', '250', '87', '122', '110', '258', '215', '249', '9', '244', '176', '174', '194', '221', '201', '62', '75'], ['187', '277', '292', '134', '105', '7', '145', '283', '8', '265', '135', '57', '31', '246', '119', '80', '97', '64', '131', '216', '252', '147', '280', '19', '12', '219', '117', '124', '276', '275'], ['158', '233', '154', '267', '159', '132', '243', '240', '3', '60', '189', '218', '29', '91', '223', '113', '139', '166', '103', '142', '279', '58', '241', '40', '44', '254', '268', '271', '230', '94']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1710\n",
      "CSV List 2A: 1710\n",
      "CSV List 3A: 1710\n",
      "CSV List 4A: 1710\n",
      "CSV List 5A: 1710\n",
      "CSV List 6A: 1710\n",
      "CSV List 7A: 1710\n",
      "CSV List 8A: 1710\n",
      "CSV List 9A: 1710\n",
      "CSV List 10A: 1710\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 300\n",
      "CSV List 2A: 300\n",
      "CSV List 3A: 300\n",
      "CSV List 4A: 300\n",
      "CSV List 5A: 300\n",
      "CSV List 6A: 300\n",
      "CSV List 7A: 300\n",
      "CSV List 8A: 300\n",
      "CSV List 9A: 300\n",
      "CSV List 10A: 300\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1170\n",
      "CSV List 2A: 1170\n",
      "CSV List 3A: 1170\n",
      "CSV List 4A: 1170\n",
      "CSV List 5A: 1170\n",
      "CSV List 6A: 1170\n",
      "CSV List 7A: 1170\n",
      "CSV List 8A: 1170\n",
      "CSV List 9A: 1170\n",
      "CSV List 10A: 1170\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1710\n",
      "PKL List 2A: 1710\n",
      "PKL List 3A: 1710\n",
      "PKL List 4A: 1710\n",
      "PKL List 5A: 1710\n",
      "PKL List 6A: 1710\n",
      "PKL List 7A: 1710\n",
      "PKL List 8A: 1710\n",
      "PKL List 9A: 1710\n",
      "PKL List 10A: 1710\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 300\n",
      "PKL List 2A: 300\n",
      "PKL List 3A: 300\n",
      "PKL List 4A: 300\n",
      "PKL List 5A: 300\n",
      "PKL List 6A: 300\n",
      "PKL List 7A: 300\n",
      "PKL List 8A: 300\n",
      "PKL List 9A: 300\n",
      "PKL List 10A: 300\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1170\n",
      "PKL List 2A: 1170\n",
      "PKL List 3A: 1170\n",
      "PKL List 4A: 1170\n",
      "PKL List 5A: 1170\n",
      "PKL List 6A: 1170\n",
      "PKL List 7A: 1170\n",
      "PKL List 8A: 1170\n",
      "PKL List 9A: 1170\n",
      "PKL List 10A: 1170\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP5 = [\"_1stQP5_\"]\n",
    "QP10 = [\"_1stQP10_\"]\n",
    "QP16 = [\"_1stQP16_\"]\n",
    "QP20 = [\"_1stQP20_\"]\n",
    "QP24 = [\"_1stQP24_\"]\n",
    "QP27 = [\"_1stQP27_\"]\n",
    "QP32 = [\"_1stQP32_\"]\n",
    "QP39 = [\"_1stQP39_\"]\n",
    "QP42 = [\"_1stQP42_\"]\n",
    "QP45 = [\"_1stQP45_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP10_QP5 = [\"_1stQP10_2ndQP5_\"]\n",
    "\n",
    "QP15_QP5 = [\"_1stQP15_2ndQP5_\"]\n",
    "QP15_QP10 = [\"_1stQP15_2ndQP10_\"]\n",
    "\n",
    "QP20_QP5 = [\"_1stQP20_2ndQP5_\"]\n",
    "QP20_QP10 = [\"_1stQP20_2ndQP10_\"]\n",
    "QP20_QP16 = [\"_1stQP20_2ndQP16_\"]\n",
    "\n",
    "QP25_QP5 = [\"_1stQP25_2ndQP5_\"]\n",
    "QP25_QP10 = [\"_1stQP25_2ndQP10_\"]\n",
    "QP25_QP16 = [\"_1stQP25_2ndQP16_\"]\n",
    "QP25_QP20 = [\"_1stQP25_2ndQP20_\"]\n",
    "QP25_QP24 = [\"_1stQP25_2ndQP24_\"]\n",
    "\n",
    "QP30_QP5 = [\"_1stQP30_2ndQP5_\"]\n",
    "QP30_QP10 = [\"_1stQP30_2ndQP10_\"]\n",
    "QP30_QP16 = [\"_1stQP30_2ndQP16_\"]\n",
    "QP30_QP20 = [\"_1stQP30_2ndQP20_\"]\n",
    "QP30_QP24 = [\"_1stQP30_2ndQP24_\"]\n",
    "QP30_QP27 = [\"_1stQP30_2ndQP27_\"]\n",
    "\n",
    "QP32_QP5 = [\"_1stQP32_2ndQP5_\"]\n",
    "QP32_QP10 = [\"_1stQP32_2ndQP10_\"]\n",
    "QP32_QP16 = [\"_1stQP32_2ndQP16_\"]\n",
    "QP32_QP20 = [\"_1stQP32_2ndQP20_\"]\n",
    "QP32_QP24 = [\"_1stQP32_2ndQP24_\"]\n",
    "QP32_QP27 = [\"_1stQP32_2ndQP27_\"]\n",
    "\n",
    "QP35_QP5 = [\"_1stQP35_2ndQP5_\"]\n",
    "QP35_QP10 = [\"_1stQP35_2ndQP10_\"]\n",
    "QP35_QP16 = [\"_1stQP35_2ndQP16_\"]\n",
    "QP35_QP20 = [\"_1stQP35_2ndQP20_\"]\n",
    "QP35_QP24 = [\"_1stQP35_2ndQP24_\"]\n",
    "QP35_QP27 = [\"_1stQP35_2ndQP27_\"]\n",
    "QP35_QP32 = [\"_1stQP35_2ndQP32_\"]\n",
    "\n",
    "QP40_QP5 = [\"_1stQP40_2ndQP5_\"]\n",
    "QP40_QP10 = [\"_1stQP40_2ndQP10_\"]\n",
    "QP40_QP16 = [\"_1stQP40_2ndQP16_\"]\n",
    "QP40_QP20 = [\"_1stQP40_2ndQP20_\"]\n",
    "QP40_QP24 = [\"_1stQP40_2ndQP24_\"]\n",
    "QP40_QP27 = [\"_1stQP40_2ndQP27_\"]\n",
    "QP40_QP32 = [\"_1stQP40_2ndQP32_\"]\n",
    "QP40_QP39 = [\"_1stQP40_2ndQP39_\"]\n",
    "\n",
    "QP45_QP5 = [\"_1stQP45_2ndQP5_\"]\n",
    "QP45_QP10 = [\"_1stQP45_2ndQP10_\"]\n",
    "QP45_QP16 = [\"_1stQP45_2ndQP16_\"]\n",
    "QP45_QP20 = [\"_1stQP45_2ndQP20_\"]\n",
    "QP45_QP24 = [\"_1stQP45_2ndQP24_\"]\n",
    "QP45_QP27 = [\"_1stQP45_2ndQP27_\"]\n",
    "QP45_QP32 = [\"_1stQP45_2ndQP32_\"]\n",
    "QP45_QP39 = [\"_1stQP45_2ndQP39_\"]\n",
    "QP45_QP42 = [\"_1stQP45_2ndQP42_\"]\n",
    "\n",
    "QP50_QP5 = [\"_1stQP50_2ndQP5_\"]\n",
    "QP50_QP10 = [\"_1stQP50_2ndQP10_\"]\n",
    "QP50_QP16 = [\"_1stQP50_2ndQP16_\"]\n",
    "QP50_QP20 = [\"_1stQP50_2ndQP20_\"]\n",
    "QP50_QP24 = [\"_1stQP50_2ndQP24_\"]\n",
    "QP50_QP27 = [\"_1stQP50_2ndQP27_\"]\n",
    "QP50_QP32 = [\"_1stQP50_2ndQP32_\"]\n",
    "QP50_QP39 = [\"_1stQP50_2ndQP39_\"]\n",
    "QP50_QP42 = [\"_1stQP50_2ndQP42_\"]\n",
    "QP50_QP45 = [\"_1stQP50_2ndQP45_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP5_QP5 = [\"_1stQP5_2ndQP5\"]\n",
    "QP10_QP10 = [\"_1stQP10_2ndQP10\"]\n",
    "QP16_QP16 = [\"_1stQP16_2ndQP16\"]\n",
    "QP20_QP20 = [\"_1stQP20_2ndQP20\"]\n",
    "QP24_QP24 = [\"_1stQP24_2ndQP24\"]\n",
    "QP27_QP27 = [\"_1stQP27_2ndQP27\"]\n",
    "QP32_QP32 = [\"_1stQP32_2ndQP32\"]\n",
    "QP39_QP39 = [\"_1stQP39_2ndQP39\"]\n",
    "QP42_QP42 = [\"_1stQP42_2ndQP42\"]\n",
    "QP45_QP45 = [\"_1stQP45_2ndQP45\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP10_QP16 = [\"_1stQP10_2ndQP16\"]\n",
    "QP10_QP20 = [\"_1stQP10_2ndQP20\"]\n",
    "QP10_QP24 = [\"_1stQP10_2ndQP24\"]\n",
    "QP10_QP27 = [\"_1stQP10_2ndQP27\"]\n",
    "QP10_QP32 = [\"_1stQP10_2ndQP32\"]\n",
    "QP10_QP39 = [\"_1stQP10_2ndQP39\"]\n",
    "QP10_QP42 = [\"_1stQP10_2ndQP42\"]\n",
    "QP10_QP45 = [\"_1stQP10_2ndQP45\"]\n",
    "\n",
    "QP15_QP16 = [\"_1stQP15_2ndQP16\"]\n",
    "QP15_QP20 = [\"_1stQP15_2ndQP20\"]\n",
    "QP15_QP24 = [\"_1stQP15_2ndQP24\"]\n",
    "QP15_QP27 = [\"_1stQP15_2ndQP27\"]\n",
    "QP15_QP32 = [\"_1stQP15_2ndQP32\"]\n",
    "QP15_QP39 = [\"_1stQP15_2ndQP39\"]\n",
    "QP15_QP42 = [\"_1stQP15_2ndQP42\"]\n",
    "QP15_QP45 = [\"_1stQP15_2ndQP45\"]\n",
    "\n",
    "QP20_QP24 = [\"_1stQP20_2ndQP24\"]\n",
    "QP20_QP27 = [\"_1stQP20_2ndQP27\"]\n",
    "QP20_QP32 = [\"_1stQP20_2ndQP32\"]\n",
    "QP20_QP39 = [\"_1stQP20_2ndQP39\"]\n",
    "QP20_QP42 = [\"_1stQP20_2ndQP42\"]\n",
    "QP20_QP45 = [\"_1stQP20_2ndQP45\"]\n",
    "\n",
    "QP25_QP27 = [\"_1stQP25_2ndQP27\"]\n",
    "QP25_QP32 = [\"_1stQP25_2ndQP32\"]\n",
    "QP25_QP39 = [\"_1stQP25_2ndQP39\"]\n",
    "QP25_QP42 = [\"_1stQP25_2ndQP42\"]\n",
    "QP25_QP45 = [\"_1stQP25_2ndQP45\"]\n",
    "\n",
    "QP30_QP32 = [\"_1stQP30_2ndQP32\"]\n",
    "QP30_QP39 = [\"_1stQP30_2ndQP39\"]\n",
    "QP30_QP42 = [\"_1stQP30_2ndQP42\"]\n",
    "QP30_QP45 = [\"_1stQP30_2ndQP45\"]\n",
    "\n",
    "QP32_QP39 = [\"_1stQP32_2ndQP39\"]\n",
    "QP32_QP42 = [\"_1stQP32_2ndQP42\"]\n",
    "QP32_QP45 = [\"_1stQP32_2ndQP45\"]\n",
    "\n",
    "QP35_QP39 = [\"_1stQP35_2ndQP39\"]\n",
    "QP35_QP42 = [\"_1stQP35_2ndQP42\"]\n",
    "QP35_QP45 = [\"_1stQP35_2ndQP45\"]\n",
    "\n",
    "QP40_QP42 = [\"_1stQP40_2ndQP42\"]\n",
    "QP40_QP45 = [\"_1stQP40_2ndQP45\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "('/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/HEIF_images_single_csv/94_1stQP5_CTU64_PRESETmedium.csv', '/Prove/Yoshihisa/HEIF_ghost/PKL/pkl_single/94_1stQP5_CTU64_PRESETmedium.pkl', '/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/HEIF_images_second_sameQP_csv/94_1stQP5_2ndQP5_CTU64_PRESETmedium.csv', '/Prove/Yoshihisa/HEIF_ghost/PKL/pkl_second_sameQP/94_1stQP5_2ndQP5_CTU64_PRESETmedium.pkl')\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "print(len(single_csv9))\n",
    "\n",
    "\n",
    "single_QP5 = [item for item in single_csv10 if any(qp in item[0] for qp in QP5)]\n",
    "single_QP10 = [item for item in single_csv10 if any(qp in item[0] for qp in QP10)]\n",
    "single_QP16 = [item for item in single_csv10 if any(qp in item[0] for qp in QP16)]\n",
    "single_QP20 = [item for item in single_csv10 if any(qp in item[0] for qp in QP20)]\n",
    "single_QP24 = [item for item in single_csv10 if any(qp in item[0] for qp in QP24)]\n",
    "single_QP27 = [item for item in single_csv10 if any(qp in item[0] for qp in QP27)]\n",
    "single_QP32 = [item for item in single_csv10 if any(qp in item[0] for qp in QP32)]\n",
    "single_QP39 = [item for item in single_csv10 if any(qp in item[0] for qp in QP39)]\n",
    "single_QP42 = [item for item in single_csv10 if any(qp in item[0] for qp in QP42)]\n",
    "single_QP45 = [item for item in single_csv10 if any(qp in item[0] for qp in QP45)]\n",
    "print((single_QP5[29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "\n",
      "double images train by QP1>QP2:  100\n",
      "\n",
      "double images test by QP1>QP2:  30\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "# second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1>QP2: ', len(second_largeQP1_csv1))\n",
    "\n",
    "second_QP10_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP10_QP5)]\n",
    "second_QP15_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP15_QP5)]\n",
    "second_QP15_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP15_QP10)]\n",
    "second_QP20_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP20_QP5)]\n",
    "second_QP20_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP20_QP10)]\n",
    "second_QP20_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP20_QP16)]\n",
    "second_QP25_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP5)]\n",
    "second_QP25_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP10)]\n",
    "second_QP25_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP16)]\n",
    "second_QP25_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP20)]\n",
    "second_QP25_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP25_QP24)]\n",
    "second_QP30_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP5)]\n",
    "second_QP30_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP10)]\n",
    "second_QP30_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP16)]\n",
    "second_QP30_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP20)]\n",
    "second_QP30_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP24)]\n",
    "second_QP30_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP30_QP27)]\n",
    "second_QP32_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP5)]\n",
    "second_QP32_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP10)]\n",
    "second_QP32_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP16)]\n",
    "second_QP32_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP20)]\n",
    "second_QP32_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP24)]\n",
    "second_QP32_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP32_QP27)]\n",
    "second_QP35_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP5)]\n",
    "second_QP35_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP10)]\n",
    "second_QP35_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP16)]\n",
    "second_QP35_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP20)]\n",
    "second_QP35_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP24)]\n",
    "second_QP35_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP27)]\n",
    "second_QP35_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP35_QP32)]\n",
    "second_QP40_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP5)]\n",
    "second_QP40_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP10)]\n",
    "second_QP40_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP16)]\n",
    "second_QP40_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP20)]\n",
    "second_QP40_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP24)]\n",
    "second_QP40_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP27)]\n",
    "second_QP40_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP32)]\n",
    "second_QP40_QP39 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP40_QP39)]\n",
    "second_QP45_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP5)]\n",
    "second_QP45_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP10)]\n",
    "second_QP45_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP16)]\n",
    "second_QP45_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP20)]\n",
    "second_QP45_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP24)]\n",
    "second_QP45_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP27)]\n",
    "second_QP45_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP32)]\n",
    "second_QP45_QP39 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP39)]\n",
    "second_QP45_QP42 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP45_QP42)]\n",
    "second_QP50_QP5 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP5)]\n",
    "second_QP50_QP10 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP10)]\n",
    "second_QP50_QP16 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP16)]\n",
    "second_QP50_QP20 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP20)]\n",
    "second_QP50_QP24 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP24)]\n",
    "second_QP50_QP27 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP27)]\n",
    "second_QP50_QP32 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP32)]\n",
    "second_QP50_QP39 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP39)]\n",
    "second_QP50_QP42 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP42)]\n",
    "second_QP50_QP45 = [item for item in second_largeQP1_csv10 if any(qp in item[0] for qp in QP50_QP45)]\n",
    "print('\\ndouble images test by QP1>QP2: ', len(second_QP50_QP45))\n",
    "\n",
    "# second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "# second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "# second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "# second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "# second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "# second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "# second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "# second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "# second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "# second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "\n",
      "double images train by QP1=QP2:  100\n",
      "\n",
      "double images test by QP1 = QP2:  30\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "print(len(second_sameQP_csv10))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 100)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 100)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 100)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 100)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 100)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 100)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 100)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 100)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 100)\n",
    "print('\\ndouble images train by QP1=QP2: ',len(second_sameQP_csv9))\n",
    "\n",
    "\n",
    "second_QP5_QP5 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP5_QP5)]\n",
    "second_QP10_QP10 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP10_QP10)]\n",
    "second_QP16_QP16 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP16_QP16)]\n",
    "second_QP20_QP20 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP20_QP20)]\n",
    "second_QP24_QP24 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP24_QP24)]\n",
    "second_QP27_QP27 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP27_QP27)]\n",
    "second_QP32_QP32 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP32_QP32)]\n",
    "second_QP39_QP39 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP39_QP39)]\n",
    "second_QP42_QP42 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP42_QP42)]\n",
    "second_QP45_QP45 = [item for item in second_sameQP_csv10 if any(qp in item[0] for qp in QP45_QP45)]\n",
    "print('\\ndouble images test by QP1 = QP2: ', len(second_QP5_QP5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170\n",
      "\n",
      "double images train by QP1<QP2:  100\n",
      "\n",
      "Double images by QP (QP1 < QP2):  30\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "print(len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 100)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 100)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 100)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 100)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 100)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 100)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 100)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 100)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 100)\n",
    "# second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1<QP2: ', len(second_largeQP2_csv1))\n",
    "\n",
    "\n",
    "second_QP10_QP16 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP16)]\n",
    "second_QP10_QP20 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP20)]\n",
    "second_QP10_QP24 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP24)]\n",
    "second_QP10_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP27)]\n",
    "second_QP10_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP32)]\n",
    "second_QP10_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP39)]\n",
    "second_QP10_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP42)]\n",
    "second_QP10_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP10_QP45)]\n",
    "second_QP15_QP16 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP16)]\n",
    "second_QP15_QP20 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP20)]\n",
    "second_QP15_QP24 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP24)]\n",
    "second_QP15_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP27)]\n",
    "second_QP15_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP32)]\n",
    "second_QP15_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP39)]\n",
    "second_QP15_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP42)]\n",
    "second_QP15_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP15_QP45)]\n",
    "second_QP20_QP24 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP24)]\n",
    "second_QP20_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP27)]\n",
    "second_QP20_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP32)]\n",
    "second_QP20_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP39)]\n",
    "second_QP20_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP42)]\n",
    "second_QP20_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP20_QP45)]\n",
    "second_QP25_QP27 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP27)]\n",
    "second_QP25_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP32)]\n",
    "second_QP25_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP39)]\n",
    "second_QP25_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP42)]\n",
    "second_QP25_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP25_QP45)]\n",
    "second_QP30_QP32 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP32)]\n",
    "second_QP30_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP39)]\n",
    "second_QP30_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP42)]\n",
    "second_QP30_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP30_QP45)]\n",
    "second_QP32_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP32_QP39)]\n",
    "second_QP32_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP32_QP42)]\n",
    "second_QP32_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP32_QP45)]\n",
    "second_QP35_QP39 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP35_QP39)]\n",
    "second_QP35_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP35_QP42)]\n",
    "second_QP35_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP35_QP45)]\n",
    "second_QP40_QP42 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP40_QP42)]\n",
    "second_QP40_QP45 = [item for item in second_largeQP2_csv10 if any(qp in item[0] for qp in QP40_QP45)]\n",
    "print('\\nDouble images by QP (QP1 < QP2): ', len(second_QP40_QP45))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "print(\"train_csv_list: \", len(train_csv_list9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP50_QP45:  60\n"
     ]
    }
   ],
   "source": [
    "test_QP10_QP5 = second_QP10_QP5 + single_QP5\n",
    "test_QP15_QP5 = second_QP15_QP5 + single_QP5\n",
    "test_QP15_QP10 = second_QP15_QP10 + single_QP10\n",
    "test_QP20_QP5 = second_QP20_QP5 + single_QP5\n",
    "test_QP20_QP10 = second_QP20_QP10 + single_QP10\n",
    "test_QP20_QP16 = second_QP20_QP16 + single_QP16\n",
    "test_QP25_QP5 = second_QP25_QP5 + single_QP5\n",
    "test_QP25_QP10 = second_QP25_QP10 + single_QP10\n",
    "test_QP25_QP16 = second_QP25_QP16 + single_QP16\n",
    "test_QP25_QP20 = second_QP25_QP20 + single_QP20\n",
    "test_QP25_QP24 = second_QP25_QP24 + single_QP24\n",
    "test_QP30_QP5 = second_QP30_QP5 + single_QP5\n",
    "test_QP30_QP10 = second_QP30_QP10 + single_QP10\n",
    "test_QP30_QP16 = second_QP30_QP16 + single_QP16\n",
    "test_QP30_QP20 = second_QP30_QP20 + single_QP20\n",
    "test_QP30_QP24 = second_QP30_QP24 + single_QP24\n",
    "test_QP30_QP27 = second_QP30_QP27 + single_QP27\n",
    "test_QP32_QP5 = second_QP32_QP5 + single_QP5\n",
    "test_QP32_QP10 = second_QP32_QP10 + single_QP10\n",
    "test_QP32_QP16 = second_QP32_QP16 + single_QP16\n",
    "test_QP32_QP20 = second_QP32_QP20 + single_QP20\n",
    "test_QP32_QP24 = second_QP32_QP24 + single_QP24\n",
    "test_QP32_QP27 = second_QP32_QP27 + single_QP27\n",
    "test_QP35_QP5 = second_QP35_QP5 + single_QP5\n",
    "test_QP35_QP10 = second_QP35_QP10 + single_QP10\n",
    "test_QP35_QP16 = second_QP35_QP16 + single_QP16\n",
    "test_QP35_QP20 = second_QP35_QP20 + single_QP20\n",
    "test_QP35_QP24 = second_QP35_QP24 + single_QP24\n",
    "test_QP35_QP27 = second_QP35_QP27 + single_QP27\n",
    "test_QP35_QP32 = second_QP35_QP32 + single_QP32\n",
    "test_QP40_QP5 = second_QP40_QP5 + single_QP5\n",
    "test_QP40_QP10 = second_QP40_QP10 + single_QP10\n",
    "test_QP40_QP16 = second_QP40_QP16 + single_QP16\n",
    "test_QP40_QP20 = second_QP40_QP20 + single_QP20\n",
    "test_QP40_QP24 = second_QP40_QP24 + single_QP24\n",
    "test_QP40_QP27 = second_QP40_QP27 + single_QP27\n",
    "test_QP40_QP32 = second_QP40_QP32 + single_QP32\n",
    "test_QP40_QP39 = second_QP40_QP39 + single_QP39\n",
    "test_QP45_QP5 = second_QP45_QP5 + single_QP5\n",
    "test_QP45_QP10 = second_QP45_QP10 + single_QP10\n",
    "test_QP45_QP16 = second_QP45_QP16 + single_QP16\n",
    "test_QP45_QP20 = second_QP45_QP20 + single_QP20\n",
    "test_QP45_QP24 = second_QP45_QP24 + single_QP24\n",
    "test_QP45_QP27 = second_QP45_QP27 + single_QP27\n",
    "test_QP45_QP32 = second_QP45_QP32 + single_QP32\n",
    "test_QP45_QP39 = second_QP45_QP39 + single_QP39\n",
    "test_QP45_QP42 = second_QP45_QP42 + single_QP42\n",
    "test_QP50_QP5 = second_QP50_QP5 + single_QP5\n",
    "test_QP50_QP10 = second_QP50_QP10 + single_QP10\n",
    "test_QP50_QP16 = second_QP50_QP16 + single_QP16\n",
    "test_QP50_QP20 = second_QP50_QP20 + single_QP20\n",
    "test_QP50_QP24 = second_QP50_QP24 + single_QP24\n",
    "test_QP50_QP27 = second_QP50_QP27 + single_QP27\n",
    "test_QP50_QP32 = second_QP50_QP32 + single_QP32\n",
    "test_QP50_QP39 = second_QP50_QP39 + single_QP39\n",
    "test_QP50_QP42 = second_QP50_QP42 + single_QP42\n",
    "test_QP50_QP45 = second_QP50_QP45 + single_QP45\n",
    "\n",
    "print('test_QP50_QP45: ', len(test_QP50_QP45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP45_QP45:  60\n"
     ]
    }
   ],
   "source": [
    "test_QP5_QP5 = second_QP5_QP5 + single_QP5\n",
    "test_QP10_QP10 = second_QP10_QP10 + single_QP10\n",
    "test_QP16_QP16 = second_QP16_QP16 + single_QP16\n",
    "test_QP20_QP20 = second_QP20_QP20 + single_QP20\n",
    "test_QP24_QP24 = second_QP24_QP24 + single_QP24\n",
    "test_QP27_QP27 = second_QP27_QP27 + single_QP27\n",
    "test_QP32_QP32 = second_QP32_QP32 + single_QP32\n",
    "test_QP39_QP39 = second_QP39_QP39 + single_QP39\n",
    "test_QP42_QP42 = second_QP42_QP42 + single_QP42\n",
    "test_QP45_QP45 = second_QP45_QP45 + single_QP45\n",
    "\n",
    "print('test_QP45_QP45: ', len(test_QP45_QP45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP40_QP45:  60\n"
     ]
    }
   ],
   "source": [
    "test_QP10_QP16 = second_QP10_QP16 + single_QP16\n",
    "test_QP10_QP20 = second_QP10_QP20 + single_QP20\n",
    "test_QP10_QP24 = second_QP10_QP24 + single_QP24\n",
    "test_QP10_QP27 = second_QP10_QP27 + single_QP27\n",
    "test_QP10_QP32 = second_QP10_QP32 + single_QP32\n",
    "test_QP10_QP39 = second_QP10_QP39 + single_QP39\n",
    "test_QP10_QP42 = second_QP10_QP42 + single_QP42\n",
    "test_QP10_QP45 = second_QP10_QP45 + single_QP45\n",
    "test_QP15_QP16 = second_QP15_QP16 + single_QP16\n",
    "test_QP15_QP20 = second_QP15_QP20 + single_QP20\n",
    "test_QP15_QP24 = second_QP15_QP24 + single_QP24\n",
    "test_QP15_QP27 = second_QP15_QP27 + single_QP27\n",
    "test_QP15_QP32 = second_QP15_QP32 + single_QP32\n",
    "test_QP15_QP39 = second_QP15_QP39 + single_QP39\n",
    "test_QP15_QP42 = second_QP15_QP42 + single_QP42\n",
    "test_QP15_QP45 = second_QP15_QP45 + single_QP45\n",
    "test_QP20_QP24 = second_QP20_QP24 + single_QP24\n",
    "test_QP20_QP27 = second_QP20_QP27 + single_QP27\n",
    "test_QP20_QP32 = second_QP20_QP32 + single_QP32\n",
    "test_QP20_QP39 = second_QP20_QP39 + single_QP39\n",
    "test_QP20_QP42 = second_QP20_QP42 + single_QP42\n",
    "test_QP20_QP45 = second_QP20_QP45 + single_QP45\n",
    "test_QP25_QP27 = second_QP25_QP27 + single_QP27\n",
    "test_QP25_QP32 = second_QP25_QP32 + single_QP32\n",
    "test_QP25_QP39 = second_QP25_QP39 + single_QP39\n",
    "test_QP25_QP42 = second_QP25_QP42 + single_QP42\n",
    "test_QP25_QP45 = second_QP25_QP45 + single_QP45\n",
    "test_QP30_QP32 = second_QP30_QP32 + single_QP32\n",
    "test_QP30_QP39 = second_QP30_QP39 + single_QP39\n",
    "test_QP30_QP42 = second_QP30_QP42 + single_QP42\n",
    "test_QP30_QP45 = second_QP30_QP45 + single_QP45\n",
    "test_QP32_QP39 = second_QP32_QP39 + single_QP39\n",
    "test_QP32_QP42 = second_QP32_QP42 + single_QP42\n",
    "test_QP32_QP45 = second_QP32_QP45 + single_QP45\n",
    "test_QP35_QP39 = second_QP35_QP39 + single_QP39\n",
    "test_QP35_QP42 = second_QP35_QP42 + single_QP42\n",
    "test_QP35_QP45 = second_QP35_QP45 + single_QP45\n",
    "test_QP40_QP42 = second_QP40_QP42 + single_QP42\n",
    "test_QP40_QP45 = second_QP40_QP45 + single_QP45\n",
    "\n",
    "\n",
    "print('test_QP40_QP45: ', len(test_QP40_QP45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_OG = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1番目のCSVファイルを処理する\n",
    "test_df1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_QP10_QP5)\n",
    "\n",
    "# 2番目のCSVファイルを処理する\n",
    "test_df2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_QP15_QP5)\n",
    "\n",
    "# 3番目のCSVファイルを処理する\n",
    "test_df3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_QP15_QP10)\n",
    "\n",
    "# 4番目のCSVファイルを処理する\n",
    "test_df4, LABEL_t4, MAE_t4, FINAL_QP_t4 = process_train_csv_lists(test_QP20_QP5)\n",
    "\n",
    "# 5番目のCSVファイルを処理する\n",
    "test_df5, LABEL_t5, MAE_t5, FINAL_QP_t5 = process_train_csv_lists(test_QP20_QP10)\n",
    "\n",
    "# 6番目のCSVファイルを処理する\n",
    "test_df6, LABEL_t6, MAE_t6, FINAL_QP_t6 = process_train_csv_lists(test_QP20_QP16)\n",
    "\n",
    "# 7番目のCSVファイルを処理する\n",
    "test_df7, LABEL_t7, MAE_t7, FINAL_QP_t7 = process_train_csv_lists(test_QP25_QP5)\n",
    "\n",
    "# 8番目のCSVファイルを処理する\n",
    "test_df8, LABEL_t8, MAE_t8, FINAL_QP_t8 = process_train_csv_lists(test_QP25_QP10)\n",
    "\n",
    "# 9番目のCSVファイルを処理する\n",
    "test_df9, LABEL_t9, MAE_t9, FINAL_QP_t9 = process_train_csv_lists(test_QP25_QP16)\n",
    "\n",
    "# 10番目のCSVファイルを処理する\n",
    "test_df10, LABEL_t10, MAE_t10, FINAL_QP_t10 = process_train_csv_lists(test_QP25_QP20)\n",
    "\n",
    "# 11番目のCSVファイルを処理する\n",
    "test_df11, LABEL_t11, MAE_t11, FINAL_QP_t11 = process_train_csv_lists(test_QP25_QP24)\n",
    "\n",
    "# 12番目のCSVファイルを処理する\n",
    "test_df12, LABEL_t12, MAE_t12, FINAL_QP_t12 = process_train_csv_lists(test_QP30_QP5)\n",
    "\n",
    "# 13番目のCSVファイルを処理する\n",
    "test_df13, LABEL_t13, MAE_t13, FINAL_QP_t13 = process_train_csv_lists(test_QP30_QP10)\n",
    "\n",
    "# 14番目のCSVファイルを処理する\n",
    "test_df14, LABEL_t14, MAE_t14, FINAL_QP_t14 = process_train_csv_lists(test_QP30_QP16)\n",
    "\n",
    "# 15番目のCSVファイルを処理する\n",
    "test_df15, LABEL_t15, MAE_t15, FINAL_QP_t15 = process_train_csv_lists(test_QP30_QP20)\n",
    "\n",
    "# 16番目のCSVファイルを処理する\n",
    "test_df16, LABEL_t16, MAE_t16, FINAL_QP_t16 = process_train_csv_lists(test_QP30_QP24)\n",
    "\n",
    "# 17番目のCSVファイルを処理する\n",
    "test_df17, LABEL_t17, MAE_t17, FINAL_QP_t17 = process_train_csv_lists(test_QP30_QP27)\n",
    "\n",
    "# 18番目のCSVファイルを処理する\n",
    "test_df18, LABEL_t18, MAE_t18, FINAL_QP_t18 = process_train_csv_lists(test_QP32_QP5)\n",
    "\n",
    "# 19番目のCSVファイルを処理する\n",
    "test_df19, LABEL_t19, MAE_t19, FINAL_QP_t19 = process_train_csv_lists(test_QP32_QP10)\n",
    "\n",
    "# 20番目のCSVファイルを処理する\n",
    "test_df20, LABEL_t20, MAE_t20, FINAL_QP_t20 = process_train_csv_lists(test_QP32_QP16)\n",
    "\n",
    "# 21番目のCSVファイルを処理する\n",
    "test_df21, LABEL_t21, MAE_t21, FINAL_QP_t21 = process_train_csv_lists(test_QP32_QP20)\n",
    "\n",
    "# 22番目のCSVファイルを処理する\n",
    "test_df22, LABEL_t22, MAE_t22, FINAL_QP_t22 = process_train_csv_lists(test_QP32_QP24)\n",
    "\n",
    "# 23番目のCSVファイルを処理する\n",
    "test_df23, LABEL_t23, MAE_t23, FINAL_QP_t23 = process_train_csv_lists(test_QP32_QP27)\n",
    "\n",
    "# 24番目のCSVファイルを処理する\n",
    "test_df24, LABEL_t24, MAE_t24, FINAL_QP_t24 = process_train_csv_lists(test_QP35_QP5)\n",
    "\n",
    "# 25番目のCSVファイルを処理する\n",
    "test_df25, LABEL_t25, MAE_t25, FINAL_QP_t25 = process_train_csv_lists(test_QP35_QP10)\n",
    "\n",
    "# 26番目のCSVファイルを処理する\n",
    "test_df26, LABEL_t26, MAE_t26, FINAL_QP_t26 = process_train_csv_lists(test_QP35_QP16)\n",
    "\n",
    "# 27番目のCSVファイルを処理する\n",
    "test_df27, LABEL_t27, MAE_t27, FINAL_QP_t27 = process_train_csv_lists(test_QP35_QP20)\n",
    "\n",
    "# 28番目のCSVファイルを処理する\n",
    "test_df28, LABEL_t28, MAE_t28, FINAL_QP_t28 = process_train_csv_lists(test_QP35_QP24)\n",
    "\n",
    "# 29番目のCSVファイルを処理する\n",
    "test_df29, LABEL_t29, MAE_t29, FINAL_QP_t29 = process_train_csv_lists(test_QP35_QP27)\n",
    "\n",
    "# 30番目のCSVファイルを処理する\n",
    "test_df30, LABEL_t30, MAE_t30, FINAL_QP_t30 = process_train_csv_lists(test_QP35_QP32)\n",
    "\n",
    "# 31番目のCSVファイルを処理する\n",
    "test_df31, LABEL_t31, MAE_t31, FINAL_QP_t31 = process_train_csv_lists(test_QP40_QP5)\n",
    "\n",
    "# 32番目のCSVファイルを処理する\n",
    "test_df32, LABEL_t32, MAE_t32, FINAL_QP_t32 = process_train_csv_lists(test_QP40_QP10)\n",
    "\n",
    "# 33番目のCSVファイルを処理する\n",
    "test_df33, LABEL_t33, MAE_t33, FINAL_QP_t33 = process_train_csv_lists(test_QP40_QP16)\n",
    "\n",
    "# 34番目のCSVファイルを処理する\n",
    "test_df34, LABEL_t34, MAE_t34, FINAL_QP_t34 = process_train_csv_lists(test_QP40_QP20)\n",
    "\n",
    "# 35番目のCSVファイルを処理する\n",
    "test_df35, LABEL_t35, MAE_t35, FINAL_QP_t35 = process_train_csv_lists(test_QP40_QP24)\n",
    "\n",
    "# 36番目のCSVファイルを処理する\n",
    "test_df36, LABEL_t36, MAE_t36, FINAL_QP_t36 = process_train_csv_lists(test_QP40_QP27)\n",
    "\n",
    "# 37番目のCSVファイルを処理する\n",
    "test_df37, LABEL_t37, MAE_t37, FINAL_QP_t37 = process_train_csv_lists(test_QP40_QP32)\n",
    "\n",
    "# 38番目のCSVファイルを処理する\n",
    "test_df38, LABEL_t38, MAE_t38, FINAL_QP_t38 = process_train_csv_lists(test_QP40_QP39)\n",
    "\n",
    "# 39番目のCSVファイルを処理する\n",
    "test_df39, LABEL_t39, MAE_t39, FINAL_QP_t39 = process_train_csv_lists(test_QP45_QP5)\n",
    "\n",
    "# 40番目のCSVファイルを処理する\n",
    "test_df40, LABEL_t40, MAE_t40, FINAL_QP_t40 = process_train_csv_lists(test_QP45_QP10)\n",
    "\n",
    "# 41番目のCSVファイルを処理する\n",
    "test_df41, LABEL_t41, MAE_t41, FINAL_QP_t41 = process_train_csv_lists(test_QP45_QP16)\n",
    "\n",
    "# 42番目のCSVファイルを処理する\n",
    "test_df42, LABEL_t42, MAE_t42, FINAL_QP_t42 = process_train_csv_lists(test_QP45_QP20)\n",
    "\n",
    "# 43番目のCSVファイルを処理する\n",
    "test_df43, LABEL_t43, MAE_t43, FINAL_QP_t43 = process_train_csv_lists(test_QP45_QP24)\n",
    "\n",
    "# 44番目のCSVファイルを処理する\n",
    "test_df44, LABEL_t44, MAE_t44, FINAL_QP_t44 = process_train_csv_lists(test_QP45_QP27)\n",
    "\n",
    "# 45番目のCSVファイルを処理する\n",
    "test_df45, LABEL_t45, MAE_t45, FINAL_QP_t45 = process_train_csv_lists(test_QP45_QP32)\n",
    "\n",
    "# 46番目のCSVファイルを処理する\n",
    "test_df46, LABEL_t46, MAE_t46, FINAL_QP_t46 = process_train_csv_lists(test_QP45_QP39)\n",
    "\n",
    "# 47番目のCSVファイルを処理する\n",
    "test_df47, LABEL_t47, MAE_t47, FINAL_QP_t47 = process_train_csv_lists(test_QP45_QP42)\n",
    "\n",
    "# 48番目のCSVファイルを処理する\n",
    "test_df48, LABEL_t48, MAE_t48, FINAL_QP_t48 = process_train_csv_lists(test_QP50_QP5)\n",
    "\n",
    "# 49番目のCSVファイルを処理する\n",
    "test_df49, LABEL_t49, MAE_t49, FINAL_QP_t49 = process_train_csv_lists(test_QP50_QP10)\n",
    "\n",
    "# 50番目のCSVファイルを処理する\n",
    "test_df50, LABEL_t50, MAE_t50, FINAL_QP_t50 = process_train_csv_lists(test_QP50_QP16)\n",
    "\n",
    "# 51番目のCSVファイルを処理する\n",
    "test_df51, LABEL_t51, MAE_t51, FINAL_QP_t51 = process_train_csv_lists(test_QP50_QP20)\n",
    "\n",
    "# 52番目のCSVファイルを処理する\n",
    "test_df52, LABEL_t52, MAE_t52, FINAL_QP_t52 = process_train_csv_lists(test_QP50_QP24)\n",
    "\n",
    "# 53番目のCSVファイルを処理する\n",
    "test_df53, LABEL_t53, MAE_t53, FINAL_QP_t53 = process_train_csv_lists(test_QP50_QP27)\n",
    "\n",
    "# 54番目のCSVファイルを処理する\n",
    "test_df54, LABEL_t54, MAE_t54, FINAL_QP_t54 = process_train_csv_lists(test_QP50_QP32)\n",
    "\n",
    "# 55番目のCSVファイルを処理する\n",
    "test_df55, LABEL_t55, MAE_t55, FINAL_QP_t55 = process_train_csv_lists(test_QP50_QP39)\n",
    "\n",
    "# 56番目のCSVファイルを処理する\n",
    "test_df56, LABEL_t56, MAE_t56, FINAL_QP_t56 = process_train_csv_lists(test_QP50_QP42)\n",
    "\n",
    "# 57番目のCSVファイルを処理する\n",
    "test_df57, LABEL_t57, MAE_t57, FINAL_QP_t57 = process_train_csv_lists(test_QP50_QP45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58番目のCSVファイルを処理する\n",
    "test_df58, LABEL_t58, MAE_t58, FINAL_QP_t58 = process_train_csv_lists(test_QP5_QP5)\n",
    "\n",
    "# 59番目のCSVファイルを処理する\n",
    "test_df59, LABEL_t59, MAE_t59, FINAL_QP_t59 = process_train_csv_lists(test_QP10_QP10)\n",
    "\n",
    "# 60番目のCSVファイルを処理する\n",
    "test_df60, LABEL_t60, MAE_t60, FINAL_QP_t60 = process_train_csv_lists(test_QP16_QP16)\n",
    "\n",
    "# 61番目のCSVファイルを処理する\n",
    "test_df61, LABEL_t61, MAE_t61, FINAL_QP_t61 = process_train_csv_lists(test_QP20_QP20)\n",
    "\n",
    "# 62番目のCSVファイルを処理する\n",
    "test_df62, LABEL_t62, MAE_t62, FINAL_QP_t62 = process_train_csv_lists(test_QP24_QP24)\n",
    "\n",
    "# 63番目のCSVファイルを処理する\n",
    "test_df63, LABEL_t63, MAE_t63, FINAL_QP_t63 = process_train_csv_lists(test_QP27_QP27)\n",
    "\n",
    "# 64番目のCSVファイルを処理する\n",
    "test_df64, LABEL_t64, MAE_t64, FINAL_QP_t64 = process_train_csv_lists(test_QP32_QP32)\n",
    "\n",
    "# 65番目のCSVファイルを処理する\n",
    "test_df65, LABEL_t65, MAE_t65, FINAL_QP_t65 = process_train_csv_lists(test_QP39_QP39)\n",
    "\n",
    "# 66番目のCSVファイルを処理する\n",
    "test_df66, LABEL_t66, MAE_t66, FINAL_QP_t66 = process_train_csv_lists(test_QP42_QP42)\n",
    "\n",
    "# 67番目のCSVファイルを処理する\n",
    "test_df67, LABEL_t67, MAE_t67, FINAL_QP_t67 = process_train_csv_lists(test_QP45_QP45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68番目のCSVファイルを処理する\n",
    "test_df68, LABEL_t68, MAE_t68, FINAL_QP_t68 = process_train_csv_lists(test_QP10_QP16)\n",
    "\n",
    "# 69番目のCSVファイルを処理する\n",
    "test_df69, LABEL_t69, MAE_t69, FINAL_QP_t69 = process_train_csv_lists(test_QP10_QP20)\n",
    "\n",
    "# 70番目のCSVファイルを処理する\n",
    "test_df70, LABEL_t70, MAE_t70, FINAL_QP_t70 = process_train_csv_lists(test_QP10_QP24)\n",
    "\n",
    "# 71番目のCSVファイルを処理する\n",
    "test_df71, LABEL_t71, MAE_t71, FINAL_QP_t71 = process_train_csv_lists(test_QP10_QP27)\n",
    "\n",
    "# 72番目のCSVファイルを処理する\n",
    "test_df72, LABEL_t72, MAE_t72, FINAL_QP_t72 = process_train_csv_lists(test_QP10_QP32)\n",
    "\n",
    "# 73番目のCSVファイルを処理する\n",
    "test_df73, LABEL_t73, MAE_t73, FINAL_QP_t73 = process_train_csv_lists(test_QP10_QP39)\n",
    "\n",
    "# 74番目のCSVファイルを処理する\n",
    "test_df74, LABEL_t74, MAE_t74, FINAL_QP_t74 = process_train_csv_lists(test_QP10_QP42)\n",
    "\n",
    "# 75番目のCSVファイルを処理する\n",
    "test_df75, LABEL_t75, MAE_t75, FINAL_QP_t75 = process_train_csv_lists(test_QP10_QP45)\n",
    "\n",
    "# 76番目のCSVファイルを処理する\n",
    "test_df76, LABEL_t76, MAE_t76, FINAL_QP_t76 = process_train_csv_lists(test_QP15_QP16)\n",
    "\n",
    "# 77番目のCSVファイルを処理する\n",
    "test_df77, LABEL_t77, MAE_t77, FINAL_QP_t77 = process_train_csv_lists(test_QP15_QP20)\n",
    "\n",
    "# 78番目のCSVファイルを処理する\n",
    "test_df78, LABEL_t78, MAE_t78, FINAL_QP_t78 = process_train_csv_lists(test_QP15_QP24)\n",
    "\n",
    "# 79番目のCSVファイルを処理する\n",
    "test_df79, LABEL_t79, MAE_t79, FINAL_QP_t79 = process_train_csv_lists(test_QP15_QP27)\n",
    "\n",
    "# 80番目のCSVファイルを処理する\n",
    "test_df80, LABEL_t80, MAE_t80, FINAL_QP_t80 = process_train_csv_lists(test_QP15_QP32)\n",
    "\n",
    "# 81番目のCSVファイルを処理する\n",
    "test_df81, LABEL_t81, MAE_t81, FINAL_QP_t81 = process_train_csv_lists(test_QP15_QP39)\n",
    "\n",
    "# 82番目のCSVファイルを処理する\n",
    "test_df82, LABEL_t82, MAE_t82, FINAL_QP_t82 = process_train_csv_lists(test_QP15_QP42)\n",
    "\n",
    "# 83番目のCSVファイルを処理する\n",
    "test_df83, LABEL_t83, MAE_t83, FINAL_QP_t83 = process_train_csv_lists(test_QP15_QP45)\n",
    "\n",
    "# 84番目のCSVファイルを処理する\n",
    "test_df84, LABEL_t84, MAE_t84, FINAL_QP_t84 = process_train_csv_lists(test_QP20_QP24)\n",
    "\n",
    "# 85番目のCSVファイルを処理する\n",
    "test_df85, LABEL_t85, MAE_t85, FINAL_QP_t85 = process_train_csv_lists(test_QP20_QP27)\n",
    "\n",
    "# 86番目のCSVファイルを処理する\n",
    "test_df86, LABEL_t86, MAE_t86, FINAL_QP_t86 = process_train_csv_lists(test_QP20_QP32)\n",
    "\n",
    "# 87番目のCSVファイルを処理する\n",
    "test_df87, LABEL_t87, MAE_t87, FINAL_QP_t87 = process_train_csv_lists(test_QP20_QP39)\n",
    "\n",
    "# 88番目のCSVファイルを処理する\n",
    "test_df88, LABEL_t88, MAE_t88, FINAL_QP_t88 = process_train_csv_lists(test_QP20_QP42)\n",
    "\n",
    "# 89番目のCSVファイルを処理する\n",
    "test_df89, LABEL_t89, MAE_t89, FINAL_QP_t89 = process_train_csv_lists(test_QP20_QP45)\n",
    "\n",
    "# 90番目のCSVファイルを処理する\n",
    "test_df90, LABEL_t90, MAE_t90, FINAL_QP_t90 = process_train_csv_lists(test_QP25_QP27)\n",
    "\n",
    "# 91番目のCSVファイルを処理する\n",
    "test_df91, LABEL_t91, MAE_t91, FINAL_QP_t91 = process_train_csv_lists(test_QP25_QP32)\n",
    "\n",
    "# 92番目のCSVファイルを処理する\n",
    "test_df92, LABEL_t92, MAE_t92, FINAL_QP_t92 = process_train_csv_lists(test_QP25_QP39)\n",
    "\n",
    "# 93番目のCSVファイルを処理する\n",
    "test_df93, LABEL_t93, MAE_t93, FINAL_QP_t93 = process_train_csv_lists(test_QP25_QP42)\n",
    "\n",
    "# 94番目のCSVファイルを処理する\n",
    "test_df94, LABEL_t94, MAE_t94, FINAL_QP_t94 = process_train_csv_lists(test_QP25_QP45)\n",
    "\n",
    "# 95番目のCSVファイルを処理する\n",
    "test_df95, LABEL_t95, MAE_t95, FINAL_QP_t95 = process_train_csv_lists(test_QP30_QP32)\n",
    "\n",
    "# 96番目のCSVファイルを処理する\n",
    "test_df96, LABEL_t96, MAE_t96, FINAL_QP_t96 = process_train_csv_lists(test_QP30_QP39)\n",
    "\n",
    "# 97番目のCSVファイルを処理する\n",
    "test_df97, LABEL_t97, MAE_t97, FINAL_QP_t97 = process_train_csv_lists(test_QP30_QP42)\n",
    "\n",
    "# 98番目のCSVファイルを処理する\n",
    "test_df98, LABEL_t98, MAE_t98, FINAL_QP_t98 = process_train_csv_lists(test_QP30_QP45)\n",
    "\n",
    "# 99番目のCSVファイルを処理する\n",
    "test_df99, LABEL_t99, MAE_t99, FINAL_QP_t99 = process_train_csv_lists(test_QP32_QP39)\n",
    "\n",
    "# 100番目のCSVファイルを処理する\n",
    "test_df100, LABEL_t100, MAE_t100, FINAL_QP_t100 = process_train_csv_lists(test_QP32_QP42)\n",
    "\n",
    "# 101番目のCSVファイルを処理する\n",
    "test_df101, LABEL_t101, MAE_t101, FINAL_QP_t101 = process_train_csv_lists(test_QP32_QP45)\n",
    "\n",
    "# 102番目のCSVファイルを処理する\n",
    "test_df102, LABEL_t102, MAE_t102, FINAL_QP_t102 = process_train_csv_lists(test_QP35_QP39)\n",
    "\n",
    "# 103番目のCSVファイルを処理する\n",
    "test_df103, LABEL_t103, MAE_t103, FINAL_QP_t103 = process_train_csv_lists(test_QP35_QP42)\n",
    "\n",
    "# 104番目のCSVファイルを処理する\n",
    "test_df104, LABEL_t104, MAE_t104, FINAL_QP_t104 = process_train_csv_lists(test_QP35_QP45)\n",
    "\n",
    "# 105番目のCSVファイルを処理する\n",
    "test_df105, LABEL_t105, MAE_t105, FINAL_QP_t105 = process_train_csv_lists(test_QP40_QP42)\n",
    "\n",
    "# 106番目のCSVファイルを処理する\n",
    "test_df106, LABEL_t106, MAE_t106, FINAL_QP_t106 = process_train_csv_lists(test_QP40_QP45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データフレームを結合\n",
    "combined_train_df = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9], ignore_index=True)\n",
    "combined_LABEL = pd.concat([LABEL1, LABEL2, LABEL3, LABEL4, LABEL5, LABEL6, LABEL7, LABEL8, LABEL9], ignore_index=True)\n",
    "combined_MAE = pd.concat([MAE1, MAE2, MAE3, MAE4, MAE5, MAE6, MAE7, MAE8, MAE9], ignore_index=True)\n",
    "combined_FINAL_QP = pd.concat([FINAL_QP1, FINAL_QP2, FINAL_QP3, FINAL_QP4, FINAL_QP5, FINAL_QP6, FINAL_QP7, FINAL_QP8, FINAL_QP9], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 44)\n",
      "(5400, 1)\n",
      "(5400, 1)\n",
      "(5400, 1)\n",
      "(60, 44)\n",
      "(60, 1)\n",
      "(60, 1)\n",
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df.shape)\n",
    "print(combined_LABEL.shape)\n",
    "print(combined_MAE.shape)\n",
    "print(combined_FINAL_QP.shape)\n",
    "\n",
    "print(test_df1.shape)\n",
    "print(LABEL_t1.shape)\n",
    "print(MAE_t1.shape)\n",
    "print(FINAL_QP_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "Combined Train DF:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0 LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0 LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10      0  16128   8736  12300  22836      0  15936   8896  12060  23108   6663  9203  3432   7812   2148   1579   2477   1189   6439  9546  3236   8395   2066   1704   2442   1167  12964  13928   6084   5968   2396  18660  13560  14384   6088   6460   2200  17308   0.00011  0.001157   0.001663  0.115798  0.085424\n",
      "1       16      0  19456  11664  11696  17184      0  19200  12128  11240  17432   8238  7578  3386   4696   2468   1298   2443   1336   8275  8343  3416   5005   2473   1497   2470   1245  10112   6016   5104   4720   1576  32472   9480   6316   4972   4800   1532  32900   0.00036  0.001208   0.000561  0.141032  0.048257\n",
      "2       20      0  20928  12608  12292  14172      0  20928  12736  12052  14284  10782  6806  2971   3888   2500   1235   2700   1102  10500  7104  2792   4437   2276   1208   2845   1105   8724   4540   3516   3212   1856  38152   8548   4520   3760   3536   1752  37884  0.000058  0.002021   0.000487  0.060082   0.04064\n",
      "3       24      0  23488  12304  12076  12132      0  22976  12800  12032  12192   9895  5634  2813   4937   2378   1334   2815   1194   9815  6261  3009   4954   2370   1383   2949   1132   7264   1740   2876   1980   1044  45096   7136   1904   3200   1928    848  44984   0.00026  0.001063   0.000788  0.038557   0.01131\n",
      "4       27      0  25984  11600  11788  10628      0  26048  11728  11528  10696  11256  5506  2895   4605   2503   1052   3559   1001  11483  6313  2823   4986   2799   1118   3365    930   4856   1136   1676   1620    668  50044   4748   1608   1128   1436    560  50520  0.000065  0.002077   0.003622  0.198049  0.127488\n",
      "\n",
      "Before scaling:\n",
      "Combined Test DF:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4 LU1_0 LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27 LU2_0 LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0        5      0   7936   5776  16572  29716      0   7872   5808  14720  31600  6593  7496  5557   8741   4858    300    548    293  6796  8159  5578   8703   4800    276    499    218  12068  12192  15444   5164   1788  13344  12080  11976  16108   5320   1568  12948  0.002826  0.001169   0.000648  0.598233  0.413988\n",
      "1        5      0    128   3376  21716  34780      0    128   3328  20292  36252  7220  4893  3215   2641   3258   3117   8172   3505  7334  5117  3236   2630   3166   3067   8341   3345  12900   8880   9936  11820   1740  14724  12992   8824  10112  11964   1740  14368  0.001324  0.000348   0.000121  0.404378   0.30363\n",
      "2        5      0    320   3056  17492  39132      0    320   3104  15852  40724  5755  6413   782    833    475   1186   1159    773  5842  6793   848    817    430   1146   1096    754  14996  10572   6796   9860   1436  16340  14876  11040   6892  10188   1304  15700  0.001899  0.000907   0.000598   0.41939  0.277129\n",
      "3        5      0   3456   4688  17712  34144      0   3392   4720  16596  35292  8355  8892  3017   3062   1934    597    777    511  8555  8862  2945   3170   2061    555    850    557  14412  11268  12896   7088   1904  12432  14240  12292  13640   7124   1616  11088  0.000938  0.000471   0.002804  0.892552  0.682752\n",
      "4        5      0   5888   8352  17444  28316      0   5760   8464  16036  29740  9600  9006  1880   2801   1547   1822   5043   1912  9608  9069  2044   2790   1421   2166   4717   1964  13128  11764   8876  10088   2264  13880  13044  12384   9112  10512   1924  13024  0.001615  0.001564   0.001395  0.661367  0.437449\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\")\n",
    "print(\"Combined Train DF:\")\n",
    "print(combined_train_df.head())\n",
    "\n",
    "print(\"\\nBefore scaling:\")\n",
    "print(\"Combined Test DF:\")\n",
    "print(test_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling:\n",
      "X_train:\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.125  0.0  0.273913  0.297652  0.419853  0.408311  0.0  0.270358  0.279738  0.411660  0.406202  0.112544  0.218073  0.259147  0.394324  0.175971  0.107044  0.077524  0.196822  0.111877  0.223266  0.249055  0.422541  0.171977  0.114879  0.076539  0.169229  0.319059  0.563339  0.333187  0.364703  0.219253  0.273284  0.332223  0.568538  0.317480  0.364807  0.225410  0.250702  0.004194  0.006644  0.016987  0.113978  0.084080\n",
      "1  0.275  0.0  0.330435  0.397597  0.399235  0.307252  0.0  0.325733  0.381552  0.383670  0.306427  0.164755  0.168978  0.255648  0.236958  0.202633  0.087994  0.076311  0.221155  0.177797  0.186698  0.262947  0.250747  0.205922  0.100924  0.077538  0.180539  0.248868  0.243326  0.279518  0.288438  0.144217  0.516179  0.232262  0.249644  0.259282  0.271064  0.156967  0.524361  0.013763  0.006939  0.005697  0.139266  0.046854\n",
      "2  0.375  0.0  0.355435  0.429820  0.419579  0.253397  0.0  0.355049  0.400706  0.411387  0.251090  0.249088  0.145654  0.224082  0.196152  0.205299  0.083723  0.085484  0.182420  0.257683  0.149036  0.214787  0.221963  0.189491  0.081440  0.090919  0.160238  0.214708  0.183627  0.192552  0.196285  0.169839  0.616066  0.209428  0.178656  0.196078  0.199684  0.179508  0.611837  0.002205  0.011627  0.004936  0.058144  0.039225\n",
      "3  0.475  0.0  0.398913  0.419443  0.412206  0.216922  0.0  0.389794  0.402722  0.410705  0.214316  0.219684  0.110245  0.212064  0.249129  0.195134  0.090435  0.089588  0.197649  0.233089  0.123412  0.231535  0.248163  0.197331  0.093238  0.094630  0.164153  0.178775  0.070377  0.157503  0.120997  0.095534  0.738182  0.174833  0.075257  0.166875  0.108877  0.086885  0.736450  0.009962  0.006103  0.008021  0.036574  0.009849\n",
      "4  0.550  0.0  0.441304  0.395412  0.402376  0.190030  0.0  0.441911  0.368952  0.393501  0.188019  0.264801  0.106378  0.218301  0.232362  0.205549  0.071317  0.116144  0.165701  0.292977  0.124992  0.217180  0.249785  0.233111  0.075372  0.109474  0.134861  0.119512  0.045947  0.091785  0.098998  0.061127  0.825197  0.116327  0.063557  0.058824  0.081093  0.057377  0.833614  0.002490  0.011952  0.037060  0.196403  0.126211\n",
      "\n",
      "Test data after scaling (test_df1):\n",
      "    0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.0  0.0  0.134783  0.196614  0.565674  0.531326  0.0  0.133550  0.182460  0.502458  0.555477  0.110223  0.166500  0.420780  0.441240  0.401766  0.020338  0.008673  0.048502  0.124695  0.181105  0.429806  0.438149  0.400000  0.018607  0.007208  0.031613  0.297007  0.493124  0.845783  0.315571  0.163616  0.179797  0.295962  0.473360  0.840008  0.300429  0.160656  0.174179  0.108091  0.006716  0.006584  0.597435  0.413167\n",
      "1  0.0  0.0  0.002174  0.114691  0.741262  0.621871  0.0  0.002172  0.104335  0.692654  0.637252  0.131008  0.087858  0.242641  0.133175  0.268455  0.211308  0.280794  0.580202  0.144011  0.088638  0.249055  0.130391  0.263720  0.206769  0.287029  0.485064  0.317484  0.359165  0.544140  0.722317  0.159224  0.204066  0.318307  0.348775  0.527326  0.675627  0.178279  0.199101  0.050665  0.001981  0.001190  0.403169  0.302633\n",
      "2  0.0  0.0  0.005435  0.103768  0.597078  0.699685  0.0  0.005429  0.097278  0.541098  0.715863  0.082444  0.133780  0.057580  0.041867  0.036577  0.080401  0.030481  0.127959  0.090442  0.139583  0.064753  0.038514  0.035530  0.077260  0.028510  0.109339  0.369069  0.427601  0.372180  0.602542  0.131406  0.232485  0.364465  0.436364  0.359408  0.575333  0.133607  0.222480  0.072643  0.005205  0.006074  0.418213  0.276090\n",
      "3  0.0  0.0  0.058696  0.159476  0.604588  0.610499  0.0  0.057546  0.148185  0.566494  0.620377  0.168634  0.208677  0.227580  0.154437  0.158140  0.040472  0.016847  0.084589  0.187850  0.202474  0.226596  0.157756  0.171560  0.037417  0.019732  0.080771  0.354696  0.455751  0.706243  0.433146  0.174231  0.163759  0.348883  0.485850  0.711306  0.402304  0.165574  0.141533  0.035886  0.002691  0.028683  0.892377  0.682359\n",
      "4  0.0  0.0  0.100000  0.284544  0.595440  0.506294  0.0  0.097720  0.266129  0.547378  0.522782  0.209905  0.212121  0.141097  0.141255  0.125896  0.123517  0.169112  0.316504  0.225657  0.208766  0.157058  0.138499  0.118182  0.146026  0.157716  0.284803  0.323095  0.475813  0.486090  0.616475  0.207174  0.189223  0.319581  0.489486  0.475177  0.593630  0.197131  0.175512  0.061760  0.008995  0.014246  0.660702  0.436665\n"
     ]
    }
   ],
   "source": [
    "def process_results_to_lists(train_df, LABEL, MAE, FINAL_QP, scaler_main=None, fit_scaler=True):\n",
    "    if fit_scaler:\n",
    "        scaler_main = MinMaxScaler()\n",
    "        X_train = scaler_main.fit_transform(train_df)\n",
    "    else:\n",
    "        X_train = scaler_main.transform(train_df)\n",
    "\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "    Y_train = LABEL['LABEL'].astype(int).values\n",
    "\n",
    "    return X_train, MAE_array, FINAL_QP_array, Y_train, scaler_main\n",
    "\n",
    "# 訓練データのスケーリング\n",
    "X_train, MAE_array, FINAL_QP_array, Y_train, scaler_main = process_results_to_lists(\n",
    "    combined_train_df, combined_LABEL, combined_MAE, combined_FINAL_QP, fit_scaler=True\n",
    ")\n",
    "\n",
    "# スケーリング後のデータを表示（任意）\n",
    "print(\"After scaling:\")\n",
    "print(\"X_train:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "\n",
    "# データを元に戻すための関数\n",
    "def restore_data_to_original_order(data, original_lengths):\n",
    "    restored_data = []\n",
    "    start_index = 0\n",
    "    for length in original_lengths:\n",
    "        restored_data.append(data[start_index:start_index + length])\n",
    "        start_index += length\n",
    "    return restored_data\n",
    "\n",
    "# 元のデータフレームの長さ\n",
    "original_lengths = [len(train_df1), len(train_df2), len(train_df3), len(train_df4), len(train_df5), \n",
    "                    len(train_df6), len(train_df7), len(train_df8), len(train_df9)]\n",
    "\n",
    "# データを元の順序に戻す\n",
    "X_train_list = restore_data_to_original_order(X_train, original_lengths)\n",
    "MAE_list = restore_data_to_original_order(MAE_array, original_lengths)\n",
    "FINAL_QP_list = restore_data_to_original_order(FINAL_QP_array, original_lengths)\n",
    "Y_train_list = restore_data_to_original_order(Y_train, original_lengths)\n",
    "\n",
    "# テストデータのスケーリング関数\n",
    "def append_results_to_lists(train_df, LABEL, MAE, FINAL_QP, X_train_list, MAE_list, FINAL_QP_list, Y_train_list, scaler_main=None, fit_scaler=True):\n",
    "    X_train, MAE_array, FINAL_QP_array, Y_train, _ = process_results_to_lists(\n",
    "        train_df, LABEL, MAE, FINAL_QP, scaler_main, fit_scaler)\n",
    "    X_train_list.append(X_train)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "    return X_train_list, MAE_list, FINAL_QP_list, Y_train_list\n",
    "\n",
    "# テストデータ用の辞書を初期化\n",
    "X_test_dict = {}\n",
    "MAE_test_dict = {}\n",
    "FINAL_QP_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "# テストデータを処理して辞書に追加\n",
    "for i in range(1, 107):\n",
    "    test_df = globals()[f'test_df{i}']\n",
    "    LABEL_t = globals()[f'LABEL_t{i}']\n",
    "    MAE_t = globals()[f'MAE_t{i}']\n",
    "    FINAL_QP_t = globals()[f'FINAL_QP_t{i}']\n",
    "    \n",
    "    X_test_dict[i] = []\n",
    "    MAE_test_dict[i] = []\n",
    "    FINAL_QP_test_dict[i] = []\n",
    "    Y_test_dict[i] = []\n",
    "    \n",
    "    X_test_dict[i], MAE_test_dict[i], FINAL_QP_test_dict[i], Y_test_dict[i] = append_results_to_lists(\n",
    "        test_df, LABEL_t, MAE_t, FINAL_QP_t, X_test_dict[i], MAE_test_dict[i], FINAL_QP_test_dict[i], Y_test_dict[i], scaler_main, fit_scaler=False\n",
    "    )\n",
    "\n",
    "# 確認用の出力\n",
    "for i in range(1, 2):\n",
    "    print(f\"\\nTest data after scaling (test_df{i}):\")\n",
    "    print(pd.DataFrame(X_test_dict[i][0]).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 8]\n",
      "Test indices: [7]\n",
      "4800\n",
      "600\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8]\n",
      "Test indices: [1]\n",
      "4800\n",
      "600\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8]\n",
      "Test indices: [5]\n",
      "4800\n",
      "600\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8]\n",
      "Test indices: [0]\n",
      "4800\n",
      "600\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [8]\n",
      "4800\n",
      "600\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8]\n",
      "Test indices: [2]\n",
      "4800\n",
      "600\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 5 6 7 8]\n",
      "Test indices: [4]\n",
      "4800\n",
      "600\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 4 5 6 7 8]\n",
      "Test indices: [3]\n",
      "4800\n",
      "600\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 3 4 5 7 8]\n",
      "Test indices: [6]\n",
      "4800\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "kfold = KFold(n_splits=9, shuffle=True, random_state=42)\n",
    "\n",
    "# データフレームを初期化\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# 1から106までの列名を作成し、データフレームに追加\n",
    "columns = []\n",
    "for i in range(1, 107):\n",
    "    columns.extend([\n",
    "        f'C_RBF{i}', f'Score_RBF{i}', f'tnr_rbf{i}', f'tpr_rbf{i}', f'AUC_RBF{i}',\n",
    "        f'C_LINEAR{i}', f'Score_LINEAR{i}', f'tnr_linear{i}', f'tpr_linear{i}', f'AUC_LINEAR{i}',\n",
    "        f'Threshold{i}', f'Score_old{i}', f'tnr_old{i}', f'tpr_old{i}', f'AUC_old{i}'\n",
    "    ])\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "X_index = np.arange(9)  # インデックスとして0から8までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "        \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    print(len(Y_train))\n",
    "    print(len(Y_val))\n",
    "    \n",
    "    # リストの作成（1から106まで）\n",
    "    for i in range(1, 107):\n",
    "        globals()[f'test_data{i}'] = [item for data in X_test_dict[i] for item in data]\n",
    "        globals()[f'test_label{i}'] = [item for data in Y_test_dict[i] for item in data]\n",
    "        globals()[f'MAE_data{i}'] = [item for data in MAE_test_dict[i] for item in data]\n",
    "        globals()[f'FINAL_QP_data{i}'] = [item for data in FINAL_QP_test_dict[i] for item in data]\n",
    "\n",
    "        globals()[f'best_threshold{i}'] = 0\n",
    "        globals()[f'best_accuracy{i}'] = 0\n",
    "        globals()[f'best_predicted_labels{i}'] = []\n",
    "        globals()[f'best_ground_truth_labels{i}'] = []\n",
    "        globals()[f'tnr_old{i}'] = 0\n",
    "        globals()[f'tpr_old{i}'] = 0\n",
    "        \n",
    "        for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "            test_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(60)])\n",
    "            predicted_labels = test_old.astype(int)\n",
    "            ground_truth_labels = np.array(globals()[f'test_label{i}'])\n",
    "            accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "            if accuracy > globals()[f'best_accuracy{i}']:\n",
    "                globals()[f'best_accuracy{i}'] = accuracy\n",
    "                globals()[f'best_threshold{i}'] = threshold\n",
    "                globals()[f'best_predicted_labels{i}'] = predicted_labels\n",
    "                globals()[f'best_ground_truth_labels{i}'] = ground_truth_labels\n",
    "\n",
    "\n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value, probability=True)\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value, probability=True)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "    \n",
    "    fold_results = {}\n",
    "    for i in range(1, 107):\n",
    "        # RBFモデルの評価\n",
    "        predictions_RBF = best_svm_model_RBF.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_RBF = best_svm_model_RBF.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_RBF = accuracy_score(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'accuracy_RBF{i}'] = accuracy_RBF\n",
    "        report_RBF = classification_report(globals()[f'test_label{i}'], predictions_RBF, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'tnr_rbf{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_rbf{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_rbf, tpr_rbf, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_RBF)\n",
    "        auc_rbf = auc(fpr_rbf, tpr_rbf)\n",
    "        globals()[f'auc_rbf{i}'] = auc_rbf\n",
    "        # print(report_RBF)\n",
    "\n",
    "        # LINEARモデルの評価\n",
    "        predictions_LINEAR = best_svm_model_LINEAR.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_LINEAR = best_svm_model_LINEAR.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_LINEAR = accuracy_score(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'accuracy_LINEAR{i}'] = accuracy_LINEAR\n",
    "        report_LINEAR = classification_report(globals()[f'test_label{i}'], predictions_LINEAR, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'tnr_linear{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_linear{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_linear, tpr_linear, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_LINEAR)\n",
    "        auc_linear = auc(fpr_linear, tpr_linear)\n",
    "        globals()[f'auc_linear{i}'] = auc_linear\n",
    "\n",
    "        # Old modelの評価\n",
    "        thresholds = np.arange(0.00, 1.01, 0.01)\n",
    "        tpr_old_list = []\n",
    "        fpr_old_list = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(60)])\n",
    "            tn, fp, fn, tp = confusion_matrix(globals()[f'test_label{i}'], predicted_labels_old).ravel()\n",
    "            tpr_old = tp / (tp + fn)\n",
    "            fpr_old = fp / (fp + tn)\n",
    "            tpr_old_list.append(tpr_old)\n",
    "            fpr_old_list.append(fpr_old)\n",
    "        \n",
    "        auc_old = auc(fpr_old_list, tpr_old_list)\n",
    "        globals()[f'auc_old{i}'] = auc_old\n",
    "\n",
    "        # fold_resultsに保存\n",
    "        fold_results[f'C_RBF{i}'] = best_c_value_RBF\n",
    "        fold_results[f'Score_RBF{i}'] = globals()[f'accuracy_RBF{i}']\n",
    "        fold_results[f'tnr_rbf{i}'] = globals()[f'tnr_rbf{i}']\n",
    "        fold_results[f'tpr_rbf{i}'] = globals()[f'tpr_rbf{i}']\n",
    "        fold_results[f'AUC_RBF{i}'] = globals()[f'auc_rbf{i}']\n",
    "\n",
    "        fold_results[f'C_LINEAR{i}'] = best_c_value_LINEAR\n",
    "        fold_results[f'Score_LINEAR{i}'] = globals()[f'accuracy_LINEAR{i}']\n",
    "        fold_results[f'tnr_linear{i}'] = globals()[f'tnr_linear{i}']\n",
    "        fold_results[f'tpr_linear{i}'] = globals()[f'tpr_linear{i}']\n",
    "        fold_results[f'AUC_LINEAR{i}'] = globals()[f'auc_linear{i}']\n",
    "\n",
    "        fold_results[f'Threshold{i}'] = globals()[f'best_threshold{i}']\n",
    "        fold_results[f'Score_old{i}'] = globals()[f'best_accuracy{i}']\n",
    "        fold_results[f'tnr_old{i}'] = globals()[f'tnr_old{i}']\n",
    "        fold_results[f'tpr_old{i}'] = globals()[f'tpr_old{i}']\n",
    "        fold_results[f'AUC_old{i}'] = globals()[f'auc_old{i}']\n",
    "\n",
    "    # 結果をデータフレームに追加\n",
    "    results = pd.concat([results, pd.DataFrame(fold_results, index=[fold])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0      RBF1        98.89        67.04               82.96                3.51           86.67           75.00         0.96     0.01     0.98     0.93\n",
      "1      RBF2        98.89        93.70               96.30                1.39           98.33           93.33         1.00     0.00     1.00     0.99\n",
      "2      RBF3        92.22        71.48               81.85                2.12           86.67           80.00         0.91     0.01     0.93     0.89\n",
      "3      RBF4        98.89        94.81               96.85                1.30           98.33           95.00         0.98     0.01     0.99     0.97\n",
      "4      RBF5        92.22        92.96               92.59                3.02           96.67           88.33         0.97     0.02     0.99     0.95\n",
      "..      ...          ...          ...                 ...                 ...             ...             ...          ...      ...      ...      ...\n",
      "313  OLD102         0.00         0.00               61.67                0.00           61.67           61.67         0.62     0.00     0.62     0.62\n",
      "314  OLD103         0.00         0.00               58.33                0.00           58.33           58.33         0.59     0.00     0.59     0.59\n",
      "315  OLD104         0.00         0.00               60.00                0.00           60.00           60.00         0.59     0.00     0.59     0.59\n",
      "316  OLD105         0.00         0.00               70.00                0.00           70.00           70.00         0.73     0.00     0.73     0.73\n",
      "317  OLD106         0.00         0.00               63.33                0.00           63.33           63.33         0.65     0.00     0.65     0.65\n",
      "\n",
      "[318 rows x 11 columns]\n",
      "           Model  Average TNR  Average TPR  Average Test Score  Test Score STD  Test Score MAX  Test Score MIN  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0       RBF_1_57        87.39        95.91               91.65            1.19          100.00           71.67         0.98     0.01     1.00     0.85\n",
      "1      RBF_58_67        81.15        89.15               85.15            1.30          100.00           65.00         0.92     0.02     1.00     0.74\n",
      "2     RBF_68_106        72.18        52.04               62.11            1.04           95.00           43.33         0.67     0.01     0.99     0.46\n",
      "3    LINEAR_1_57        86.23        96.17               91.20            0.93          100.00           71.67         0.98     0.01     1.00     0.85\n",
      "4   LINEAR_58_67        79.41        87.93               83.67            1.40           96.67           68.33         0.91     0.01     1.00     0.78\n",
      "5  LINEAR_68_106        69.77        55.36               62.56            0.71           90.00           41.67         0.68     0.00     0.99     0.47\n",
      "6       OLD_1_57         0.00         0.00               94.27            0.00          100.00           51.67         0.94     0.00     1.00     0.38\n",
      "7      OLD_58_67         0.00         0.00               53.33            0.00           61.67           50.00         0.40     0.00     0.65     0.19\n",
      "8     OLD_68_106         0.00         0.00               56.79            0.00           70.00           51.67         0.54     0.00     0.73     0.32\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第2位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': [f'RBF{i}' for i in range(1, 107)] + [f'LINEAR{i}' for i in range(1, 107)] + [f'OLD{i}' for i in range(1, 107)],\n",
    "    'Average TNR': [\n",
    "        round(results[f'tnr_rbf{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tnr_linear{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tnr_old{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Average TPR': [\n",
    "        round(results[f'tpr_rbf{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tpr_linear{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'tpr_old{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Average Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].mean() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Standard Deviation': [\n",
    "        round(results[f'Score_RBF{i}'].std() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].std() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].std() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Max Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].max() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].max() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].max() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Min Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].min() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].min() * 100, 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].min() * 100, 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Average AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].mean(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].mean(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].mean(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'AUC STD': [\n",
    "        round(results[f'AUC_RBF{i}'].std(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].std(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].std(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Max AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].max(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].max(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].max(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "    'Min AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].min(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].min(), 2) for i in range(1, 107)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].min(), 2) for i in range(1, 107)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)\n",
    "\n",
    "\n",
    "\n",
    "# 関数を定義して、各セグメントの統計情報を計算\n",
    "def calculate_statistics(segment, prefix):\n",
    "    # モデル番号を抽出してフラットなリストに変換\n",
    "    model_numbers = statistics_df['Model'].str.extract(r'(\\d+)').astype(int)[0]\n",
    "    is_in_segment = model_numbers.isin(segment)\n",
    "    is_correct_prefix = statistics_df['Model'].str.startswith(prefix)\n",
    "    \n",
    "    tnr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TNR'].mean(), 2)\n",
    "    tpr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TPR'].mean(), 2)\n",
    "    acc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average Test Score'].mean(), 2)\n",
    "    acc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Standard Deviation'].std(), 2)\n",
    "    \n",
    "    acc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max Test Score'].max(), 2)\n",
    "    acc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min Test Score'].min(), 2)\n",
    "\n",
    "    auc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average AUC'].mean(), 2)\n",
    "    auc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'AUC STD'].std(), 2)\n",
    "    auc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max AUC'].max(), 2)\n",
    "    auc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min AUC'].min(), 2)\n",
    "    \n",
    "    return tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min\n",
    "\n",
    "# セグメントを定義\n",
    "segments = {\n",
    "    '1_57': list(range(1, 58)),\n",
    "    '58_67': list(range(58, 68)),\n",
    "    '68_106': list(range(68, 107))\n",
    "}\n",
    "\n",
    "# 結果を保存するリスト\n",
    "results_summary = []\n",
    "\n",
    "# 統計情報を計算して表示\n",
    "for model in ['RBF', 'LINEAR', 'OLD']:\n",
    "    for segment_name, segment in segments.items():\n",
    "        tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min = calculate_statistics(segment, model)\n",
    "        results_summary.append({\n",
    "            'Model': f'{model}_{segment_name}',\n",
    "            'Average TNR': tnr_mean,\n",
    "            'Average TPR': tpr_mean,\n",
    "            'Average Test Score': acc_mean,\n",
    "            'Test Score STD': acc_std,\n",
    "            'Test Score MAX': acc_max,\n",
    "            'Test Score MIN': acc_min,\n",
    "            'Average AUC': auc_mean,\n",
    "            'AUC STD': auc_std,\n",
    "            'Max AUC': auc_max,\n",
    "            'Min AUC': auc_min\n",
    "        })\n",
    "\n",
    "# DataFrameに変換\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# 表示\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     100\n",
      "1     100\n",
      "2     100\n",
      "3    1000\n",
      "4     100\n",
      "5     100\n",
      "6      10\n",
      "7    1000\n",
      "8     100\n",
      "Name: C_RBF1, dtype: object\n",
      "0     100\n",
      "1     100\n",
      "2     100\n",
      "3     100\n",
      "4    5000\n",
      "5     100\n",
      "6      10\n",
      "7    2000\n",
      "8     100\n",
      "Name: C_LINEAR1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF1'])\n",
    "print(results['C_LINEAR1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
