{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['259', '100', '118', '104', '17', '262', '16', '18', '78', '254', '92', '129', '225', '96', '134', '200', '216', '140', '270', '230', '246', '122', '199', '198', '271', '236'], ['136', '128', '218', '149', '126', '71', '242', '102', '94', '207', '261', '267', '145', '297', '252', '50', '197', '5', '112', '19', '79', '300', '25', '67', '42', '264'], ['70', '53', '279', '55', '194', '201', '58', '45', '255', '210', '249', '206', '108', '124', '2', '101', '193', '29', '135', '142', '1', '76', '294', '48', '117', '6'], ['280', '106', '61', '137', '120', '110', '277', '250', '224', '85', '192', '35', '273', '15', '220', '69', '90', '24', '212', '46', '292', '282', '265', '219', '256', '131'], ['139', '239', '240', '93', '89', '12', '268', '119', '66', '31', '23', '221', '295', '41', '148', '65', '39', '287', '8', '203', '11', '138', '205', '123', '133', '278'], ['21', '95', '237', '227', '63', '191', '60', '208', '56', '37', '235', '284', '299', '33', '290', '247', '272', '223', '113', '83', '251', '229', '82', '222', '116', '3'], ['214', '68', '266', '34', '234', '32', '72', '285', '26', '253', '38', '231', '103', '143', '130', '4', '244', '36', '288', '13', '9', '107', '81', '217', '238', '7'], ['84', '80', '75', '147', '109', '127', '293', '204', '30', '44', '77', '144', '57', '209', '263', '115', '196', '105', '59', '228', '49', '257', '91', '146', '54', '286'], ['283', '233', '150', '281', '22', '296', '291', '43', '258', '88', '40', '213', '260', '98', '276', '248', '111', '195', '73', '20', '275', '10', '215', '226', '97', '202'], ['62', '241', '211', '28', '132', '274', '99', '86', '64', '27', '74', '52', '125', '87', '114', '298', '47', '245', '243', '121', '141', '289', '51', '232', '269', '14']]\n",
      "\n",
      "CSV Single ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Single Recompress ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Large QP1 ListsA:\n",
      "CSV List 1A: 1482\n",
      "CSV List 2A: 1482\n",
      "CSV List 3A: 1482\n",
      "CSV List 4A: 1482\n",
      "CSV List 5A: 1482\n",
      "CSV List 6A: 1482\n",
      "CSV List 7A: 1482\n",
      "CSV List 8A: 1482\n",
      "CSV List 9A: 1482\n",
      "CSV List 10A: 1482\n",
      "\n",
      "CSV Second Recompress Large QP1 ListsA:\n",
      "CSV List 1A: 1482\n",
      "CSV List 2A: 1482\n",
      "CSV List 3A: 1482\n",
      "CSV List 4A: 1482\n",
      "CSV List 5A: 1482\n",
      "CSV List 6A: 1482\n",
      "CSV List 7A: 1482\n",
      "CSV List 8A: 1482\n",
      "CSV List 9A: 1482\n",
      "CSV List 10A: 1482\n",
      "\n",
      "CSV Second Same QP ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Recompress Same QP ListsA:\n",
      "CSV List 1A: 260\n",
      "CSV List 2A: 260\n",
      "CSV List 3A: 260\n",
      "CSV List 4A: 260\n",
      "CSV List 5A: 260\n",
      "CSV List 6A: 260\n",
      "CSV List 7A: 260\n",
      "CSV List 8A: 260\n",
      "CSV List 9A: 260\n",
      "CSV List 10A: 260\n",
      "\n",
      "CSV Second Large QP2 ListsA:\n",
      "CSV List 1A: 1014\n",
      "CSV List 2A: 1014\n",
      "CSV List 3A: 1014\n",
      "CSV List 4A: 1014\n",
      "CSV List 5A: 1014\n",
      "CSV List 6A: 1014\n",
      "CSV List 7A: 1014\n",
      "CSV List 8A: 1014\n",
      "CSV List 9A: 1014\n",
      "CSV List 10A: 1014\n",
      "\n",
      "CSV Second Recompress Large QP2 ListsA:\n",
      "CSV List 1A: 1014\n",
      "CSV List 2A: 1014\n",
      "CSV List 3A: 1014\n",
      "CSV List 4A: 1014\n",
      "CSV List 5A: 1014\n",
      "CSV List 6A: 1014\n",
      "CSV List 7A: 1014\n",
      "CSV List 8A: 1014\n",
      "CSV List 9A: 1014\n",
      "CSV List 10A: 1014\n",
      "\n",
      "PKL Single ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Single Recompress ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Large QP1 ListsA:\n",
      "PKL List 1A: 1482\n",
      "PKL List 2A: 1482\n",
      "PKL List 3A: 1482\n",
      "PKL List 4A: 1482\n",
      "PKL List 5A: 1482\n",
      "PKL List 6A: 1482\n",
      "PKL List 7A: 1482\n",
      "PKL List 8A: 1482\n",
      "PKL List 9A: 1482\n",
      "PKL List 10A: 1482\n",
      "\n",
      "PKL Second Recompress Large QP1 ListsA:\n",
      "PKL List 1A: 1482\n",
      "PKL List 2A: 1482\n",
      "PKL List 3A: 1482\n",
      "PKL List 4A: 1482\n",
      "PKL List 5A: 1482\n",
      "PKL List 6A: 1482\n",
      "PKL List 7A: 1482\n",
      "PKL List 8A: 1482\n",
      "PKL List 9A: 1482\n",
      "PKL List 10A: 1482\n",
      "\n",
      "PKL Second Same QP ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Recompress Same QP ListsA:\n",
      "PKL List 1A: 260\n",
      "PKL List 2A: 260\n",
      "PKL List 3A: 260\n",
      "PKL List 4A: 260\n",
      "PKL List 5A: 260\n",
      "PKL List 6A: 260\n",
      "PKL List 7A: 260\n",
      "PKL List 8A: 260\n",
      "PKL List 9A: 260\n",
      "PKL List 10A: 260\n",
      "\n",
      "PKL Second Large QP2 ListsA:\n",
      "PKL List 1A: 1014\n",
      "PKL List 2A: 1014\n",
      "PKL List 3A: 1014\n",
      "PKL List 4A: 1014\n",
      "PKL List 5A: 1014\n",
      "PKL List 6A: 1014\n",
      "PKL List 7A: 1014\n",
      "PKL List 8A: 1014\n",
      "PKL List 9A: 1014\n",
      "PKL List 10A: 1014\n",
      "\n",
      "PKL Second Recompress Large QP2 ListsA:\n",
      "PKL List 1A: 1014\n",
      "PKL List 2A: 1014\n",
      "PKL List 3A: 1014\n",
      "PKL List 4A: 1014\n",
      "PKL List 5A: 1014\n",
      "PKL List 6A: 1014\n",
      "PKL List 7A: 1014\n",
      "PKL List 8A: 1014\n",
      "PKL List 9A: 1014\n",
      "PKL List 10A: 1014\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "\n",
    "train_list6 = [\"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\"]\n",
    "\n",
    "train_list7 = [\"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\", \"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\"]\n",
    "\n",
    "train_list8 = [\"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\", \"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\"]\n",
    "\n",
    "train_list9 = [\"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\", \"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\"]\n",
    "\n",
    "train_list10 = [\"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+26] for i in range(0, len(combined_train_list), 26)]\n",
    "print(train_lists)\n",
    "\n",
    "\n",
    "\n",
    "# CSV関連のリストを生成\n",
    "csv_single_listsA = [[] for _ in range(10)]\n",
    "csv_single_recompress_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "csv_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "csv_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "csv_second_recompress_largeQP2_listsA = [[] for _ in range(10)]\n",
    "\n",
    "def process_csv_lists(rootpath, train_list, single_list, single_recompress_list, \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのCSVリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           csv_single_listsA,\n",
    "                                                           csv_single_recompress_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, single_list, single_recompress_list, \n",
    "                      [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   csv_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   csv_second_recompress_largeQP2_listsA):\n",
    "    process_csv_lists(rootpath_csv, train_list, [], [], \n",
    "                      second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                      second_sameQP_list, second_recompress_sameQP_list,\n",
    "                      second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# 出力リストを初期化\n",
    "pkl_single_listsA = [[] for _ in range(10)]\n",
    "pkl_single_recompress_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP1_listsA = [[] for _ in range(10)]\n",
    "pkl_second_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_sameQP_listsA = [[] for _ in range(10)]\n",
    "pkl_second_largeQP2_listsA = [[] for _ in range(10)]\n",
    "pkl_second_recompress_largeQP2_listsA = [[] for _ in range(10)]    \n",
    "\n",
    "def process_train_lists_pkl(rootpath, train_list, single_list, single_recompress_list, \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath, f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath, f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath, f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath, f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath, f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath, f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath, f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "\n",
    "# 各カテゴリのリストを生成\n",
    "for train_list, single_list, single_recompress_list in zip(train_lists, \n",
    "                                                           pkl_single_listsA,\n",
    "                                                           pkl_single_recompress_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, single_list, single_recompress_list, \n",
    "                            [], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for train_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                   pkl_second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                   pkl_second_recompress_largeQP2_listsA):\n",
    "    process_train_lists_pkl(rootpath_pkl, train_list, [], [], \n",
    "                            second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                            second_sameQP_list, second_recompress_sameQP_list,\n",
    "                            second_largeQP2_list, second_recompress_largeQP2_list)\n",
    "\n",
    "\n",
    "print(\"\\nCSV Single ListsA:\")\n",
    "for i, lst in enumerate(csv_single_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(csv_single_recompress_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nCSV Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(csv_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"CSV List {i}A: {len(lst)}\")\n",
    "\n",
    "# 出力リストを表示\n",
    "print(\"\\nPKL Single ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Single Recompress ListsA:\")\n",
    "for i, lst in enumerate(pkl_single_recompress_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP1 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP1_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Same QP ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_sameQP_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")\n",
    "\n",
    "print(\"\\nPKL Second Recompress Large QP2 ListsA:\")\n",
    "for i, lst in enumerate(pkl_second_recompress_largeQP2_listsA, 1):\n",
    "    print(f\"PKL List {i}A: {len(lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "# single_listsおよびsingle_recompress_listsは初期化されている前提\n",
    "single_csv1 = list(zip(csv_single_listsA[0], pkl_single_listsA[0], csv_single_recompress_listsA[0], pkl_single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(csv_single_listsA[1], pkl_single_listsA[1], csv_single_recompress_listsA[1], pkl_single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(csv_single_listsA[2], pkl_single_listsA[2], csv_single_recompress_listsA[2], pkl_single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(csv_single_listsA[3], pkl_single_listsA[3], csv_single_recompress_listsA[3], pkl_single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(csv_single_listsA[4], pkl_single_listsA[4], csv_single_recompress_listsA[4], pkl_single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(csv_single_listsA[5], pkl_single_listsA[5], csv_single_recompress_listsA[5], pkl_single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(csv_single_listsA[6], pkl_single_listsA[6], csv_single_recompress_listsA[6], pkl_single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(csv_single_listsA[7], pkl_single_listsA[7], csv_single_recompress_listsA[7], pkl_single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(csv_single_listsA[8], pkl_single_listsA[8], csv_single_recompress_listsA[8], pkl_single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(csv_single_listsA[9], pkl_single_listsA[9], csv_single_recompress_listsA[9], pkl_single_recompress_listsA[9]))\n",
    "\n",
    "single_csv1 = random.sample(single_csv1, 240)\n",
    "single_csv2 = random.sample(single_csv2, 240)\n",
    "single_csv3 = random.sample(single_csv3, 240)\n",
    "single_csv4 = random.sample(single_csv4, 240)\n",
    "single_csv5 = random.sample(single_csv5, 240)\n",
    "single_csv6 = random.sample(single_csv6, 240)\n",
    "single_csv7 = random.sample(single_csv7, 240)\n",
    "single_csv8 = random.sample(single_csv8, 240)\n",
    "single_csv9 = random.sample(single_csv9, 240)\n",
    "single_csv10 = random.sample(single_csv10, 240)\n",
    "print(len(single_csv1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482\n",
      "\n",
      "double images train by QP1>QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# Large_QP1\n",
    "second_largeQP1_csv1 = list(zip(csv_second_largeQP1_listsA[0], pkl_second_largeQP1_listsA[0], csv_second_recompress_largeQP1_listsA[0], pkl_second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(csv_second_largeQP1_listsA[1], pkl_second_largeQP1_listsA[1], csv_second_recompress_largeQP1_listsA[1], pkl_second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(csv_second_largeQP1_listsA[2], pkl_second_largeQP1_listsA[2], csv_second_recompress_largeQP1_listsA[2], pkl_second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(csv_second_largeQP1_listsA[3], pkl_second_largeQP1_listsA[3], csv_second_recompress_largeQP1_listsA[3], pkl_second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(csv_second_largeQP1_listsA[4], pkl_second_largeQP1_listsA[4], csv_second_recompress_largeQP1_listsA[4], pkl_second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(csv_second_largeQP1_listsA[5], pkl_second_largeQP1_listsA[5], csv_second_recompress_largeQP1_listsA[5], pkl_second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(csv_second_largeQP1_listsA[6], pkl_second_largeQP1_listsA[6], csv_second_recompress_largeQP1_listsA[6], pkl_second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(csv_second_largeQP1_listsA[7], pkl_second_largeQP1_listsA[7], csv_second_recompress_largeQP1_listsA[7], pkl_second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(csv_second_largeQP1_listsA[8], pkl_second_largeQP1_listsA[8], csv_second_recompress_largeQP1_listsA[8], pkl_second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(csv_second_largeQP1_listsA[9], pkl_second_largeQP1_listsA[9], csv_second_recompress_largeQP1_listsA[9], pkl_second_recompress_largeQP1_listsA[9]))\n",
    "print(len(second_largeQP1_csv1))\n",
    "\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 80)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 80)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 80)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 80)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 80)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 80)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 80)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 80)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 80)\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 80)\n",
    "# second_largeQP1_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1>QP2: ', len(second_largeQP1_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "\n",
      "double images train by QP1=QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# sameQP\n",
    "second_sameQP_csv1 = list(zip(csv_second_sameQP_listsA[0], pkl_second_sameQP_listsA[0], csv_second_recompress_sameQP_listsA[0], pkl_second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(csv_second_sameQP_listsA[1], pkl_second_sameQP_listsA[1], csv_second_recompress_sameQP_listsA[1], pkl_second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(csv_second_sameQP_listsA[2], pkl_second_sameQP_listsA[2], csv_second_recompress_sameQP_listsA[2], pkl_second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(csv_second_sameQP_listsA[3], pkl_second_sameQP_listsA[3], csv_second_recompress_sameQP_listsA[3], pkl_second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(csv_second_sameQP_listsA[4], pkl_second_sameQP_listsA[4], csv_second_recompress_sameQP_listsA[4], pkl_second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(csv_second_sameQP_listsA[5], pkl_second_sameQP_listsA[5], csv_second_recompress_sameQP_listsA[5], pkl_second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(csv_second_sameQP_listsA[6], pkl_second_sameQP_listsA[6], csv_second_recompress_sameQP_listsA[6], pkl_second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(csv_second_sameQP_listsA[7], pkl_second_sameQP_listsA[7], csv_second_recompress_sameQP_listsA[7], pkl_second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(csv_second_sameQP_listsA[8], pkl_second_sameQP_listsA[8], csv_second_recompress_sameQP_listsA[8], pkl_second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(csv_second_sameQP_listsA[9], pkl_second_sameQP_listsA[9], csv_second_recompress_sameQP_listsA[9], pkl_second_recompress_sameQP_listsA[9]))\n",
    "print(len(second_sameQP_csv10))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 80)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 80)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 80)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 80)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 80)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 80)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 80)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 80)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 80)\n",
    "second_sameQP_csv10 = random.sample(second_sameQP_csv10, 80)\n",
    "print('\\ndouble images train by QP1=QP2: ',len(second_sameQP_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "\n",
      "double images train by QP1<QP2:  80\n"
     ]
    }
   ],
   "source": [
    "# Large_QP2\n",
    "second_largeQP2_csv1 = list(zip(csv_second_largeQP2_listsA[0], pkl_second_largeQP2_listsA[0], csv_second_recompress_largeQP2_listsA[0], pkl_second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(csv_second_largeQP2_listsA[1], pkl_second_largeQP2_listsA[1], csv_second_recompress_largeQP2_listsA[1], pkl_second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(csv_second_largeQP2_listsA[2], pkl_second_largeQP2_listsA[2], csv_second_recompress_largeQP2_listsA[2], pkl_second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(csv_second_largeQP2_listsA[3], pkl_second_largeQP2_listsA[3], csv_second_recompress_largeQP2_listsA[3], pkl_second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(csv_second_largeQP2_listsA[4], pkl_second_largeQP2_listsA[4], csv_second_recompress_largeQP2_listsA[4], pkl_second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(csv_second_largeQP2_listsA[5], pkl_second_largeQP2_listsA[5], csv_second_recompress_largeQP2_listsA[5], pkl_second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(csv_second_largeQP2_listsA[6], pkl_second_largeQP2_listsA[6], csv_second_recompress_largeQP2_listsA[6], pkl_second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(csv_second_largeQP2_listsA[7], pkl_second_largeQP2_listsA[7], csv_second_recompress_largeQP2_listsA[7], pkl_second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(csv_second_largeQP2_listsA[8], pkl_second_largeQP2_listsA[8], csv_second_recompress_largeQP2_listsA[8], pkl_second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(csv_second_largeQP2_listsA[9], pkl_second_largeQP2_listsA[9], csv_second_recompress_largeQP2_listsA[9], pkl_second_recompress_largeQP2_listsA[9]))\n",
    "print(len(second_largeQP2_csv1))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 80)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 80)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 80)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 80)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 80)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 80)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 80)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 80)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 80)\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 80)\n",
    "# second_largeQP2_csv10 = selected_data[9]\n",
    "print('\\ndouble images train by QP1<QP2: ', len(second_largeQP2_csv10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2 = [\"_1stQP2_\"]\n",
    "QP4 = [\"_1stQP4_\"]\n",
    "QP12 = [\"_1stQP12_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP4_QP2 = [\"_1stQP4_2ndQP2_\"]\n",
    "QP12_QP2 = [\"_1stQP12_2ndQP2_\"]\n",
    "QP12_QP4 = [\"_1stQP12_2ndQP4_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2_QP2 = [\"_1stQP2_2ndQP2\"]\n",
    "QP4_QP4 = [\"_1stQP4_2ndQP4\"]\n",
    "QP12_QP12 = [\"_1stQP12_2ndQP12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QP2_QP4 = [\"_1stQP2_2ndQP4\"]\n",
    "QP2_QP12 = [\"_1stQP2_2ndQP12\"]\n",
    "QP4_QP12 = [\"_1stQP4_2ndQP12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath2 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath2, 'GIMP_csv')\n",
    "GIMP_path2 = os.path.join(rootpath2, 'GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_path1_csv = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_csv = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath2, 'LIBHEIF_csv')\n",
    "LIBHEIF_path2 = os.path.join(rootpath2, 'LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_path1_csv = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_csv = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath2, 'GIMP_GIMP_csv')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath2, 'GIMP_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_GIMP_path1_csv = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_csv = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath2, 'LIBHEIF_GIMP_csv')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath2, 'LIBHEIF_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_GIMP_path1_csv = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_csv = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath2, 'GIMP_LIBHEIF_csv')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath2, 'GIMP_LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_LIBHEIF_path1_csv = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_csv = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath2 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath2, 'GIMP_csv')\n",
    "GIMP_path2 = os.path.join(rootpath2, 'GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_path1_csv = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_csv = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath2, 'LIBHEIF_csv')\n",
    "LIBHEIF_path2 = os.path.join(rootpath2, 'LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_path1_csv = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_csv = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath2, 'GIMP_GIMP_csv')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath2, 'GIMP_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_GIMP_path1_csv = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_csv = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath2, 'LIBHEIF_GIMP_csv')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath2, 'LIBHEIF_GIMP_RECOMPRESSED_csv')\n",
    "\n",
    "LIBHEIF_GIMP_path1_csv = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_csv = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath2, 'GIMP_LIBHEIF_csv')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath2, 'GIMP_LIBHEIF_RECOMPRESSED_csv')\n",
    "\n",
    "GIMP_LIBHEIF_path1_csv = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_csv = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath3 = \"/Prove/Yoshihisa/HEIF_ghost/EXPERIMENT_DIFFERENT_SOFTWARE/PKL/\"\n",
    "\n",
    "# SINGLE\n",
    "GIMP_path1 = os.path.join(rootpath3, 'pkl_GIMP')\n",
    "GIMP_path2 = os.path.join(rootpath3, 'pkl_GIMP_RECOMPRESSED')\n",
    "\n",
    "GIMP_path1_pkl = [os.path.join(GIMP_path1, file) for file in sorted(os.listdir(GIMP_path1))]\n",
    "GIMP_path2_pkl = [os.path.join(GIMP_path2, file) for file in sorted(os.listdir(GIMP_path2))]\n",
    "\n",
    "LIBHEIF_path1 = os.path.join(rootpath3, 'pkl_LIBHEIF')\n",
    "LIBHEIF_path2 = os.path.join(rootpath3, 'pkl_LIBHEIF_RECOMPRESSED')\n",
    "\n",
    "LIBHEIF_path1_pkl = [os.path.join(LIBHEIF_path1, file) for file in sorted(os.listdir(LIBHEIF_path1))]\n",
    "LIBHEIF_path2_pkl = [os.path.join(LIBHEIF_path2, file) for file in sorted(os.listdir(LIBHEIF_path2))]\n",
    "\n",
    "\n",
    "# DOUBLE\n",
    "GIMP_GIMP_path1 = os.path.join(rootpath3, 'pkl_GIMP_GIMP')\n",
    "GIMP_GIMP_path2 = os.path.join(rootpath3, 'pkl_GIMP_GIMP_RECOMPRESSED')\n",
    "\n",
    "GIMP_GIMP_path1_pkl = [os.path.join(GIMP_GIMP_path1, file) for file in sorted(os.listdir(GIMP_GIMP_path1))]\n",
    "GIMP_GIMP_path2_pkl = [os.path.join(GIMP_GIMP_path2, file) for file in sorted(os.listdir(GIMP_GIMP_path2))]\n",
    "\n",
    "LIBHEIF_GIMP_path1 = os.path.join(rootpath3, 'pkl_LIBHEIF_GIMP')\n",
    "LIBHEIF_GIMP_path2 = os.path.join(rootpath3, 'pkl_LIBHEIF_GIMP_RECOMPRESSED')\n",
    "\n",
    "LIBHEIF_GIMP_path1_pkl = [os.path.join(LIBHEIF_GIMP_path1, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path1))]\n",
    "LIBHEIF_GIMP_path2_pkl = [os.path.join(LIBHEIF_GIMP_path2, file) for file in sorted(os.listdir(LIBHEIF_GIMP_path2))]\n",
    "\n",
    "GIMP_LIBHEIF_path1 = os.path.join(rootpath3, 'pkl_GIMP_LIBHEIF')\n",
    "GIMP_LIBHEIF_path2 = os.path.join(rootpath3, 'pkl_GIMP_LIBHEIF_RECOMPRESSED')\n",
    "\n",
    "GIMP_LIBHEIF_path1_pkl = [os.path.join(GIMP_LIBHEIF_path1, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path1))]\n",
    "GIMP_LIBHEIF_path2_pkl = [os.path.join(GIMP_LIBHEIF_path2, file) for file in sorted(os.listdir(GIMP_LIBHEIF_path2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIMP_csv = list(zip(GIMP_path1_csv, GIMP_path1_pkl, GIMP_path2_csv, GIMP_path2_pkl))\n",
    "\n",
    "\n",
    "single_QP2_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP2)]\n",
    "single_QP4_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP4)]\n",
    "single_QP12_GIMP = [item for item in GIMP_csv if any(qp in item[0] for qp in QP12)]\n",
    "\n",
    "single_QP2_GIMP = random.sample(single_QP2_GIMP, 10)\n",
    "single_QP4_GIMP = random.sample(single_QP4_GIMP, 10)\n",
    "single_QP12_GIMP = random.sample(single_QP12_GIMP, 10)\n",
    "\n",
    "LIBHEIF_csv = list(zip(LIBHEIF_path1_csv, LIBHEIF_path1_pkl, LIBHEIF_path2_csv, LIBHEIF_path2_pkl))\n",
    "# LIBHEIF_csv1 = random.sample(LIBHEIF_csv, 10)\n",
    "\n",
    "single_QP2_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP2)]\n",
    "single_QP4_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP4)]\n",
    "single_QP12_LIBHEIF = [item for item in LIBHEIF_csv if any(qp in item[0] for qp in QP12)]\n",
    "\n",
    "single_QP2_LIBHEIF = random.sample(single_QP2_LIBHEIF, 10)\n",
    "single_QP4_LIBHEIF = random.sample(single_QP4_LIBHEIF, 10)\n",
    "single_QP12_LIBHEIF = random.sample(single_QP12_LIBHEIF, 10)\n",
    "\n",
    "\n",
    "GIMP_GIMP_csv = list(zip(GIMP_GIMP_path1_csv, GIMP_GIMP_path1_pkl, GIMP_GIMP_path2_csv, GIMP_GIMP_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_GG = [item for item in GIMP_GIMP_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "LIBHEIF_GIMP_csv = list(zip(LIBHEIF_GIMP_path1_csv, LIBHEIF_GIMP_path1_pkl, LIBHEIF_GIMP_path2_csv, LIBHEIF_GIMP_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_LG = [item for item in LIBHEIF_GIMP_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "GIMP_LIBHEIF_csv = list(zip(GIMP_LIBHEIF_path1_csv, GIMP_LIBHEIF_path1_pkl, GIMP_LIBHEIF_path2_csv, GIMP_LIBHEIF_path2_pkl))\n",
    "\n",
    "second_QP4_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP2)]\n",
    "second_QP12_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP2)]\n",
    "second_QP12_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP4)]\n",
    "\n",
    "second_QP2_QP2_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP2)]\n",
    "second_QP4_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP4)]\n",
    "second_QP12_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP12_QP12)]\n",
    "\n",
    "second_QP2_QP4_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP4)]\n",
    "second_QP2_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP2_QP12)]\n",
    "second_QP4_QP12_GL = [item for item in GIMP_LIBHEIF_csv if any(qp in item[0] for qp in QP4_QP12)]\n",
    "\n",
    "# print(len(single_QP2_GIMP))\n",
    "# print(len(single_QP2_LIBHEIF))\n",
    "# print(len(second_QP4_QP2_GG))\n",
    "# print(len(second_QP4_QP2_LG))\n",
    "# print(len(second_QP4_QP2_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv_list:  480\n"
     ]
    }
   ],
   "source": [
    "# Training_data\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv10 + second_largeQP2_csv10\n",
    "print(\"train_csv_list: \", len(train_csv_list10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP12_QP4_GG:  20\n",
      "test_QP12_QP4_LG:  20\n",
      "test_QP12_QP4_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP4_QP2_GG = second_QP4_QP2_GG + single_QP2_GIMP\n",
    "test_QP12_QP2_GG = second_QP12_QP2_GG + single_QP2_GIMP\n",
    "test_QP12_QP4_GG = second_QP12_QP4_GG + single_QP4_GIMP\n",
    "\n",
    "test_QP4_QP2_LG = second_QP4_QP2_LG + single_QP2_GIMP\n",
    "test_QP12_QP2_LG = second_QP12_QP2_LG + single_QP2_GIMP\n",
    "test_QP12_QP4_LG = second_QP12_QP4_LG + single_QP4_GIMP\n",
    "\n",
    "test_QP4_QP2_GL = second_QP4_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP12_QP2_GL = second_QP12_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP12_QP4_GL = second_QP12_QP4_GL + single_QP4_LIBHEIF\n",
    "\n",
    "print('test_QP12_QP4_GG: ', len(test_QP12_QP4_GG))\n",
    "print('test_QP12_QP4_LG: ', len(test_QP12_QP4_LG))\n",
    "print('test_QP12_QP4_GL: ', len(test_QP12_QP4_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP12_QP12_GG:  20\n",
      "test_QP12_QP12_LG:  20\n",
      "test_QP12_QP12_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP2_QP2_GG = second_QP2_QP2_GG + single_QP2_GIMP\n",
    "test_QP4_QP4_GG = second_QP4_QP4_GG + single_QP4_GIMP\n",
    "test_QP12_QP12_GG = second_QP12_QP12_GG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP2_LG = second_QP2_QP2_LG + single_QP2_GIMP\n",
    "test_QP4_QP4_LG = second_QP4_QP4_LG + single_QP4_GIMP\n",
    "test_QP12_QP12_LG = second_QP12_QP12_LG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP2_GL = second_QP2_QP2_GL + single_QP2_LIBHEIF\n",
    "test_QP4_QP4_GL = second_QP4_QP4_GL + single_QP4_LIBHEIF\n",
    "test_QP12_QP12_GL = second_QP12_QP12_GL + single_QP12_LIBHEIF\n",
    "\n",
    "print('test_QP12_QP12_GG: ', len(test_QP12_QP12_GG))\n",
    "print('test_QP12_QP12_LG: ', len(test_QP12_QP12_LG))\n",
    "print('test_QP12_QP12_GL: ', len(test_QP12_QP12_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_QP4_QP12_GG:  20\n",
      "test_QP4_QP12_LG:  20\n",
      "test_QP4_QP12_GL:  20\n"
     ]
    }
   ],
   "source": [
    "test_QP2_QP4_GG = second_QP2_QP4_GG + single_QP4_GIMP\n",
    "test_QP2_QP12_GG = second_QP2_QP12_GG + single_QP12_GIMP\n",
    "test_QP4_QP12_GG = second_QP4_QP12_GG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP4_LG = second_QP2_QP4_LG + single_QP4_GIMP\n",
    "test_QP2_QP12_LG = second_QP2_QP12_LG + single_QP12_GIMP\n",
    "test_QP4_QP12_LG = second_QP4_QP12_LG + single_QP12_GIMP\n",
    "\n",
    "test_QP2_QP4_GL = second_QP2_QP4_GL + single_QP4_LIBHEIF\n",
    "test_QP2_QP12_GL = second_QP2_QP12_GL + single_QP12_LIBHEIF\n",
    "test_QP4_QP12_GL = second_QP4_QP12_GL + single_QP12_LIBHEIF\n",
    "\n",
    "print('test_QP4_QP12_GG: ', len(test_QP4_QP12_GG))\n",
    "print('test_QP4_QP12_LG: ', len(test_QP4_QP12_LG))\n",
    "print('test_QP4_QP12_GL: ', len(test_QP4_QP12_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_OG = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1番目のCSVファイルを処理する\n",
    "test_df1, LABEL_t1, MAE_t1, FINAL_QP_t1 = process_train_csv_lists(test_QP4_QP2_GG)\n",
    "\n",
    "# 2番目のCSVファイルを処理する\n",
    "test_df2, LABEL_t2, MAE_t2, FINAL_QP_t2 = process_train_csv_lists(test_QP12_QP2_GG)\n",
    "\n",
    "# 3番目のCSVファイルを処理する\n",
    "test_df3, LABEL_t3, MAE_t3, FINAL_QP_t3 = process_train_csv_lists(test_QP12_QP4_GG)\n",
    "\n",
    "# 4番目のCSVファイルを処理する\n",
    "test_df4, LABEL_t4, MAE_t4, FINAL_QP_t4 = process_train_csv_lists(test_QP4_QP2_LG)\n",
    "\n",
    "# 5番目のCSVファイルを処理する\n",
    "test_df5, LABEL_t5, MAE_t5, FINAL_QP_t5 = process_train_csv_lists(test_QP12_QP2_LG)\n",
    "\n",
    "# 6番目のCSVファイルを処理する\n",
    "test_df6, LABEL_t6, MAE_t6, FINAL_QP_t6 = process_train_csv_lists(test_QP12_QP4_LG)\n",
    "\n",
    "# 7番目のCSVファイルを処理する\n",
    "test_df7, LABEL_t7, MAE_t7, FINAL_QP_t7 = process_train_csv_lists(test_QP4_QP2_GL)\n",
    "\n",
    "# 8番目のCSVファイルを処理する\n",
    "test_df8, LABEL_t8, MAE_t8, FINAL_QP_t8 = process_train_csv_lists(test_QP12_QP2_GL)\n",
    "\n",
    "# 9番目のCSVファイルを処理する\n",
    "test_df9, LABEL_t9, MAE_t9, FINAL_QP_t9 = process_train_csv_lists(test_QP12_QP4_GL)\n",
    "\n",
    "\n",
    "# 10番目のCSVファイルを処理する\n",
    "test_df10, LABEL_t10, MAE_t10, FINAL_QP_t10 = process_train_csv_lists(test_QP2_QP2_GG)\n",
    "\n",
    "# 11番目のCSVファイルを処理する\n",
    "test_df11, LABEL_t11, MAE_t11, FINAL_QP_t11 = process_train_csv_lists(test_QP4_QP4_GG)\n",
    "\n",
    "# 12番目のCSVファイルを処理する\n",
    "test_df12, LABEL_t12, MAE_t12, FINAL_QP_t12 = process_train_csv_lists(test_QP12_QP12_GG)\n",
    "\n",
    "# 13番目のCSVファイルを処理する\n",
    "test_df13, LABEL_t13, MAE_t13, FINAL_QP_t13 = process_train_csv_lists(test_QP2_QP2_LG)\n",
    "\n",
    "# 14番目のCSVファイルを処理する\n",
    "test_df14, LABEL_t14, MAE_t14, FINAL_QP_t14 = process_train_csv_lists(test_QP4_QP4_LG)\n",
    "\n",
    "# 15番目のCSVファイルを処理する\n",
    "test_df15, LABEL_t15, MAE_t15, FINAL_QP_t15 = process_train_csv_lists(test_QP12_QP12_LG)\n",
    "\n",
    "# 16番目のCSVファイルを処理する\n",
    "test_df16, LABEL_t16, MAE_t16, FINAL_QP_t16 = process_train_csv_lists(test_QP2_QP2_GL)\n",
    "\n",
    "# 17番目のCSVファイルを処理する\n",
    "test_df17, LABEL_t17, MAE_t17, FINAL_QP_t17 = process_train_csv_lists(test_QP4_QP4_GL)\n",
    "\n",
    "# 18番目のCSVファイルを処理する\n",
    "test_df18, LABEL_t18, MAE_t18, FINAL_QP_t18 = process_train_csv_lists(test_QP12_QP12_GL)\n",
    "\n",
    "\n",
    "# 19番目のCSVファイルを処理する\n",
    "test_df19, LABEL_t19, MAE_t19, FINAL_QP_t19 = process_train_csv_lists(test_QP2_QP4_GG)\n",
    "\n",
    "# 20番目のCSVファイルを処理する\n",
    "test_df20, LABEL_t20, MAE_t20, FINAL_QP_t20 = process_train_csv_lists(test_QP2_QP12_GG)\n",
    "\n",
    "# 21番目のCSVファイルを処理する\n",
    "test_df21, LABEL_t21, MAE_t21, FINAL_QP_t21 = process_train_csv_lists(test_QP4_QP12_GG)\n",
    "\n",
    "# 22番目のCSVファイルを処理する\n",
    "test_df22, LABEL_t22, MAE_t22, FINAL_QP_t22 = process_train_csv_lists(test_QP2_QP4_LG)\n",
    "\n",
    "# 23番目のCSVファイルを処理する\n",
    "test_df23, LABEL_t23, MAE_t23, FINAL_QP_t23 = process_train_csv_lists(test_QP2_QP12_LG)\n",
    "\n",
    "# 24番目のCSVファイルを処理する\n",
    "test_df24, LABEL_t24, MAE_t24, FINAL_QP_t24 = process_train_csv_lists(test_QP4_QP12_LG)\n",
    "\n",
    "# 25番目のCSVファイルを処理する\n",
    "test_df25, LABEL_t25, MAE_t25, FINAL_QP_t25 = process_train_csv_lists(test_QP2_QP4_GL)\n",
    "\n",
    "# 26番目のCSVファイルを処理する\n",
    "test_df26, LABEL_t26, MAE_t26, FINAL_QP_t26 = process_train_csv_lists(test_QP2_QP12_GL)\n",
    "\n",
    "# 27番目のCSVファイルを処理する\n",
    "test_df27, LABEL_t27, MAE_t27, FINAL_QP_t27 = process_train_csv_lists(test_QP4_QP12_GL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データフレームを結合\n",
    "combined_train_df = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], ignore_index=True)\n",
    "combined_LABEL = pd.concat([LABEL1, LABEL2, LABEL3, LABEL4, LABEL5, LABEL6, LABEL7, LABEL8, LABEL9, LABEL10], ignore_index=True)\n",
    "combined_MAE = pd.concat([MAE1, MAE2, MAE3, MAE4, MAE5, MAE6, MAE7, MAE8, MAE9, MAE10], ignore_index=True)\n",
    "combined_FINAL_QP = pd.concat([FINAL_QP1, FINAL_QP2, FINAL_QP3, FINAL_QP4, FINAL_QP5, FINAL_QP6, FINAL_QP7, FINAL_QP8, FINAL_QP9, FINAL_QP10], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 44)\n",
      "(4800, 1)\n",
      "(4800, 1)\n",
      "(4800, 1)\n",
      "(20, 44)\n",
      "(20, 1)\n",
      "(20, 1)\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df.shape)\n",
    "print(combined_LABEL.shape)\n",
    "print(combined_MAE.shape)\n",
    "print(combined_FINAL_QP.shape)\n",
    "\n",
    "print(test_df1.shape)\n",
    "print(LABEL_t1.shape)\n",
    "print(MAE_t1.shape)\n",
    "print(FINAL_QP_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "Combined Train DF:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10      0   1088   1024  10168  47720      0   1088   1008   8944  48960   9500   6559  2009   2369   1619   1392   1865   1001   9566   6762  2003   2385   1601   1355   1898    985  18720  11648   7784   6444   1688  13716  18768  11944   7960   6596   1552  13180  0.001602  0.000125     0.0004  0.029046  0.022969\n",
      "1       27      0  30208  18768   7216   3808      0  29952  19376   6856   3816  10261   4807   240   2455   1934   3726   5705   3468   9679   6278   449   2517   2612   4036   6067   3464   2608    960   1708    760    384  53580   2424    984   2104    900    540  53048  0.000334  0.008968   0.001435  0.237427  0.202655\n",
      "2       39      0  18176  11520  15996  14308      0  18688  11584  15732  13996  12607  11874   193    905    203   2496   6203    790  13163  13619   202   1102    199   2323   6310    709   4184   2464   1652   1988   1576  48136   3436   2024   1324   1464   1336  50416  0.000215  0.002384   0.005253  0.171953  0.184839\n",
      "3       39      0  30144  14224  11496   4136      0  30784  14368  11000   3848  15814   5271  2697   4093   2023   1665   3868    885  16452   6057  2745   4191   1852   1433   5010    935   1076    372    332    368    476  57376    968    420    232    244    492  57644  0.000483  0.004465   0.000926  0.155397  0.167929\n",
      "4        5      0      0   8976  23248  27776      0      0   8384  21880  29736  12717   3732  1534   1412   1472   1226   1345   1265  13107   4183  1689   1457   1383   1240   1397   1294  17240   8860   8408   8000   1968  15524  17196   9576   8988   8624   1628  13988   0.00214   0.00088   0.003073  0.736115  0.374955\n",
      "\n",
      "Before scaling:\n",
      "Combined Test DF:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0        2      0     64   2944  24068  32924      0      0   2544  23268  34188  10177   7936  3797   2715   3118   1082   1296    947  10408   7877  3775   2631   3061   1139   1298    975  16032   8248  12060   9220   3268  11172  16388   8948  12644   9420   2984   9616  0.004575  0.000207   0.003009  0.924978  0.842385\n",
      "1        2      0      0   1104  11216  47680      0      0   1168   9200  49632   7899   6087  1403   1924   1420   1474   2027   1390   7962   6410  1445   2014   1342   1531   1957   1382  18960  11312   8168  10072   1704   9784  18912  12848   8108  10164   1432   8536  0.004116  0.000511   0.003463  0.883246  0.868847\n",
      "2        2      0     64  15520  24580  19836      0      0  16176  23040  20784  14064  13897  1872   1866   1230   1413   1427    882  14514  14160  1812   1836   1132   1370   1526    851  16852   8916  10672  11052   4604   7904  16932   9632  10680  11148   4260   7348  0.004885  0.000417   0.001032  0.995059  0.995078\n",
      "3        2      0      0   8544  22528  28928      0      0   8752  21452  29796   6998   6662   764    733    429   1133   1011    683   7227   6606   746    788    408   1216    946    697  16208   9848   7428  10532   1772  14212  16180  10384   7904  10608   1764  13160  0.000697  0.000571   0.001166  0.901612  0.888913\n",
      "4        2      0     64   9792  25976  24168      0     64   9888  25060  24988  10079   7951  2317   3003   1909   4840   4515   1856  10466   8279  2278   3037   1735   4857   4486   1823  13724   8968   9400  13144   2600  12164  13252   9272   9688  13940   2264  11584   0.00051  0.000547   0.001315  0.951786  0.946901\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\")\n",
    "print(\"Combined Train DF:\")\n",
    "print(combined_train_df.head())\n",
    "\n",
    "print(\"\\nBefore scaling:\")\n",
    "print(\"Combined Test DF:\")\n",
    "print(test_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling:\n",
      "X_train:\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.125  0.0  0.018478  0.031770  0.347600  0.853240  0.0  0.018458  0.032391  0.305298  0.860638  0.206590  0.154632  0.154422  0.107997  0.132834  0.098878  0.059804  0.111806  0.224984  0.154999  0.153716  0.111694  0.132326  0.091858  0.056991  0.108040  0.606139  0.471121  0.442172  0.393791  0.153287  0.147228  0.623853  0.472095  0.414066  0.372487  0.157149  0.143808  0.089821  0.000694  0.004103  0.027042  0.021526\n",
      "1  0.550  0.0  0.513043  0.591024  0.246684  0.068088  0.0  0.508143  0.622622  0.234025  0.067079  0.231817  0.095402  0.017904  0.112028  0.159051  0.264668  0.206263  0.387356  0.229037  0.138551  0.034398  0.118049  0.216731  0.273609  0.205397  0.379950  0.084445  0.038829  0.097023  0.046443  0.034871  0.881967  0.080574  0.038893  0.109447  0.050824  0.054678  0.872870  0.018714  0.051680  0.014706  0.235855  0.201484\n",
      "2  0.850  0.0  0.308696  0.362582  0.546834  0.255829  0.0  0.317047  0.372237  0.537002  0.246027  0.309587  0.334314  0.014277  0.039374  0.014981  0.177298  0.225256  0.088239  0.353992  0.388024  0.015433  0.049925  0.015278  0.157481  0.214047  0.077767  0.135475  0.099660  0.093842  0.121486  0.143117  0.781628  0.114214  0.080000  0.068872  0.082674  0.135277  0.824738  0.012063  0.013720  0.053834  0.170245  0.183641\n",
      "3  0.850  0.0  0.511957  0.447806  0.392999  0.073952  0.0  0.522258  0.461697  0.375478  0.067642  0.415899  0.111089  0.207517  0.188807  0.166459  0.118270  0.136199  0.098850  0.471953  0.131041  0.210688  0.198642  0.153281  0.097146  0.167770  0.102556  0.034840  0.015046  0.018859  0.022488  0.043226  0.951932  0.032177  0.016601  0.012068  0.013779  0.049818  0.956916  0.027056  0.025720  0.009491  0.153655  0.166706\n",
      "4  0.000  0.0  0.000000  0.282400  0.794749  0.496639  0.0  0.000000  0.269409  0.746860  0.522711  0.313233  0.059060  0.117765  0.063139  0.120599  0.087086  0.039971  0.141293  0.351983  0.067355  0.129607  0.067017  0.114126  0.084062  0.039157  0.141933  0.558218  0.358356  0.477619  0.488878  0.178714  0.180551  0.571600  0.378498  0.467541  0.487012  0.164844  0.158584  0.119959  0.005047  0.031494  0.735577  0.374044\n",
      "\n",
      "Test data after scaling (test_df1):\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0 -0.075  0.0  0.001087  0.092284  0.822781  0.588685  0.0  0.000000  0.081748  0.794238  0.600970  0.229033  0.201183  0.292406  0.124215  0.257595  0.076858  0.038102  0.105775  0.255183  0.192891  0.289773  0.123538  0.254216  0.077215  0.035633  0.106943  0.519104  0.333603  0.685072  0.563432  0.296767  0.100339  0.544741  0.353676  0.657720  0.531963  0.302147  0.078634  0.256486  0.001166  0.030832  0.924832  0.842179\n",
      "1 -0.075  0.0  0.000000  0.034291  0.383427  0.852525  0.0  0.000000  0.037532  0.314036  0.872451  0.153517  0.138675  0.107656  0.087138  0.116271  0.104702  0.065983  0.155255  0.167456  0.143037  0.110872  0.093833  0.110703  0.103790  0.059092  0.151585  0.613910  0.457531  0.463985  0.615497  0.154740  0.074757  0.628640  0.507826  0.421764  0.573978  0.144998  0.058884  0.230770  0.002918  0.035486  0.883013  0.868680\n",
      "2 -0.075  0.0  0.001087  0.488654  0.840284  0.354670  0.0  0.000000  0.519794  0.786455  0.365349  0.357886  0.402705  0.143849  0.084419  0.100458  0.100369  0.043099  0.098514  0.402446  0.406409  0.139051  0.085263  0.093171  0.092875  0.043749  0.093342  0.545655  0.360621  0.606226  0.675385  0.418089  0.040106  0.562824  0.380711  0.555556  0.629546  0.431349  0.037159  0.273896  0.002376  0.010579  0.995059  0.995101\n",
      "3 -0.075  0.0  0.000000  0.268785  0.770135  0.517236  0.0  0.000000  0.281234  0.732250  0.523766  0.123649  0.158114  0.058342  0.031312  0.033791  0.080480  0.027232  0.076287  0.141095  0.149698  0.057202  0.034808  0.032727  0.082435  0.023103  0.076451  0.524802  0.398317  0.421950  0.643608  0.160915  0.156370  0.537827  0.410435  0.411153  0.599051  0.178615  0.143442  0.039059  0.003265  0.011952  0.901418  0.888776\n",
      "4 -0.075  0.0  0.001087  0.308119  0.888008  0.432127  0.0  0.001086  0.317738  0.855407  0.439249  0.225784  0.201690  0.178191  0.137714  0.156970  0.343799  0.160876  0.207305  0.257263  0.206552  0.174831  0.143084  0.143513  0.329266  0.149117  0.199956  0.444372  0.362724  0.533970  0.803227  0.236106  0.118623  0.440500  0.366482  0.503953  0.787215  0.229243  0.114622  0.028602  0.003125  0.013473  0.951696  0.946852\n"
     ]
    }
   ],
   "source": [
    "def process_results_to_lists(train_df, LABEL, MAE, FINAL_QP, scaler_main=None, fit_scaler=True):\n",
    "    if fit_scaler:\n",
    "        scaler_main = MinMaxScaler()\n",
    "        X_train = scaler_main.fit_transform(train_df)\n",
    "    else:\n",
    "        X_train = scaler_main.transform(train_df)\n",
    "\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "    Y_train = LABEL['LABEL'].astype(int).values\n",
    "\n",
    "    return X_train, MAE_array, FINAL_QP_array, Y_train, scaler_main\n",
    "\n",
    "# 訓練データのスケーリング\n",
    "X_train, MAE_array, FINAL_QP_array, Y_train, scaler_main = process_results_to_lists(\n",
    "    combined_train_df, combined_LABEL, combined_MAE, combined_FINAL_QP, fit_scaler=True\n",
    ")\n",
    "\n",
    "# スケーリング後のデータを表示（任意）\n",
    "print(\"After scaling:\")\n",
    "print(\"X_train:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "\n",
    "# データを元に戻すための関数\n",
    "def restore_data_to_original_order(data, original_lengths):\n",
    "    restored_data = []\n",
    "    start_index = 0\n",
    "    for length in original_lengths:\n",
    "        restored_data.append(data[start_index:start_index + length])\n",
    "        start_index += length\n",
    "    return restored_data\n",
    "\n",
    "# 元のデータフレームの長さ\n",
    "original_lengths = [len(train_df1), len(train_df2), len(train_df3), len(train_df4), len(train_df5), \n",
    "                    len(train_df6), len(train_df7), len(train_df8), len(train_df9), len(train_df10)]\n",
    "\n",
    "# データを元の順序に戻す\n",
    "X_train_list = restore_data_to_original_order(X_train, original_lengths)\n",
    "MAE_list = restore_data_to_original_order(MAE_array, original_lengths)\n",
    "FINAL_QP_list = restore_data_to_original_order(FINAL_QP_array, original_lengths)\n",
    "Y_train_list = restore_data_to_original_order(Y_train, original_lengths)\n",
    "\n",
    "# テストデータのスケーリング関数\n",
    "def append_results_to_lists(train_df, LABEL, MAE, FINAL_QP, X_train_list, MAE_list, FINAL_QP_list, Y_train_list, scaler_main=None, fit_scaler=True):\n",
    "    X_train, MAE_array, FINAL_QP_array, Y_train, _ = process_results_to_lists(\n",
    "        train_df, LABEL, MAE, FINAL_QP, scaler_main, fit_scaler)\n",
    "    X_train_list.append(X_train)\n",
    "    MAE_list.append(MAE_array)\n",
    "    FINAL_QP_list.append(FINAL_QP_array)\n",
    "    Y_train_list.append(Y_train)\n",
    "    return X_train_list, MAE_list, FINAL_QP_list, Y_train_list\n",
    "\n",
    "# テストデータ用の辞書を初期化\n",
    "X_test_dict = {}\n",
    "MAE_test_dict = {}\n",
    "FINAL_QP_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "# テストデータを処理して辞書に追加\n",
    "for i in range(1, 28):\n",
    "    test_df = globals()[f'test_df{i}']\n",
    "    LABEL_t = globals()[f'LABEL_t{i}']\n",
    "    MAE_t = globals()[f'MAE_t{i}']\n",
    "    FINAL_QP_t = globals()[f'FINAL_QP_t{i}']\n",
    "    \n",
    "    X_test_dict[i] = []\n",
    "    MAE_test_dict[i] = []\n",
    "    FINAL_QP_test_dict[i] = []\n",
    "    Y_test_dict[i] = []\n",
    "    \n",
    "    X_test_dict[i], MAE_test_dict[i], FINAL_QP_test_dict[i], Y_test_dict[i] = append_results_to_lists(\n",
    "        test_df, LABEL_t, MAE_t, FINAL_QP_t, X_test_dict[i], MAE_test_dict[i], FINAL_QP_test_dict[i], Y_test_dict[i], scaler_main, fit_scaler=False\n",
    "    )\n",
    "\n",
    "# 確認用の出力\n",
    "for i in range(1, 2):\n",
    "    print(f\"\\nTest data after scaling (test_df{i}):\")\n",
    "    print(pd.DataFrame(X_test_dict[i][0]).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "4320\n",
      "480\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "4320\n",
      "480\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "4320\n",
      "480\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "4320\n",
      "480\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "4320\n",
      "480\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "4320\n",
      "480\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "4320\n",
      "480\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "4320\n",
      "480\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "4320\n",
      "480\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "4320\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# データフレームを初期化\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# 1から106までの列名を作成し、データフレームに追加\n",
    "columns = []\n",
    "for i in range(1, 28):\n",
    "    columns.extend([\n",
    "        f'C_RBF{i}', f'Score_RBF{i}', f'tnr_rbf{i}', f'tpr_rbf{i}', f'AUC_RBF{i}',\n",
    "        f'C_LINEAR{i}', f'Score_LINEAR{i}', f'tnr_linear{i}', f'tpr_linear{i}', f'AUC_LINEAR{i}',\n",
    "        f'Threshold{i}', f'Score_old{i}', f'tnr_old{i}', f'tpr_old{i}', f'AUC_old{i}'\n",
    "    ])\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "X_index = np.arange(10)  # インデックスとして0から8までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    val_data = [X_train_list[i] for i in test_ids]\n",
    "    val_label = [Y_train_list[i] for i in test_ids]\n",
    "        \n",
    "    X_train = [item for data in train_data for item in data]\n",
    "    Y_train = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_val = [item for data in val_data for item in data]\n",
    "    Y_val = [item for data in val_label for item in data]\n",
    "    \n",
    "    print(len(Y_train))\n",
    "    print(len(Y_val))\n",
    "    \n",
    "    # リストの作成（1から106まで）\n",
    "    for i in range(1, 28):\n",
    "        globals()[f'test_data{i}'] = [item for data in X_test_dict[i] for item in data]\n",
    "        globals()[f'test_label{i}'] = [item for data in Y_test_dict[i] for item in data]\n",
    "        globals()[f'MAE_data{i}'] = [item for data in MAE_test_dict[i] for item in data]\n",
    "        globals()[f'FINAL_QP_data{i}'] = [item for data in FINAL_QP_test_dict[i] for item in data]\n",
    "\n",
    "        globals()[f'best_threshold{i}'] = 0\n",
    "        globals()[f'best_accuracy{i}'] = 0\n",
    "        globals()[f'best_predicted_labels{i}'] = []\n",
    "        globals()[f'best_ground_truth_labels{i}'] = []\n",
    "        globals()[f'tnr_old{i}'] = 0\n",
    "        globals()[f'tpr_old{i}'] = 0\n",
    "        \n",
    "        for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "            test_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(20)])\n",
    "            predicted_labels = test_old.astype(int)\n",
    "            ground_truth_labels = np.array(globals()[f'test_label{i}'])\n",
    "            accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "            if accuracy > globals()[f'best_accuracy{i}']:\n",
    "                globals()[f'best_accuracy{i}'] = accuracy\n",
    "                globals()[f'best_threshold{i}'] = threshold\n",
    "                globals()[f'best_predicted_labels{i}'] = predicted_labels\n",
    "                globals()[f'best_ground_truth_labels{i}'] = ground_truth_labels\n",
    "\n",
    "\n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        # SVMモデルのインスタンスを作成\n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value, probability=True)\n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value, probability=True)\n",
    "\n",
    "        # 訓練データで訓練\n",
    "        svm_model_RBF.fit(X_train, Y_train)        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "\n",
    "        # 検証データでの精度が最も高かった場合、そのモデルを保存\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "    \n",
    "    fold_results = {}\n",
    "    for i in range(1, 28):\n",
    "        # RBFモデルの評価\n",
    "        predictions_RBF = best_svm_model_RBF.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_RBF = best_svm_model_RBF.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_RBF = accuracy_score(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'accuracy_RBF{i}'] = accuracy_RBF\n",
    "        report_RBF = classification_report(globals()[f'test_label{i}'], predictions_RBF, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_RBF)\n",
    "        globals()[f'tnr_rbf{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_rbf{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_rbf, tpr_rbf, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_RBF)\n",
    "        auc_rbf = auc(fpr_rbf, tpr_rbf)\n",
    "        globals()[f'auc_rbf{i}'] = auc_rbf\n",
    "        \n",
    "        \n",
    "        # LINEARモデルの評価\n",
    "        predictions_LINEAR = best_svm_model_LINEAR.predict(globals()[f'test_data{i}'])\n",
    "        predictions_prob_LINEAR = best_svm_model_LINEAR.predict_proba(globals()[f'test_data{i}'])[:, 1]  # ROCカーブ用のスコア\n",
    "        accuracy_LINEAR = accuracy_score(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'accuracy_LINEAR{i}'] = accuracy_LINEAR\n",
    "        report_LINEAR = classification_report(globals()[f'test_label{i}'], predictions_LINEAR, digits=4, zero_division=1)\n",
    "        conf_matrix = confusion_matrix(globals()[f'test_label{i}'], predictions_LINEAR)\n",
    "        globals()[f'tnr_linear{i}'] = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        globals()[f'tpr_linear{i}'] = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "        fpr_linear, tpr_linear, _ = roc_curve(globals()[f'test_label{i}'], predictions_prob_LINEAR)\n",
    "        auc_linear = auc(fpr_linear, tpr_linear)\n",
    "        globals()[f'auc_linear{i}'] = auc_linear\n",
    "\n",
    "        # Old modelの評価\n",
    "        thresholds = np.arange(0.00, 1.01, 0.01)\n",
    "        tpr_old_list = []\n",
    "        fpr_old_list = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels_old = np.array([is_double_compressed(globals()[f'MAE_data{i}'][j], globals()[f'FINAL_QP_data{i}'][j], threshold) for j in range(20)])\n",
    "            tn, fp, fn, tp = confusion_matrix(globals()[f'test_label{i}'], predicted_labels_old).ravel()\n",
    "            tpr_old = tp / (tp + fn)\n",
    "            fpr_old = fp / (fp + tn)\n",
    "            tpr_old_list.append(tpr_old)\n",
    "            fpr_old_list.append(fpr_old)\n",
    "        \n",
    "        auc_old = auc(fpr_old_list, tpr_old_list)\n",
    "        globals()[f'auc_old{i}'] = auc_old\n",
    "\n",
    "        # fold_resultsに保存\n",
    "        fold_results[f'C_RBF{i}'] = best_c_value_RBF\n",
    "        fold_results[f'Score_RBF{i}'] = globals()[f'accuracy_RBF{i}']\n",
    "        fold_results[f'tnr_rbf{i}'] = globals()[f'tnr_rbf{i}']\n",
    "        fold_results[f'tpr_rbf{i}'] = globals()[f'tpr_rbf{i}']\n",
    "        fold_results[f'AUC_RBF{i}'] = globals()[f'auc_rbf{i}']\n",
    "\n",
    "        fold_results[f'C_LINEAR{i}'] = best_c_value_LINEAR\n",
    "        fold_results[f'Score_LINEAR{i}'] = globals()[f'accuracy_LINEAR{i}']\n",
    "        fold_results[f'tnr_linear{i}'] = globals()[f'tnr_linear{i}']\n",
    "        fold_results[f'tpr_linear{i}'] = globals()[f'tpr_linear{i}']\n",
    "        fold_results[f'AUC_LINEAR{i}'] = globals()[f'auc_linear{i}']\n",
    "\n",
    "        fold_results[f'Threshold{i}'] = globals()[f'best_threshold{i}']\n",
    "        fold_results[f'Score_old{i}'] = globals()[f'best_accuracy{i}']\n",
    "        fold_results[f'tnr_old{i}'] = globals()[f'tnr_old{i}']\n",
    "        fold_results[f'tpr_old{i}'] = globals()[f'tpr_old{i}']\n",
    "        fold_results[f'AUC_old{i}'] = globals()[f'auc_old{i}']\n",
    "\n",
    "    # 結果をデータフレームに追加\n",
    "    results = pd.concat([results, pd.DataFrame(fold_results, index=[fold])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0    RBF1         59.0         82.0                70.5                6.43            75.0            55.0         0.74     0.04     0.79     0.70\n",
      "1    RBF2         59.0         87.0                73.0                5.37            80.0            60.0         0.85     0.06     0.94     0.76\n",
      "2    RBF3         42.0         89.0                65.5                3.69            70.0            60.0         0.83     0.03     0.88     0.78\n",
      "3    RBF4         59.0         60.0                59.5               10.39            70.0            40.0         0.59     0.10     0.73     0.48\n",
      "4    RBF5         59.0         98.0                78.5                6.26            85.0            65.0         0.90     0.03     0.95     0.84\n",
      "..    ...          ...          ...                 ...                 ...             ...             ...          ...      ...      ...      ...\n",
      "76  OLD23          0.0          0.0                50.0                0.00            50.0            50.0         0.31     0.00     0.31     0.31\n",
      "77  OLD24          0.0          0.0                50.0                0.00            50.0            50.0         0.32     0.00     0.32     0.32\n",
      "78  OLD25          0.0          0.0                50.0                0.00            50.0            50.0         0.25     0.00     0.25     0.25\n",
      "79  OLD26          0.0          0.0                50.0                0.00            50.0            50.0         0.29     0.00     0.28     0.28\n",
      "80  OLD27          0.0          0.0                50.0                0.00            50.0            50.0         0.27     0.00     0.27     0.27\n",
      "\n",
      "[81 rows x 11 columns]\n",
      "           Model  Average TNR  Average TPR  Average Test Score  Test Score STD  Test Score MAX  Test Score MIN  Average AUC  AUC STD  Max AUC  Min AUC\n",
      "0        RBF_1_3        53.33        86.00               69.67            1.38            80.0            55.0         0.81     0.02     0.94     0.70\n",
      "1      RBF_10_12        61.33        87.00               74.17            1.87            95.0            55.0         0.86     0.03     0.98     0.71\n",
      "2      RBF_19_21        69.33        65.00               67.17            1.62            85.0            55.0         0.76     0.00     0.87     0.60\n",
      "3        RBF_4_6        53.33        86.00               69.67            3.63            85.0            40.0         0.81     0.04     1.00     0.48\n",
      "4      RBF_13_15        61.33        73.67               67.50            1.31            95.0            45.0         0.71     0.05     0.99     0.47\n",
      "5      RBF_22_24        69.33        61.67               65.50            0.29            85.0            40.0         0.74     0.01     0.92     0.43\n",
      "6        RBF_7_9        82.00        73.67               77.83            2.56            95.0            50.0         0.85     0.03     1.00     0.56\n",
      "7      RBF_16_18        83.67        43.33               63.50            1.83            90.0            40.0         0.75     0.01     0.90     0.54\n",
      "8      RBF_25_27        85.33        10.00               47.67            1.81            60.0            35.0         0.62     0.02     0.73     0.50\n",
      "9     LINEAR_1_3        47.00        97.67               72.33            1.82            85.0            65.0         0.90     0.01     0.98     0.72\n",
      "10  LINEAR_10_12        53.33        95.33               74.33            1.56            85.0            60.0         0.93     0.03     1.00     0.78\n",
      "11  LINEAR_19_21        59.67        63.33               61.50            2.48            75.0            50.0         0.72     0.02     0.82     0.61\n",
      "12    LINEAR_4_6        47.00        96.67               71.83            1.65            85.0            65.0         0.88     0.03     0.97     0.69\n",
      "13  LINEAR_13_15        53.33        75.67               64.50            1.84            85.0            45.0         0.72     0.01     0.99     0.55\n",
      "14  LINEAR_22_24        59.67        80.33               70.00            0.55            85.0            50.0         0.75     0.01     0.90     0.56\n",
      "15    LINEAR_7_9        44.00        93.00               68.50            1.13            95.0            50.0         0.89     0.01     1.00     0.64\n",
      "16  LINEAR_16_18        61.00        60.00               60.50            1.46            90.0            45.0         0.72     0.01     0.86     0.56\n",
      "17  LINEAR_25_27        78.00         2.67               40.33            1.40            50.0            25.0         0.50     0.02     0.65     0.35\n",
      "18       OLD_1_3         0.00         0.00               60.00            0.00            70.0            50.0         0.49     0.00     0.57     0.34\n",
      "19     OLD_10_12         0.00         0.00               51.67            0.00            55.0            50.0         0.34     0.00     0.42     0.30\n",
      "20     OLD_19_21         0.00         0.00               53.33            0.00            60.0            50.0         0.36     0.00     0.45     0.31\n",
      "21       OLD_4_6         0.00         0.00               68.33            0.00            80.0            50.0         0.61     0.00     0.78     0.34\n",
      "22     OLD_13_15         0.00         0.00               51.67            0.00            55.0            50.0         0.30     0.00     0.33     0.28\n",
      "23     OLD_22_24         0.00         0.00               51.67            0.00            55.0            50.0         0.36     0.00     0.45     0.31\n",
      "24       OLD_7_9         0.00         0.00               51.67            0.00            55.0            50.0         0.30     0.00     0.48     0.14\n",
      "25     OLD_16_18         0.00         0.00               51.67            0.00            55.0            50.0         0.24     0.00     0.30     0.15\n",
      "26     OLD_25_27         0.00         0.00               50.00            0.00            50.0            50.0         0.27     0.00     0.28     0.25\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第2位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': [f'RBF{i}' for i in range(1, 28)] + [f'LINEAR{i}' for i in range(1, 28)] + [f'OLD{i}' for i in range(1, 28)],\n",
    "    'Average TNR': [\n",
    "        round(results[f'tnr_rbf{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tnr_linear{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tnr_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average TPR': [\n",
    "        round(results[f'tpr_rbf{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tpr_linear{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'tpr_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].mean() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Standard Deviation': [\n",
    "        round(results[f'Score_RBF{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].std() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Max Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].max() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Min Test Score': [\n",
    "        round(results[f'Score_RBF{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_LINEAR{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'Score_old{i}'].min() * 100, 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Average AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].mean(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'AUC STD': [\n",
    "        round(results[f'AUC_RBF{i}'].std(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].std(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].std(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Max AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].max(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].max(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].max(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "    'Min AUC': [\n",
    "        round(results[f'AUC_RBF{i}'].min(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_LINEAR{i}'].min(), 2) for i in range(1, 28)\n",
    "    ] + [\n",
    "        round(results[f'AUC_old{i}'].min(), 2) for i in range(1, 28)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)\n",
    "\n",
    "\n",
    "\n",
    "# 関数を定義して、各セグメントの統計情報を計算\n",
    "def calculate_statistics(segment, prefix):\n",
    "    # モデル番号を抽出してフラットなリストに変換\n",
    "    model_numbers = statistics_df['Model'].str.extract(r'(\\d+)').astype(int)[0]\n",
    "    is_in_segment = model_numbers.isin(segment)\n",
    "    is_correct_prefix = statistics_df['Model'].str.startswith(prefix)\n",
    "    \n",
    "    tnr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TNR'].mean(), 2)\n",
    "    tpr_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average TPR'].mean(), 2)\n",
    "    acc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average Test Score'].mean(), 2)\n",
    "    acc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Standard Deviation'].std(), 2)\n",
    "    \n",
    "    acc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max Test Score'].max(), 2)\n",
    "    acc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min Test Score'].min(), 2)\n",
    "\n",
    "    auc_mean = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Average AUC'].mean(), 2)\n",
    "    auc_std = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'AUC STD'].std(), 2)\n",
    "    auc_max = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Max AUC'].max(), 2)\n",
    "    auc_min = round(statistics_df.loc[is_correct_prefix & is_in_segment, 'Min AUC'].min(), 2)\n",
    "    \n",
    "    return tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min\n",
    "\n",
    "\n",
    "# segments = {\n",
    "#     '1_10': list(range(1, 10)),\n",
    "#     '10_19': list(range(10, 19)),\n",
    "#     '19_28': list(range(19, 28))\n",
    "# }\n",
    "\n",
    "segments = {\n",
    "    '1_3': list(range(1, 4)),\n",
    "    '10_12': list(range(10, 13)),\n",
    "    '19_21': list(range(19, 22)),\n",
    "    \n",
    "    '4_6': list(range(4, 7)),\n",
    "    '13_15': list(range(13, 16)),\n",
    "    '22_24': list(range(22, 25)),\n",
    "    \n",
    "    '7_9': list(range(7, 10)),\n",
    "    '16_18': list(range(16, 19)),\n",
    "    '25_27': list(range(25, 28))\n",
    "}\n",
    "\n",
    "# 結果を保存するリスト\n",
    "results_summary = []\n",
    "\n",
    "# 統計情報を計算して表示\n",
    "for model in ['RBF', 'LINEAR', 'OLD']:\n",
    "    for segment_name, segment in segments.items():\n",
    "        tnr_mean, tpr_mean, acc_mean, acc_std, acc_max, acc_min, auc_mean, auc_std, auc_max, auc_min = calculate_statistics(segment, model)\n",
    "        results_summary.append({\n",
    "            'Model': f'{model}_{segment_name}',\n",
    "            'Average TNR': tnr_mean,\n",
    "            'Average TPR': tpr_mean,\n",
    "            'Average Test Score': acc_mean,\n",
    "            'Test Score STD': acc_std,\n",
    "            'Test Score MAX': acc_max,\n",
    "            'Test Score MIN': acc_min,\n",
    "            'Average AUC': auc_mean,\n",
    "            'AUC STD': auc_std,\n",
    "            'Max AUC': auc_max,\n",
    "            'Min AUC': auc_min\n",
    "        })\n",
    "\n",
    "# DataFrameに変換\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# 表示\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     100\n",
      "1      10\n",
      "2      10\n",
      "3      10\n",
      "4      10\n",
      "5    1000\n",
      "6      10\n",
      "7     100\n",
      "8     100\n",
      "9     100\n",
      "Name: C_RBF1, dtype: object\n",
      "0      10\n",
      "1      10\n",
      "2    2000\n",
      "3      10\n",
      "4     100\n",
      "5     100\n",
      "6     100\n",
      "7     100\n",
      "8     100\n",
      "9    3000\n",
      "Name: C_LINEAR1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF1'])\n",
    "print(results['C_LINEAR1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
