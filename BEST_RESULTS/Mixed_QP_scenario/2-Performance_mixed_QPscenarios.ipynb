{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['266', '16', '104', '183', '142', '100', '233', '139', '98', '297', '209', '116', '148', '229', '284', '177', '300', '293', '237', '35', '202', '283', '243', '126', '145', '164', '28', '72', '285', '120'], ['87', '107', '217', '20', '172', '49', '273', '171', '186', '31', '86', '41', '108', '45', '230', '203', '4', '213', '210', '298', '159', '110', '251', '59', '78', '240', '155', '299', '111', '9'], ['277', '288', '232', '182', '259', '212', '227', '274', '29', '295', '262', '1', '113', '137', '294', '189', '188', '231', '117', '254', '191', '257', '19', '27', '271', '14', '281', '264', '197', '218'], ['128', '226', '167', '22', '289', '105', '207', '94', '206', '282', '56', '112', '248', '247', '11', '129', '287', '23', '138', '12', '234', '18', '93', '258', '157', '133', '124', '57', '169', '166'], ['97', '6', '146', '130', '74', '33', '77', '39', '88', '103', '175', '89', '40', '221', '38', '223', '219', '132', '122', '66', '109', '201', '170', '58', '180', '275', '140', '17', '196', '263'], ['279', '85', '149', '48', '152', '290', '141', '241', '143', '245', '123', '150', '63', '26', '7', '296', '119', '51', '53', '80', '268', '165', '15', '222', '235', '42', '199', '215', '253', '134'], ['272', '181', '135', '153', '185', '50', '156', '228', '75', '187', '5', '101', '244', '60', '71', '280', '225', '47', '278', '62', '256', '176', '250', '68', '168', '3', '114', '73', '270', '34'], ['260', '118', '246', '69', '252', '193', '55', '236', '174', '92', '147', '90', '54', '32', '160', '242', '30', '261', '216', '61', '21', '276', '179', '43', '158', '161', '265', '70', '173', '8'], ['249', '83', '2', '292', '24', '127', '13', '96', '36', '224', '239', '95', '67', '198', '286', '200', '184', '121', '163', '267', '214', '52', '154', '82', '178', '44', '46', '99', '190', '205'], ['79', '91', '131', '151', '106', '195', '64', '102', '76', '65', '115', '10', '255', '192', '81', '211', '269', '162', '220', '194', '144', '208', '25', '37', '136', '84', '125', '291', '238', '204']]\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "single_list1 = []\n",
    "single_list2 = []\n",
    "single_list3 = []\n",
    "single_list4 = []\n",
    "single_list5 = []\n",
    "single_list6 = []\n",
    "single_list7 = []\n",
    "single_list8 = []\n",
    "single_list9 = []\n",
    "single_list10 = []\n",
    "\n",
    "single_recompress_list1 = []\n",
    "single_recompress_list2 = []\n",
    "single_recompress_list3 = []\n",
    "single_recompress_list4 = []\n",
    "single_recompress_list5 = []\n",
    "single_recompress_list6 = []\n",
    "single_recompress_list7 = []\n",
    "single_recompress_list8 = []\n",
    "single_recompress_list9 = []\n",
    "single_recompress_list10 = []\n",
    "\n",
    "second_largeQP1_list1 = []\n",
    "second_largeQP1_list2 = []\n",
    "second_largeQP1_list3 = []\n",
    "second_largeQP1_list4 = []\n",
    "second_largeQP1_list5 = []\n",
    "second_largeQP1_list6 = []\n",
    "second_largeQP1_list7 = []\n",
    "second_largeQP1_list8 = []\n",
    "second_largeQP1_list9 = []\n",
    "second_largeQP1_list10 = []\n",
    "\n",
    "second_recompress_largeQP1_list1 = []\n",
    "second_recompress_largeQP1_list2 = []\n",
    "second_recompress_largeQP1_list3 = []\n",
    "second_recompress_largeQP1_list4 = []\n",
    "second_recompress_largeQP1_list5 = []\n",
    "second_recompress_largeQP1_list6 = []\n",
    "second_recompress_largeQP1_list7 = []\n",
    "second_recompress_largeQP1_list8 = []\n",
    "second_recompress_largeQP1_list9 = []\n",
    "second_recompress_largeQP1_list10 = []\n",
    "\n",
    "second_sameQP_list1 = []\n",
    "second_sameQP_list2 = []\n",
    "second_sameQP_list3 = []\n",
    "second_sameQP_list4 = []\n",
    "second_sameQP_list5 = []\n",
    "second_sameQP_list6 = []\n",
    "second_sameQP_list7 = []\n",
    "second_sameQP_list8 = []\n",
    "second_sameQP_list9 = []\n",
    "second_sameQP_list10 = []\n",
    "\n",
    "second_recompress_sameQP_list1 = []\n",
    "second_recompress_sameQP_list2 = []\n",
    "second_recompress_sameQP_list3 = []\n",
    "second_recompress_sameQP_list4 = []\n",
    "second_recompress_sameQP_list5 = []\n",
    "second_recompress_sameQP_list6 = []\n",
    "second_recompress_sameQP_list7 = []\n",
    "second_recompress_sameQP_list8 = []\n",
    "second_recompress_sameQP_list9 = []\n",
    "second_recompress_sameQP_list10 = []\n",
    "\n",
    "second_largeQP2_list1 = []\n",
    "second_largeQP2_list2 = []\n",
    "second_largeQP2_list3 = []\n",
    "second_largeQP2_list4 = []\n",
    "second_largeQP2_list5 = []\n",
    "second_largeQP2_list6 = []\n",
    "second_largeQP2_list7 = []\n",
    "second_largeQP2_list8 = []\n",
    "second_largeQP2_list9 = []\n",
    "second_largeQP2_list10 = []\n",
    "\n",
    "second_recompress_largeQP2_list1 = []\n",
    "second_recompress_largeQP2_list2 = []\n",
    "second_recompress_largeQP2_list3 = []\n",
    "second_recompress_largeQP2_list4 = []\n",
    "second_recompress_largeQP2_list5 = []\n",
    "second_recompress_largeQP2_list6 = []\n",
    "second_recompress_largeQP2_list7 = []\n",
    "second_recompress_largeQP2_list8 = []\n",
    "second_recompress_largeQP2_list9 = []\n",
    "second_recompress_largeQP2_list10 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_lists(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath_csv+f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath_csv+f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath_csv+f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath_csv+f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath_csv+f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath_csv+f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath_csv+f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath_csv+f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "            \n",
    "\n",
    "# train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5, train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "single_lists = [single_list1, single_list2,\n",
    "                single_list3, single_list4,\n",
    "                single_list5, single_list6,\n",
    "                single_list7, single_list8,\n",
    "                single_list9, single_list10]\n",
    "\n",
    "single_recompress_lists = [single_recompress_list1, single_recompress_list2,\n",
    "                           single_recompress_list3, single_recompress_list4,\n",
    "                           single_recompress_list5, single_recompress_list6,\n",
    "                           single_recompress_list7, single_recompress_list8,\n",
    "                           single_recompress_list9, single_recompress_list10]\n",
    "\n",
    "\n",
    "second_largeQP1_lists = [second_largeQP1_list1, second_largeQP1_list2,\n",
    "                        second_largeQP1_list3, second_largeQP1_list4,\n",
    "                        second_largeQP1_list5, second_largeQP1_list6,\n",
    "                        second_largeQP1_list7, second_largeQP1_list8,\n",
    "                        second_largeQP1_list9, second_largeQP1_list10]\n",
    "\n",
    "second_recompress_largeQP1_lists = [second_recompress_largeQP1_list1, second_recompress_largeQP1_list2,\n",
    "                           second_recompress_largeQP1_list3, second_recompress_largeQP1_list4,\n",
    "                           second_recompress_largeQP1_list5, second_recompress_largeQP1_list6,\n",
    "                           second_recompress_largeQP1_list7, second_recompress_largeQP1_list8,\n",
    "                           second_recompress_largeQP1_list9, second_recompress_largeQP1_list10]\n",
    "\n",
    "\n",
    "second_sameQP_lists = [second_sameQP_list1, second_sameQP_list2,\n",
    "                        second_sameQP_list3, second_sameQP_list4,\n",
    "                        second_sameQP_list5, second_sameQP_list6,\n",
    "                        second_sameQP_list7, second_sameQP_list8,\n",
    "                        second_sameQP_list9, second_sameQP_list10]\n",
    "\n",
    "second_recompress_sameQP_lists = [second_recompress_sameQP_list1, second_recompress_sameQP_list2,\n",
    "                           second_recompress_sameQP_list3, second_recompress_sameQP_list4,\n",
    "                           second_recompress_sameQP_list5, second_recompress_sameQP_list6,\n",
    "                           second_recompress_sameQP_list7, second_recompress_sameQP_list8,\n",
    "                           second_recompress_sameQP_list9, second_recompress_sameQP_list10]\n",
    "\n",
    "\n",
    "second_largeQP2_lists = [second_largeQP2_list1, second_largeQP2_list2,\n",
    "                        second_largeQP2_list3, second_largeQP2_list4,\n",
    "                        second_largeQP2_list5, second_largeQP2_list6,\n",
    "                        second_largeQP2_list7, second_largeQP2_list8,\n",
    "                        second_largeQP2_list9, second_largeQP2_list10]\n",
    "\n",
    "second_recompress_largeQP2_lists = [second_recompress_largeQP2_list1, second_recompress_largeQP2_list2,\n",
    "                           second_recompress_largeQP2_list3, second_recompress_largeQP2_list4,\n",
    "                           second_recompress_largeQP2_list5, second_recompress_largeQP2_list6,\n",
    "                           second_recompress_largeQP2_list7, second_recompress_largeQP2_list8,\n",
    "                           second_recompress_largeQP2_list9, second_recompress_largeQP2_list10]\n",
    "\n",
    "\n",
    "\n",
    "for train_list, single_list, single_recompress_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                           single_lists,\n",
    "                                                                                                                                                                                                                           single_recompress_lists,\n",
    "                                                                                                                                                                                                                           second_largeQP1_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP1_lists,\n",
    "                                                                                                                                                                                                                           second_sameQP_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_sameQP_lists,\n",
    "                                                                                                                                                                                                                           second_largeQP2_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP2_lists\n",
    "                                                                                                                                                                                                                          ):\n",
    "    process_train_lists(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "1710\n",
      "1710\n",
      "300\n",
      "300\n",
      "1170\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(single_lists[6]))\n",
    "print(len(single_recompress_lists[6]))\n",
    "print(len(second_largeQP1_lists[6]))\n",
    "print(len(second_recompress_largeQP1_lists[6]))\n",
    "print(len(second_sameQP_lists[6]))\n",
    "print(len(second_recompress_sameQP_lists[6]))\n",
    "print(len(second_largeQP2_lists[6]))\n",
    "print(len(second_recompress_largeQP2_lists[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list1 = []\n",
    "single_list2 = []\n",
    "single_list3 = []\n",
    "single_list4 = []\n",
    "single_list5 = []\n",
    "single_list6 = []\n",
    "single_list7 = []\n",
    "single_list8 = []\n",
    "single_list9 = []\n",
    "single_list10 = []\n",
    "\n",
    "single_recompress_list1 = []\n",
    "single_recompress_list2 = []\n",
    "single_recompress_list3 = []\n",
    "single_recompress_list4 = []\n",
    "single_recompress_list5 = []\n",
    "single_recompress_list6 = []\n",
    "single_recompress_list7 = []\n",
    "single_recompress_list8 = []\n",
    "single_recompress_list9 = []\n",
    "single_recompress_list10 = []\n",
    "\n",
    "second_largeQP1_list1 = []\n",
    "second_largeQP1_list2 = []\n",
    "second_largeQP1_list3 = []\n",
    "second_largeQP1_list4 = []\n",
    "second_largeQP1_list5 = []\n",
    "second_largeQP1_list6 = []\n",
    "second_largeQP1_list7 = []\n",
    "second_largeQP1_list8 = []\n",
    "second_largeQP1_list9 = []\n",
    "second_largeQP1_list10 = []\n",
    "\n",
    "second_recompress_largeQP1_list1 = []\n",
    "second_recompress_largeQP1_list2 = []\n",
    "second_recompress_largeQP1_list3 = []\n",
    "second_recompress_largeQP1_list4 = []\n",
    "second_recompress_largeQP1_list5 = []\n",
    "second_recompress_largeQP1_list6 = []\n",
    "second_recompress_largeQP1_list7 = []\n",
    "second_recompress_largeQP1_list8 = []\n",
    "second_recompress_largeQP1_list9 = []\n",
    "second_recompress_largeQP1_list10 = []\n",
    "\n",
    "second_sameQP_list1 = []\n",
    "second_sameQP_list2 = []\n",
    "second_sameQP_list3 = []\n",
    "second_sameQP_list4 = []\n",
    "second_sameQP_list5 = []\n",
    "second_sameQP_list6 = []\n",
    "second_sameQP_list7 = []\n",
    "second_sameQP_list8 = []\n",
    "second_sameQP_list9 = []\n",
    "second_sameQP_list10 = []\n",
    "\n",
    "second_recompress_sameQP_list1 = []\n",
    "second_recompress_sameQP_list2 = []\n",
    "second_recompress_sameQP_list3 = []\n",
    "second_recompress_sameQP_list4 = []\n",
    "second_recompress_sameQP_list5 = []\n",
    "second_recompress_sameQP_list6 = []\n",
    "second_recompress_sameQP_list7 = []\n",
    "second_recompress_sameQP_list8 = []\n",
    "second_recompress_sameQP_list9 = []\n",
    "second_recompress_sameQP_list10 = []\n",
    "\n",
    "second_largeQP2_list1 = []\n",
    "second_largeQP2_list2 = []\n",
    "second_largeQP2_list3 = []\n",
    "second_largeQP2_list4 = []\n",
    "second_largeQP2_list5 = []\n",
    "second_largeQP2_list6 = []\n",
    "second_largeQP2_list7 = []\n",
    "second_largeQP2_list8 = []\n",
    "second_largeQP2_list9 = []\n",
    "second_largeQP2_list10 = []\n",
    "\n",
    "second_recompress_largeQP2_list1 = []\n",
    "second_recompress_largeQP2_list2 = []\n",
    "second_recompress_largeQP2_list3 = []\n",
    "second_recompress_largeQP2_list4 = []\n",
    "second_recompress_largeQP2_list5 = []\n",
    "second_recompress_largeQP2_list6 = []\n",
    "second_recompress_largeQP2_list7 = []\n",
    "second_recompress_largeQP2_list8 = []\n",
    "second_recompress_largeQP2_list9 = []\n",
    "second_recompress_largeQP2_list10 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "\n",
    "def process_train_lists2(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath_pkl+f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath_pkl+f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath_pkl+f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath_pkl+f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath_pkl+f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath_pkl+f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath_pkl+f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath_pkl+f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "            \n",
    "\n",
    "# train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5, train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "single_listsA = [single_list1, single_list2,\n",
    "                single_list3, single_list4,\n",
    "                single_list5, single_list6,\n",
    "                single_list7, single_list8,\n",
    "                single_list9, single_list10]\n",
    "\n",
    "single_recompress_listsA = [single_recompress_list1, single_recompress_list2,\n",
    "                           single_recompress_list3, single_recompress_list4,\n",
    "                           single_recompress_list5, single_recompress_list6,\n",
    "                           single_recompress_list7, single_recompress_list8,\n",
    "                           single_recompress_list9, single_recompress_list10]\n",
    "\n",
    "\n",
    "second_largeQP1_listsA = [second_largeQP1_list1, second_largeQP1_list2,\n",
    "                        second_largeQP1_list3, second_largeQP1_list4,\n",
    "                        second_largeQP1_list5, second_largeQP1_list6,\n",
    "                        second_largeQP1_list7, second_largeQP1_list8,\n",
    "                        second_largeQP1_list9, second_largeQP1_list10]\n",
    "\n",
    "second_recompress_largeQP1_listsA = [second_recompress_largeQP1_list1, second_recompress_largeQP1_list2,\n",
    "                           second_recompress_largeQP1_list3, second_recompress_largeQP1_list4,\n",
    "                           second_recompress_largeQP1_list5, second_recompress_largeQP1_list6,\n",
    "                           second_recompress_largeQP1_list7, second_recompress_largeQP1_list8,\n",
    "                           second_recompress_largeQP1_list9, second_recompress_largeQP1_list10]\n",
    "\n",
    "\n",
    "second_sameQP_listsA = [second_sameQP_list1, second_sameQP_list2,\n",
    "                        second_sameQP_list3, second_sameQP_list4,\n",
    "                        second_sameQP_list5, second_sameQP_list6,\n",
    "                        second_sameQP_list7, second_sameQP_list8,\n",
    "                        second_sameQP_list9, second_sameQP_list10]\n",
    "\n",
    "second_recompress_sameQP_listsA = [second_recompress_sameQP_list1, second_recompress_sameQP_list2,\n",
    "                           second_recompress_sameQP_list3, second_recompress_sameQP_list4,\n",
    "                           second_recompress_sameQP_list5, second_recompress_sameQP_list6,\n",
    "                           second_recompress_sameQP_list7, second_recompress_sameQP_list8,\n",
    "                           second_recompress_sameQP_list9, second_recompress_sameQP_list10]\n",
    "\n",
    "\n",
    "second_largeQP2_listsA = [second_largeQP2_list1, second_largeQP2_list2,\n",
    "                        second_largeQP2_list3, second_largeQP2_list4,\n",
    "                        second_largeQP2_list5, second_largeQP2_list6,\n",
    "                        second_largeQP2_list7, second_largeQP2_list8,\n",
    "                        second_largeQP2_list9, second_largeQP2_list10]\n",
    "\n",
    "second_recompress_largeQP2_listsA = [second_recompress_largeQP2_list1, second_recompress_largeQP2_list2,\n",
    "                           second_recompress_largeQP2_list3, second_recompress_largeQP2_list4,\n",
    "                           second_recompress_largeQP2_list5, second_recompress_largeQP2_list6,\n",
    "                           second_recompress_largeQP2_list7, second_recompress_largeQP2_list8,\n",
    "                           second_recompress_largeQP2_list9, second_recompress_largeQP2_list10]\n",
    "\n",
    "\n",
    "\n",
    "for train_list, single_list, single_recompress_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                           single_listsA,\n",
    "                                                                                                                                                                                                                           single_recompress_listsA,\n",
    "                                                                                                                                                                                                                           second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                           second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                           second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP2_listsA\n",
    "                                                                                                                                                                                                                          ):\n",
    "    process_train_lists2(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "1710\n",
      "1710\n",
      "300\n",
      "300\n",
      "1170\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(single_listsA[6]))\n",
    "print(len(single_recompress_listsA[6]))\n",
    "print(len(second_largeQP1_listsA[6]))\n",
    "print(len(second_recompress_largeQP1_listsA[6]))\n",
    "print(len(second_sameQP_listsA[6]))\n",
    "print(len(second_recompress_sameQP_listsA[6]))\n",
    "print(len(second_largeQP2_listsA[6]))\n",
    "print(len(second_recompress_largeQP2_listsA[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "100\n",
      "100\n",
      "100\n",
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "single_csv1 = list(zip(single_lists[0], single_listsA[0], single_recompress_lists[0], single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(single_lists[1], single_listsA[1], single_recompress_lists[1], single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(single_lists[2], single_listsA[2], single_recompress_lists[2], single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(single_lists[3], single_listsA[3], single_recompress_lists[3], single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(single_lists[4], single_listsA[4], single_recompress_lists[4], single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(single_lists[5], single_listsA[5], single_recompress_lists[5], single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(single_lists[6], single_listsA[6], single_recompress_lists[6], single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(single_lists[7], single_listsA[7], single_recompress_lists[7], single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(single_lists[8], single_listsA[8], single_recompress_lists[8], single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(single_lists[9], single_listsA[9], single_recompress_lists[9], single_recompress_listsA[9]))\n",
    "print(len(single_csv7))\n",
    "\n",
    "second_largeQP1_csv1 = list(zip(second_largeQP1_lists[0], second_largeQP1_listsA[0], second_recompress_largeQP1_lists[0], second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(second_largeQP1_lists[1], second_largeQP1_listsA[1], second_recompress_largeQP1_lists[1], second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(second_largeQP1_lists[2], second_largeQP1_listsA[2], second_recompress_largeQP1_lists[2], second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(second_largeQP1_lists[3], second_largeQP1_listsA[3], second_recompress_largeQP1_lists[3], second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(second_largeQP1_lists[4], second_largeQP1_listsA[4], second_recompress_largeQP1_lists[4], second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(second_largeQP1_lists[5], second_largeQP1_listsA[5], second_recompress_largeQP1_lists[5], second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(second_largeQP1_lists[6], second_largeQP1_listsA[6], second_recompress_largeQP1_lists[6], second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(second_largeQP1_lists[7], second_largeQP1_listsA[7], second_recompress_largeQP1_lists[7], second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(second_largeQP1_lists[8], second_largeQP1_listsA[8], second_recompress_largeQP1_lists[8], second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(second_largeQP1_lists[9], second_largeQP1_listsA[9], second_recompress_largeQP1_lists[9], second_recompress_largeQP1_listsA[9]))\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 100)\n",
    "print(len(second_largeQP1_csv7))\n",
    "\n",
    "second_sameQP_csv1 = list(zip(second_sameQP_lists[0], second_sameQP_listsA[0], second_recompress_sameQP_lists[0], second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(second_sameQP_lists[1], second_sameQP_listsA[1], second_recompress_sameQP_lists[1], second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(second_sameQP_lists[2], second_sameQP_listsA[2], second_recompress_sameQP_lists[2], second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(second_sameQP_lists[3], second_sameQP_listsA[3], second_recompress_sameQP_lists[3], second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(second_sameQP_lists[4], second_sameQP_listsA[4], second_recompress_sameQP_lists[4], second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(second_sameQP_lists[5], second_sameQP_listsA[5], second_recompress_sameQP_lists[5], second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(second_sameQP_lists[6], second_sameQP_listsA[6], second_recompress_sameQP_lists[6], second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(second_sameQP_lists[7], second_sameQP_listsA[7], second_recompress_sameQP_lists[7], second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(second_sameQP_lists[8], second_sameQP_listsA[8], second_recompress_sameQP_lists[8], second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(second_sameQP_lists[9], second_sameQP_listsA[9], second_recompress_sameQP_lists[9], second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 100)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 100)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 100)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 100)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 100)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 100)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 100)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 100)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 100)\n",
    "second_sameQP_csv10 = random.sample(second_sameQP_csv10, 100)\n",
    "print(len(second_sameQP_csv7))\n",
    "\n",
    "second_largeQP2_csv1 = list(zip(second_largeQP2_lists[0], second_largeQP2_listsA[0], second_recompress_largeQP2_lists[0], second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(second_largeQP2_lists[1], second_largeQP2_listsA[1], second_recompress_largeQP2_lists[1], second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(second_largeQP2_lists[2], second_largeQP2_listsA[2], second_recompress_largeQP2_lists[2], second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(second_largeQP2_lists[3], second_largeQP2_listsA[3], second_recompress_largeQP2_lists[3], second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(second_largeQP2_lists[4], second_largeQP2_listsA[4], second_recompress_largeQP2_lists[4], second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(second_largeQP2_lists[5], second_largeQP2_listsA[5], second_recompress_largeQP2_lists[5], second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(second_largeQP2_lists[6], second_largeQP2_listsA[6], second_recompress_largeQP2_lists[6], second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(second_largeQP2_lists[7], second_largeQP2_listsA[7], second_recompress_largeQP2_lists[7], second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(second_largeQP2_lists[8], second_largeQP2_listsA[8], second_recompress_largeQP2_lists[8], second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(second_largeQP2_lists[9], second_largeQP2_listsA[9], second_recompress_largeQP2_lists[9], second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 100)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 100)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 100)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 100)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 100)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 100)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 100)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 100)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 100)\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 100)\n",
    "print(len(second_largeQP2_csv7))\n",
    "\n",
    "\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv10 + second_largeQP2_csv10\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10  0.000117  0.001725   0.000397  0.077808  0.049714\n",
      "1         16  0.000113  0.002531     0.0006  0.046845  0.023674\n",
      "2         20  0.000121  0.001922   0.000495  0.106747   0.08973\n",
      "3         24  0.000024  0.002134   0.000411  0.093607  0.093301\n",
      "4         27  0.000088  0.003461   0.001467  0.227197   0.21089\n",
      "..       ...       ...       ...        ...       ...       ...\n",
      "595       45  0.006565  0.004924   0.008172  0.129242  0.117437\n",
      "596       45  0.005415  0.013404   0.008618  0.117051  0.113138\n",
      "597       42  0.000226  0.004042   0.003488  0.162958  0.161254\n",
      "598       20  0.000995  0.000563    0.00574   0.07299   0.02724\n",
      "599       16  0.000069  0.000269   0.001488  0.029769  0.015148\n",
      "\n",
      "[600 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データフレームを結合\n",
    "combined_train_df = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], ignore_index=True)\n",
    "combined_train_df_onlyGhost = pd.concat([train_df_onlyGhost1, train_df_onlyGhost2, train_df_onlyGhost3, train_df_onlyGhost4, train_df_onlyGhost5, train_df_onlyGhost6, train_df_onlyGhost7, train_df_onlyGhost8, train_df_onlyGhost9, train_df_onlyGhost10], ignore_index=True)\n",
    "combined_LABEL = pd.concat([LABEL1, LABEL2, LABEL3, LABEL4, LABEL5, LABEL6, LABEL7, LABEL8, LABEL9, LABEL10], ignore_index=True)\n",
    "combined_MAE = pd.concat([MAE1, MAE2, MAE3, MAE4, MAE5, MAE6, MAE7, MAE8, MAE9, MAE10], ignore_index=True)\n",
    "combined_FINAL_QP = pd.concat([FINAL_QP1, FINAL_QP2, FINAL_QP3, FINAL_QP4, FINAL_QP5, FINAL_QP6, FINAL_QP7, FINAL_QP8, FINAL_QP9, FINAL_QP10], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 6)\n",
      "(6000, 44)\n",
      "(6000, 1)\n",
      "(6000, 1)\n",
      "(6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df.shape)\n",
    "print(combined_train_df_onlyGhost.shape)\n",
    "print(combined_LABEL.shape)\n",
    "print(combined_MAE.shape)\n",
    "print(combined_FINAL_QP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "Combined Train DF:\n",
      "  FINAL_QP    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10  0.000117  0.001725   0.000397  0.077808  0.049714\n",
      "1       16  0.000113  0.002531     0.0006  0.046845  0.023674\n",
      "2       20  0.000121  0.001922   0.000495  0.106747   0.08973\n",
      "3       24  0.000024  0.002134   0.000411  0.093607  0.093301\n",
      "4       27  0.000088  0.003461   0.001467  0.227197   0.21089\n",
      "Combined Train DF Only Ghost:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0  LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0  LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0  CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10      0   3264   6448  19628  30660      0   3328   6288  19344  31040   9501   8496  1943   4189   1407   1132   2501    963   9804   9334  1988   4163   1366   1132   3044    965  14440  10596   7096   7260   2308  18300  13872  10900   7292   7280   2464  18192  0.000117  0.001725   0.000397  0.077808  0.049714\n",
      "1       16      0   9792  10560  19300  20348      0   9856  10240  19296  20608  10156  12064  1611   3919   1594    969   3170    937  10493  12804  1650   3793   1582    936   2598    950   7948   5700   3816   3160   1760  37616   7504   5772   3472   3216   1764  38272  0.000113  0.002531     0.0006  0.046845  0.023674\n",
      "2       20      0  13952  16320  15484  14244      0  14144  16592  15272  13992  10662  12601  1803   4442   1303   1067   3031    770  10814  14293  1906   4600   1232   1085   3351    661   6460   3740   2724   2084   1464  43528   6256   4072   2792   2060   1288  43532  0.000121  0.001922   0.000495  0.106747   0.08973\n",
      "3       24      0  19712  17136  12696  10456      0  19776  17200  12724  10300  12343   9795  1877   5104   1326   1235   2998    847  12555  10767  1873   5332   1312   1194   3763    844   5368   3284   1920   1120    816  47492   4932   3408   2000   1132    772  47756  0.000024  0.002134   0.000411  0.093607  0.093301\n",
      "4       27      0  21952  16784  12236   9028      0  22272  16816  12028   8884  14447   8440  1789   5296   1489    889   4312    752  14420  10026  1646   5830   1422   1016   4271    691   3628   2692    876   1352   1008  50444   3456   2720   1092   1404    700  50628  0.000088  0.003461   0.001467  0.227197   0.21089\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\")\n",
    "print(\"Combined Train DF:\")\n",
    "print(combined_train_df.head())\n",
    "print(\"Combined Train DF Only Ghost:\")\n",
    "print(combined_train_df_onlyGhost.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling:\n",
      "X_train:\n",
      "       0         1         2         3         4         5\n",
      "0  0.125  0.006585  0.009921  0.003682  0.076446  0.048521\n",
      "1  0.275  0.006357  0.014569  0.005581  0.045436  0.022446\n",
      "2  0.375  0.006794  0.011058  0.004599  0.105429  0.088592\n",
      "3  0.475  0.001335  0.012281  0.003814  0.092269  0.092168\n",
      "4  0.550  0.004942  0.019929  0.013700  0.226062  0.209915\n",
      "X_train_onlyGhost:\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.125  0.0  0.055375  0.207216  0.680394  0.547774  0.0  0.056460  0.206842  0.660295  0.540880  0.225043  0.192582  0.147713  0.161789  0.115692  0.078573  0.077077  0.107562  0.252841  0.210258  0.152571  0.162764  0.113424  0.076741  0.091591  0.105846  0.395227  0.428571  0.381014  0.409984  0.131856  0.274883  0.363522  0.430830  0.374410  0.400705  0.155634  0.274015  0.006585  0.009921  0.003682  0.076446  0.048521\n",
      "1  0.275  0.0  0.166124  0.339691  0.669024  0.363539  0.0  0.167210  0.336842  0.658656  0.359099  0.246252  0.298116  0.122355  0.151351  0.131112  0.067259  0.100504  0.104658  0.276929  0.312542  0.126513  0.148129  0.131412  0.063453  0.076976  0.104201  0.217539  0.230545  0.204897  0.178450  0.100548  0.610767  0.196646  0.228142  0.178271  0.177015  0.111420  0.622699  0.006357  0.014569  0.005581  0.045436  0.022446\n",
      "2  0.375  0.0  0.236699  0.525258  0.536744  0.254484  0.0  0.239957  0.545789  0.521300  0.243814  0.262636  0.313999  0.137020  0.171570  0.107116  0.074061  0.095637  0.086005  0.288152  0.356433  0.146249  0.180049  0.102265  0.073554  0.101652  0.072502  0.176812  0.151270  0.146263  0.117687  0.083638  0.713570  0.163941  0.160949  0.143356  0.113386  0.081354  0.714038  0.006794  0.011058  0.004599  0.105429  0.088592\n",
      "3  0.475  0.0  0.334419  0.551546  0.440100  0.186808  0.0  0.335505  0.565789  0.434326  0.179480  0.317068  0.231004  0.142672  0.197162  0.109013  0.085722  0.094481  0.094605  0.349019  0.252498  0.143705  0.209002  0.108927  0.080944  0.115153  0.092574  0.146924  0.132826  0.103093  0.063248  0.046618  0.782500  0.129245  0.134704  0.102690  0.062307  0.048762  0.787386  0.001335  0.012281  0.003814  0.092269  0.092168\n",
      "4  0.550  0.0  0.372421  0.540206  0.424154  0.161295  0.0  0.377850  0.553158  0.410568  0.154806  0.385196  0.190925  0.135951  0.204585  0.122454  0.061706  0.140496  0.083994  0.414222  0.230656  0.126205  0.228700  0.118088  0.068877  0.131800  0.075792  0.099299  0.108882  0.047036  0.076350  0.057587  0.833832  0.090566  0.107510  0.056069  0.077279  0.044214  0.837258  0.004942  0.019929  0.013700  0.226062  0.209915\n",
      "Length of X_train_restored[0]: 600\n",
      "Length of X_train_onlyGhost_restored[0]: 600\n",
      "Length of MAE_restored[0]: 600\n",
      "Length of FINAL_QP_restored[0]: 600\n",
      "Length of Y_train_restored[0]: 600\n",
      "Length of X_train_restored[1]: 600\n",
      "Length of X_train_onlyGhost_restored[1]: 600\n",
      "Length of MAE_restored[1]: 600\n",
      "Length of FINAL_QP_restored[1]: 600\n",
      "Length of Y_train_restored[1]: 600\n",
      "Length of X_train_restored[2]: 600\n",
      "Length of X_train_onlyGhost_restored[2]: 600\n",
      "Length of MAE_restored[2]: 600\n",
      "Length of FINAL_QP_restored[2]: 600\n",
      "Length of Y_train_restored[2]: 600\n",
      "Length of X_train_restored[3]: 600\n",
      "Length of X_train_onlyGhost_restored[3]: 600\n",
      "Length of MAE_restored[3]: 600\n",
      "Length of FINAL_QP_restored[3]: 600\n",
      "Length of Y_train_restored[3]: 600\n",
      "Length of X_train_restored[4]: 600\n",
      "Length of X_train_onlyGhost_restored[4]: 600\n",
      "Length of MAE_restored[4]: 600\n",
      "Length of FINAL_QP_restored[4]: 600\n",
      "Length of Y_train_restored[4]: 600\n",
      "Length of X_train_restored[5]: 600\n",
      "Length of X_train_onlyGhost_restored[5]: 600\n",
      "Length of MAE_restored[5]: 600\n",
      "Length of FINAL_QP_restored[5]: 600\n",
      "Length of Y_train_restored[5]: 600\n",
      "Length of X_train_restored[6]: 600\n",
      "Length of X_train_onlyGhost_restored[6]: 600\n",
      "Length of MAE_restored[6]: 600\n",
      "Length of FINAL_QP_restored[6]: 600\n",
      "Length of Y_train_restored[6]: 600\n",
      "Length of X_train_restored[7]: 600\n",
      "Length of X_train_onlyGhost_restored[7]: 600\n",
      "Length of MAE_restored[7]: 600\n",
      "Length of FINAL_QP_restored[7]: 600\n",
      "Length of Y_train_restored[7]: 600\n",
      "Length of X_train_restored[8]: 600\n",
      "Length of X_train_onlyGhost_restored[8]: 600\n",
      "Length of MAE_restored[8]: 600\n",
      "Length of FINAL_QP_restored[8]: 600\n",
      "Length of Y_train_restored[8]: 600\n",
      "Length of X_train_restored[9]: 600\n",
      "Length of X_train_onlyGhost_restored[9]: 600\n",
      "Length of MAE_restored[9]: 600\n",
      "Length of FINAL_QP_restored[9]: 600\n",
      "Length of Y_train_restored[9]: 600\n"
     ]
    }
   ],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP, scaler_main=None, scaler_ghost=None, fit_scaler=True):\n",
    "\n",
    "    scaler_main = MinMaxScaler()\n",
    "    scaler_ghost = MinMaxScaler()\n",
    "    X_train = scaler_main.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler_ghost.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train, scaler_main, scaler_ghost\n",
    "\n",
    "# スケーリングの処理\n",
    "X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train, scaler_main, scaler_ghost = process_results_to_lists(\n",
    "    combined_train_df, combined_train_df_onlyGhost, combined_LABEL, combined_MAE, combined_FINAL_QP, fit_scaler=True\n",
    ")\n",
    "\n",
    "# スケーリング後のデータを表示\n",
    "print(\"After scaling:\")\n",
    "print(\"X_train:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "print(\"X_train_onlyGhost:\")\n",
    "print(pd.DataFrame(X_train_onlyGhost).head())\n",
    "\n",
    "# データを元に戻すための関数\n",
    "def restore_data_to_original_order(data, num_splits, original_lengths):\n",
    "    restored_data = []\n",
    "    start_index = 0\n",
    "    for length in original_lengths:\n",
    "        restored_data.append(data[start_index:start_index + length])\n",
    "        start_index += length\n",
    "    return restored_data\n",
    "\n",
    "# 元のデータフレームの長さ\n",
    "original_lengths = [len(train_df1), len(train_df2), len(train_df3), len(train_df4), len(train_df5), \n",
    "                    len(train_df6), len(train_df7), len(train_df8), len(train_df9), len(train_df10)]\n",
    "\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "# データを元の順序に戻す\n",
    "X_train_list = restore_data_to_original_order(X_train, 10, original_lengths)\n",
    "X_train_onlyGhost_list = restore_data_to_original_order(X_train_onlyGhost, 10, original_lengths)\n",
    "MAE_list = restore_data_to_original_order(MAE_array, 10, original_lengths)\n",
    "FINAL_QP_list = restore_data_to_original_order(FINAL_QP_array, 10, original_lengths)\n",
    "Y_train_list = restore_data_to_original_order(Y_train, 10, original_lengths)\n",
    "\n",
    "# 確認用の出力\n",
    "for i in range(10):\n",
    "    print(f\"Length of X_train_restored[{i}]: {len(X_train_list[i])}\")\n",
    "    print(f\"Length of X_train_onlyGhost_restored[{i}]: {len(X_train_onlyGhost_list[i])}\")\n",
    "    print(f\"Length of MAE_restored[{i}]: {len(MAE_list[i])}\")\n",
    "    print(f\"Length of FINAL_QP_restored[{i}]: {len(FINAL_QP_list[i])}\")\n",
    "    print(f\"Length of Y_train_restored[{i}]: {len(Y_train_list[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7384    0.8467    0.7888       300\n",
      "           1     0.8203    0.7000    0.7554       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7793    0.7733    0.7721       600\n",
      "weighted avg     0.7793    0.7733    0.7721       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6364    0.7467    0.6871       300\n",
      "           1     0.6935    0.5733    0.6277       300\n",
      "\n",
      "    accuracy                         0.6600       600\n",
      "   macro avg     0.6650    0.6600    0.6574       600\n",
      "weighted avg     0.6650    0.6600    0.6574       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7781    0.8767    0.8245       300\n",
      "           1     0.8588    0.7500    0.8007       300\n",
      "\n",
      "    accuracy                         0.8133       600\n",
      "   macro avg     0.8184    0.8133    0.8126       600\n",
      "weighted avg     0.8184    0.8133    0.8126       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8361    0.8333    0.8347       300\n",
      "           1     0.8339    0.8367    0.8353       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8350    0.8350    0.8350       600\n",
      "weighted avg     0.8350    0.8350    0.8350       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5709    0.9667    0.7178       300\n",
      "           1     0.8913    0.2733    0.4184       300\n",
      "\n",
      "    accuracy                         0.6200       600\n",
      "   macro avg     0.7311    0.6200    0.5681       600\n",
      "weighted avg     0.7311    0.6200    0.5681       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7394    0.8133    0.7746       300\n",
      "           1     0.7926    0.7133    0.7509       300\n",
      "\n",
      "    accuracy                         0.7633       600\n",
      "   macro avg     0.7660    0.7633    0.7627       600\n",
      "weighted avg     0.7660    0.7633    0.7627       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6695    0.7767    0.7191       300\n",
      "           1     0.7341    0.6167    0.6703       300\n",
      "\n",
      "    accuracy                         0.6967       600\n",
      "   macro avg     0.7018    0.6967    0.6947       600\n",
      "weighted avg     0.7018    0.6967    0.6947       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8367    0.8367    0.8367       300\n",
      "           1     0.8367    0.8367    0.8367       300\n",
      "\n",
      "    accuracy                         0.8367       600\n",
      "   macro avg     0.8367    0.8367    0.8367       600\n",
      "weighted avg     0.8367    0.8367    0.8367       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.7667    0.7986       300\n",
      "           1     0.7840    0.8467    0.8141       300\n",
      "\n",
      "    accuracy                         0.8067       600\n",
      "   macro avg     0.8086    0.8067    0.8064       600\n",
      "weighted avg     0.8086    0.8067    0.8064       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5665    0.9367    0.7060       300\n",
      "           1     0.8173    0.2833    0.4208       300\n",
      "\n",
      "    accuracy                         0.6100       600\n",
      "   macro avg     0.6919    0.6100    0.5634       600\n",
      "weighted avg     0.6919    0.6100    0.5634       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7386    0.8667    0.7975       300\n",
      "           1     0.8387    0.6933    0.7591       300\n",
      "\n",
      "    accuracy                         0.7800       600\n",
      "   macro avg     0.7887    0.7800    0.7783       600\n",
      "weighted avg     0.7887    0.7800    0.7783       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6378    0.7867    0.7045       300\n",
      "           1     0.7217    0.5533    0.6264       300\n",
      "\n",
      "    accuracy                         0.6700       600\n",
      "   macro avg     0.6798    0.6700    0.6654       600\n",
      "weighted avg     0.6798    0.6700    0.6654       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8273    0.9100    0.8667       300\n",
      "           1     0.9000    0.8100    0.8526       300\n",
      "\n",
      "    accuracy                         0.8600       600\n",
      "   macro avg     0.8636    0.8600    0.8596       600\n",
      "weighted avg     0.8636    0.8600    0.8596       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8076    0.8533    0.8298       300\n",
      "           1     0.8445    0.7967    0.8199       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8260    0.8250    0.8249       600\n",
      "weighted avg     0.8260    0.8250    0.8249       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5697    0.9267    0.7056       300\n",
      "           1     0.8036    0.3000    0.4369       300\n",
      "\n",
      "    accuracy                         0.6133       600\n",
      "   macro avg     0.6866    0.6133    0.5712       600\n",
      "weighted avg     0.6866    0.6133    0.5712       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7071    0.7967    0.7492       300\n",
      "           1     0.7672    0.6700    0.7153       300\n",
      "\n",
      "    accuracy                         0.7333       600\n",
      "   macro avg     0.7371    0.7333    0.7323       600\n",
      "weighted avg     0.7371    0.7333    0.7323       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5843    0.9700    0.7293       300\n",
      "           1     0.9118    0.3100    0.4627       300\n",
      "\n",
      "    accuracy                         0.6400       600\n",
      "   macro avg     0.7481    0.6400    0.5960       600\n",
      "weighted avg     0.7481    0.6400    0.5960       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7941    0.8100    0.8020       300\n",
      "           1     0.8061    0.7900    0.7980       300\n",
      "\n",
      "    accuracy                         0.8000       600\n",
      "   macro avg     0.8001    0.8000    0.8000       600\n",
      "weighted avg     0.8001    0.8000    0.8000       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7664    0.8200    0.7923       300\n",
      "           1     0.8065    0.7500    0.7772       300\n",
      "\n",
      "    accuracy                         0.7850       600\n",
      "   macro avg     0.7864    0.7850    0.7847       600\n",
      "weighted avg     0.7864    0.7850    0.7847       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5691    0.9467    0.7109       300\n",
      "           1     0.8416    0.2833    0.4239       300\n",
      "\n",
      "    accuracy                         0.6150       600\n",
      "   macro avg     0.7054    0.6150    0.5674       600\n",
      "weighted avg     0.7054    0.6150    0.5674       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7425    0.8267    0.7823       300\n",
      "           1     0.8045    0.7133    0.7562       300\n",
      "\n",
      "    accuracy                         0.7700       600\n",
      "   macro avg     0.7735    0.7700    0.7693       600\n",
      "weighted avg     0.7735    0.7700    0.7693       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6353    0.7433    0.6851       300\n",
      "           1     0.6908    0.5733    0.6266       300\n",
      "\n",
      "    accuracy                         0.6583       600\n",
      "   macro avg     0.6630    0.6583    0.6558       600\n",
      "weighted avg     0.6630    0.6583    0.6558       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8148    0.8800    0.8462       300\n",
      "           1     0.8696    0.8000    0.8333       300\n",
      "\n",
      "    accuracy                         0.8400       600\n",
      "   macro avg     0.8422    0.8400    0.8397       600\n",
      "weighted avg     0.8422    0.8400    0.8397       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8185    0.8267    0.8226       300\n",
      "           1     0.8249    0.8167    0.8208       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8217    0.8217    0.8217       600\n",
      "weighted avg     0.8217    0.8217    0.8217       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5657    0.9333    0.7044       300\n",
      "           1     0.8095    0.2833    0.4198       300\n",
      "\n",
      "    accuracy                         0.6083       600\n",
      "   macro avg     0.6876    0.6083    0.5621       600\n",
      "weighted avg     0.6876    0.6083    0.5621       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7416    0.8800    0.8049       300\n",
      "           1     0.8525    0.6933    0.7647       300\n",
      "\n",
      "    accuracy                         0.7867       600\n",
      "   macro avg     0.7970    0.7867    0.7848       600\n",
      "weighted avg     0.7970    0.7867    0.7848       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5801    0.9900    0.7315       300\n",
      "           1     0.9659    0.2833    0.4381       300\n",
      "\n",
      "    accuracy                         0.6367       600\n",
      "   macro avg     0.7730    0.6367    0.5848       600\n",
      "weighted avg     0.7730    0.6367    0.5848       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7874    0.8767    0.8297       300\n",
      "           1     0.8609    0.7633    0.8092       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8242    0.8200    0.8194       600\n",
      "weighted avg     0.8242    0.8200    0.8194       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8115    0.8467    0.8287       300\n",
      "           1     0.8397    0.8033    0.8211       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8256    0.8250    0.8249       600\n",
      "weighted avg     0.8256    0.8250    0.8249       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5693    0.8900    0.6944       300\n",
      "           1     0.7481    0.3267    0.4548       300\n",
      "\n",
      "    accuracy                         0.6083       600\n",
      "   macro avg     0.6587    0.6083    0.5746       600\n",
      "weighted avg     0.6587    0.6083    0.5746       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7147    0.8767    0.7874       300\n",
      "           1     0.8405    0.6500    0.7331       300\n",
      "\n",
      "    accuracy                         0.7633       600\n",
      "   macro avg     0.7776    0.7633    0.7603       600\n",
      "weighted avg     0.7776    0.7633    0.7603       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6361    0.8333    0.7215       300\n",
      "           1     0.7585    0.5233    0.6193       300\n",
      "\n",
      "    accuracy                         0.6783       600\n",
      "   macro avg     0.6973    0.6783    0.6704       600\n",
      "weighted avg     0.6973    0.6783    0.6704       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7946    0.8900    0.8396       300\n",
      "           1     0.8750    0.7700    0.8191       300\n",
      "\n",
      "    accuracy                         0.8300       600\n",
      "   macro avg     0.8348    0.8300    0.8294       600\n",
      "weighted avg     0.8348    0.8300    0.8294       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8019    0.8233    0.8125       300\n",
      "           1     0.8185    0.7967    0.8074       300\n",
      "\n",
      "    accuracy                         0.8100       600\n",
      "   macro avg     0.8102    0.8100    0.8100       600\n",
      "weighted avg     0.8102    0.8100    0.8100       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5972    0.7167    0.6515       300\n",
      "           1     0.6458    0.5167    0.5741       300\n",
      "\n",
      "    accuracy                         0.6167       600\n",
      "   macro avg     0.6215    0.6167    0.6128       600\n",
      "weighted avg     0.6215    0.6167    0.6128       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7447    0.8267    0.7836       300\n",
      "           1     0.8052    0.7167    0.7584       300\n",
      "\n",
      "    accuracy                         0.7717       600\n",
      "   macro avg     0.7750    0.7717    0.7710       600\n",
      "weighted avg     0.7750    0.7717    0.7710       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6550    0.7467    0.6978       300\n",
      "           1     0.7054    0.6067    0.6523       300\n",
      "\n",
      "    accuracy                         0.6767       600\n",
      "   macro avg     0.6802    0.6767    0.6751       600\n",
      "weighted avg     0.6802    0.6767    0.6751       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8533    0.8258       300\n",
      "           1     0.8429    0.7867    0.8138       300\n",
      "\n",
      "    accuracy                         0.8200       600\n",
      "   macro avg     0.8214    0.8200    0.8198       600\n",
      "weighted avg     0.8214    0.8200    0.8198       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8218    0.8300    0.8259       300\n",
      "           1     0.8283    0.8200    0.8241       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8250    0.8250    0.8250       600\n",
      "weighted avg     0.8250    0.8250    0.8250       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5783    0.8367    0.6839       300\n",
      "           1     0.7048    0.3900    0.5021       300\n",
      "\n",
      "    accuracy                         0.6133       600\n",
      "   macro avg     0.6416    0.6133    0.5930       600\n",
      "weighted avg     0.6416    0.6133    0.5930       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7318    0.8733    0.7964       300\n",
      "           1     0.8430    0.6800    0.7528       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7874    0.7767    0.7746       600\n",
      "weighted avg     0.7874    0.7767    0.7746       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6628    0.7667    0.7110       300\n",
      "           1     0.7233    0.6100    0.6618       300\n",
      "\n",
      "    accuracy                         0.6883       600\n",
      "   macro avg     0.6931    0.6883    0.6864       600\n",
      "weighted avg     0.6931    0.6883    0.6864       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8170    0.8633    0.8395       300\n",
      "           1     0.8551    0.8067    0.8302       300\n",
      "\n",
      "    accuracy                         0.8350       600\n",
      "   macro avg     0.8361    0.8350    0.8349       600\n",
      "weighted avg     0.8361    0.8350    0.8349       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8243    0.8600    0.8418       300\n",
      "           1     0.8537    0.8167    0.8348       300\n",
      "\n",
      "    accuracy                         0.8383       600\n",
      "   macro avg     0.8390    0.8383    0.8383       600\n",
      "weighted avg     0.8390    0.8383    0.8383       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5683    0.9567    0.7130       300\n",
      "           1     0.8632    0.2733    0.4152       300\n",
      "\n",
      "    accuracy                         0.6150       600\n",
      "   macro avg     0.7157    0.6150    0.5641       600\n",
      "weighted avg     0.7157    0.6150    0.5641       600\n",
      "\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7304    0.8400    0.7814       300\n",
      "           1     0.8118    0.6900    0.7459       300\n",
      "\n",
      "    accuracy                         0.7650       600\n",
      "   macro avg     0.7711    0.7650    0.7637       600\n",
      "weighted avg     0.7711    0.7650    0.7637       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6374    0.7733    0.6988       300\n",
      "           1     0.7119    0.5600    0.6269       300\n",
      "\n",
      "    accuracy                         0.6667       600\n",
      "   macro avg     0.6746    0.6667    0.6628       600\n",
      "weighted avg     0.6746    0.6667    0.6628       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8084    0.8300    0.8191       300\n",
      "           1     0.8253    0.8033    0.8142       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8169    0.8167    0.8166       600\n",
      "weighted avg     0.8169    0.8167    0.8166       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8271    0.8133    0.8202       300\n",
      "           1     0.8164    0.8300    0.8231       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8218    0.8217    0.8217       600\n",
      "weighted avg     0.8218    0.8217    0.8217       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5664    0.9100    0.6982       300\n",
      "           1     0.7712    0.3033    0.4354       300\n",
      "\n",
      "    accuracy                         0.6067       600\n",
      "   macro avg     0.6688    0.6067    0.5668       600\n",
      "weighted avg     0.6688    0.6067    0.5668       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Score_RBF', 'tnr_rbf', 'tpr_rbf',\n",
    "                                'C_LINEAR', 'Score_LINEAR', 'tnr_linear', 'tpr_linear',\n",
    "                                'C_OG_RBF', 'Score_OG_RBF', 'tnr_og_rbf', 'tpr_og_rbf',\n",
    "                                'C_OG_LINEAR', 'Score_OG_LINEAR', 'tnr_og_linear', 'tpr_og_linear',\n",
    "                                'Threshold', 'Score_old', 'tnr_old', 'tpr_old'])\n",
    "\n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "        \n",
    "    test_data = [X_train_list[i] for i in test_ids]\n",
    "    test_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    test_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    train_data = [item for data in train_data for item in data]\n",
    "    train_data_OG = [item for data in train_data_OG for item in data]\n",
    "    train_label = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(train_data, train_label, test_size=600, random_state=42)\n",
    "    X_train_OG, X_val_OG, _, _ = train_test_split(train_data_OG, train_label, test_size=600, random_state=42)\n",
    "    \n",
    "    test_data = [item for data in test_data for item in data]\n",
    "    test_data_OG = [item for data in test_data_OG for item in data]\n",
    "    test_label = [item for data in test_label for item in data]\n",
    "\n",
    "    MAE_data = [MAE_list[i] for i in test_ids]\n",
    "    MAE_data = [item for data in MAE_data for item in data]\n",
    "    \n",
    "    FINAL_QP_data = [FINAL_QP_list[i] for i in test_ids]\n",
    "    FINAL_QP_data = [item for data in FINAL_QP_data for item in data]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "                \n",
    "    for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "        results_old = np.array([is_double_compressed(MAE_data[i], FINAL_QP_data[i], threshold) for i in range(600)])\n",
    "        predicted_labels = results_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(test_data)\n",
    "    test_accuracy_RBF = accuracy_score(test_label, test_predictions_RBF)\n",
    "    report_RBF = classification_report(test_label, test_predictions_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_RBF)\n",
    "    tnr_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data)\n",
    "    test_accuracy_LINEAR = accuracy_score(test_label, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label, test_predictions_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_LINEAR)\n",
    "    tnr_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(test_data_OG)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(test_label, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(test_label, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_RBF)\n",
    "    tnr_og_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary2_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(test_data_OG)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(test_label, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    tnr_og_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary2_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Score_RBF': test_accuracy_RBF, 'tnr_rbf': tnr_rbf, 'tpr_rbf': tpr_rbf,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Score_LINEAR': test_accuracy_LINEAR, 'tnr_linear': tnr_linear, 'tpr_linear': tpr_linear,\n",
    "              'C_OG_RBF': best_c_value_onlyGhost_RBF, 'Score_OG_RBF': test_accuracy_onlyGhost_RBF, 'tnr_og_rbf': tnr_og_rbf, 'tpr_og_rbf': tpr_og_rbf,\n",
    "              'C_OG_LINEAR': best_c_value_onlyGhost_LINEAR, 'Score_OG_LINEAR': test_accuracy_onlyGhost_LINEAR, 'tnr_og_linear': tnr_og_linear, 'tpr_og_linear': tpr_og_linear,\n",
    "              'Threshold': best_threshold, 'Score_old': best_accuracy, 'tnr_old': tnr_old, 'tpr_old': tpr_old}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0      RBF        84.47        69.20               76.83                1.44           78.67           73.33\n",
      "1   LINEAR        81.33        52.10               66.72                1.93           69.67           63.67\n",
      "2     RBF2        86.27        79.17               82.72                1.68           86.00           80.00\n",
      "3  LINEAR2        82.73        81.13               81.93                1.54           83.83           78.50\n",
      "4      OLD        90.20        32.33               61.27                0.42           62.00           60.67\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF', 'LINEAR', 'RBF2', 'LINEAR2', 'OLD'],\n",
    "    'Average TNR': [round(results['tnr_rbf'].mean() * 100, 2), round(results['tnr_linear'].mean() * 100, 2), round(results['tnr_og_rbf'].mean() * 100, 2), round(results['tnr_og_linear'].mean() * 100, 2), round(results['tnr_old'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf'].mean() * 100, 2), round(results['tpr_linear'].mean() * 100, 2), round(results['tpr_og_rbf'].mean() * 100, 2), round(results['tpr_og_linear'].mean() * 100, 2), round(results['tpr_old'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF'].mean() * 100, 2), round(results['Score_LINEAR'].mean() * 100, 2), round(results['Score_OG_RBF'].mean() * 100, 2), round(results['Score_OG_LINEAR'].mean() * 100, 2), round(results['Score_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF'].std() * 100, 2), round(results['Score_LINEAR'].std() * 100, 2), round(results['Score_OG_RBF'].std() * 100, 2), round(results['Score_OG_LINEAR'].std() * 100, 2), round(results['Score_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF'].max() * 100, 2), round(results['Score_LINEAR'].max() * 100, 2), round(results['Score_OG_RBF'].max() * 100, 2), round(results['Score_OG_LINEAR'].max() * 100, 2), round(results['Score_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF'].min() * 100, 2), round(results['Score_LINEAR'].min() * 100, 2), round(results['Score_OG_RBF'].min() * 100, 2), round(results['Score_OG_LINEAR'].min() * 100, 2), round(results['Score_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3000\n",
      "1    4000\n",
      "2    5000\n",
      "3    1000\n",
      "4    3000\n",
      "5    3000\n",
      "6    5000\n",
      "7    5000\n",
      "8    4000\n",
      "9    5000\n",
      "Name: C_RBF, dtype: object\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3    0.1\n",
      "4      1\n",
      "5    0.1\n",
      "6      1\n",
      "7      1\n",
      "8      1\n",
      "9      1\n",
      "Name: C_LINEAR, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF'])\n",
    "print(results['C_LINEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
