{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # DataFrameを改行せずに表示\n",
    "pd.set_option('display.max_columns', None)  # すべての列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_finalQP(filename):\n",
    "    match = re.search(r'2ndQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_1stQP(filename):\n",
    "    match = re.search(r'1stQP(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def ratio_double_compressed(mean_difference, final_QP):\n",
    "    # mean_difference = mean_difference[0]\n",
    "    # final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "\n",
    "        \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy > 0:\n",
    "        return right_energy / energy\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def is_double_compressed(mean_difference, final_QP, threshold):\n",
    "    mean_difference = mean_difference[0]\n",
    "    final_QP = final_QP[0]\n",
    "    clamped_mean_difference = np.maximum(mean_difference, -0.01)\n",
    "    \n",
    "    #全体のエネルギーを計算\n",
    "    energy = np.sum(np.square(clamped_mean_difference))\n",
    "    # energy = np.sum(np.square(mean_difference))\n",
    "    \n",
    "    #QP2より右側のエネルギーを計算\n",
    "    right_energy = np.sum(np.square(clamped_mean_difference[final_QP+1:52]))\n",
    "    # right_energy = np.sum(np.square(mean_difference[final_QP+1:52]))\n",
    "    \n",
    "    # print('energy: ', energy)\n",
    "    # print('R-energy: ', right_energy)\n",
    "    # print('Ratio: ', right_energy / energy)\n",
    "    \n",
    "    \n",
    "    # エネルギー比を計算して閾値と比較\n",
    "    if energy <= 0:\n",
    "        return -1\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) > threshold:\n",
    "        return True\n",
    "    \n",
    "    elif (right_energy / energy) != 0 and (right_energy / energy) <= threshold:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def calculate_mae(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            loaded_data, loaded_data_shifted = pickle.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # タプル内のリストを抽出\n",
    "    original_mae = np.array(loaded_data)\n",
    "    shifted_mae = np.array(loaded_data_shifted)\n",
    "\n",
    "    # Coding ghostを計算してリストに格納する\n",
    "    mae_difference = shifted_mae - original_mae\n",
    "    \n",
    "    # mae_differenceの各要素においてマイナスの値を0に変換\n",
    "    # mae_difference_positive = np.maximum(mae_difference, 0)\n",
    "    \n",
    "    return mae_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['98', '16', '170', '43', '287', '192', '110', '265', '35', '37', '17', '232', '149', '210', '281', '115', '145', '124', '187', '54', '28', '202', '80', '122', '206', '109', '68', '143', '178', '180'], ['173', '61', '69', '166', '36', '20', '84', '148', '234', '85', '267', '188', '177', '125', '66', '56', '262', '251', '119', '273', '78', '299', '191', '298', '116', '179', '126', '150', '13', '252'], ['266', '91', '4', '75', '147', '64', '132', '33', '6', '94', '24', '293', '268', '86', '96', '221', '219', '159', '242', '29', '199', '26', '278', '79', '2', '167', '277', '195', '288', '227'], ['215', '163', '50', '112', '30', '74', '297', '5', '247', '1', '254', '263', '162', '222', '185', '280', '233', '123', '182', '44', '102', '214', '155', '225', '83', '89', '19', '283', '172', '52'], ['223', '67', '144', '168', '157', '23', '271', '193', '141', '244', '11', '114', '165', '174', '139', '39', '209', '198', '213', '99', '46', '189', '156', '194', '258', '151', '218', '171', '181', '108'], ['135', '133', '203', '42', '7', '264', '300', '239', '229', '100', '22', '82', '230', '249', '131', '200', '286', '32', '140', '256', '136', '25', '224', '60', '63', '27', '270', '146', '272', '216'], ['235', '289', '134', '111', '220', '9', '117', '31', '292', '12', '106', '18', '137', '45', '120', '236', '130', '246', '259', '113', '154', '231', '201', '127', '282', '226', '176', '38', '142', '274'], ['152', '217', '212', '276', '101', '58', '158', '47', '153', '240', '208', '204', '243', '197', '285', '15', '41', '296', '40', '10', '59', '55', '73', '65', '245', '284', '290', '260', '161', '70'], ['248', '237', '105', '49', '104', '76', '121', '241', '72', '207', '184', '291', '21', '103', '3', '138', '160', '255', '93', '34', '62', '186', '196', '53', '81', '71', '228', '261', '175', '279'], ['211', '97', '77', '294', '14', '95', '164', '183', '257', '118', '238', '295', '88', '51', '90', '92', '107', '253', '275', '205', '169', '87', '128', '129', '190', '250', '269', '57', '8', '48']]\n"
     ]
    }
   ],
   "source": [
    "rootpath_csv = \"/Prove/Yoshihisa/HEIF_ghost/HEIF_IMAGES_CSV/\"\n",
    "\n",
    "train_list1 = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"]\n",
    "\n",
    "train_list2 = [\"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\"]\n",
    "\n",
    "train_list3 = [\"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\"]\n",
    "\n",
    "train_list4 = [\"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\", \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"120\"]\n",
    "\n",
    "train_list5 = [\"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\", \"130\", \"131\", \"132\", \"133\", \"134\", \"135\", \"136\", \"137\", \"138\", \"139\", \"140\", \"141\", \"142\", \"143\", \"144\", \"145\", \"146\", \"147\", \"148\", \"149\", \"150\"]\n",
    "\n",
    "train_list6 = [\"151\", \"152\", \"153\", \"154\", \"155\", \"156\", \"157\", \"158\", \"159\", \"160\", \"161\", \"162\", \"163\", \"164\", \"165\", \"166\", \"167\", \"168\", \"169\", \"170\", \"171\", \"172\", \"173\", \"174\", \"175\", \"176\", \"177\", \"178\", \"179\", \"180\"]\n",
    "\n",
    "train_list7 = [\"181\", \"182\", \"183\", \"184\", \"185\", \"186\", \"187\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\", \"195\", \"196\", \"197\", \"198\", \"199\", \"200\", \"201\", \"202\", \"203\", \"204\", \"205\", \"206\", \"207\", \"208\", \"209\", \"210\"]\n",
    "\n",
    "train_list8 = [\"211\", \"212\", \"213\", \"214\", \"215\", \"216\", \"217\", \"218\", \"219\", \"220\", \"221\", \"222\", \"223\", \"224\", \"225\", \"226\", \"227\", \"228\", \"229\", \"230\", \"231\", \"232\", \"233\", \"234\", \"235\", \"236\", \"237\", \"238\", \"239\", \"240\"]\n",
    "\n",
    "train_list9 = [\"241\", \"242\", \"243\", \"244\", \"245\", \"246\", \"247\", \"248\", \"249\", \"250\", \"251\", \"252\", \"253\", \"254\", \"255\", \"256\", \"257\", \"258\", \"259\", \"260\", \"261\", \"262\", \"263\", \"264\", \"265\", \"266\", \"267\", \"268\", \"269\", \"270\"]\n",
    "\n",
    "train_list10 = [\"271\", \"272\", \"273\", \"274\", \"275\", \"276\", \"277\", \"278\", \"279\", \"280\", \"281\", \"282\", \"283\", \"284\", \"285\", \"286\", \"287\", \"288\", \"289\", \"290\", \"291\", \"292\", \"293\", \"294\", \"295\", \"296\", \"297\", \"298\", \"299\", \"300\"]\n",
    "\n",
    "\n",
    "all_train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5,\n",
    "                   train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "# すべてのリストを1つのリストに結合する\n",
    "combined_train_list = sum(all_train_lists, [])\n",
    "\n",
    "# リストの順序をランダムにシャッフルする\n",
    "random.shuffle(combined_train_list)\n",
    "\n",
    "# シャッフルされたリストを10個のグループに分割する\n",
    "train_lists = [combined_train_list[i:i+30] for i in range(0, len(combined_train_list), 30)]\n",
    "print(train_lists)\n",
    "\n",
    "single_list1 = []\n",
    "single_list2 = []\n",
    "single_list3 = []\n",
    "single_list4 = []\n",
    "single_list5 = []\n",
    "single_list6 = []\n",
    "single_list7 = []\n",
    "single_list8 = []\n",
    "single_list9 = []\n",
    "single_list10 = []\n",
    "\n",
    "single_recompress_list1 = []\n",
    "single_recompress_list2 = []\n",
    "single_recompress_list3 = []\n",
    "single_recompress_list4 = []\n",
    "single_recompress_list5 = []\n",
    "single_recompress_list6 = []\n",
    "single_recompress_list7 = []\n",
    "single_recompress_list8 = []\n",
    "single_recompress_list9 = []\n",
    "single_recompress_list10 = []\n",
    "\n",
    "second_largeQP1_list1 = []\n",
    "second_largeQP1_list2 = []\n",
    "second_largeQP1_list3 = []\n",
    "second_largeQP1_list4 = []\n",
    "second_largeQP1_list5 = []\n",
    "second_largeQP1_list6 = []\n",
    "second_largeQP1_list7 = []\n",
    "second_largeQP1_list8 = []\n",
    "second_largeQP1_list9 = []\n",
    "second_largeQP1_list10 = []\n",
    "\n",
    "second_recompress_largeQP1_list1 = []\n",
    "second_recompress_largeQP1_list2 = []\n",
    "second_recompress_largeQP1_list3 = []\n",
    "second_recompress_largeQP1_list4 = []\n",
    "second_recompress_largeQP1_list5 = []\n",
    "second_recompress_largeQP1_list6 = []\n",
    "second_recompress_largeQP1_list7 = []\n",
    "second_recompress_largeQP1_list8 = []\n",
    "second_recompress_largeQP1_list9 = []\n",
    "second_recompress_largeQP1_list10 = []\n",
    "\n",
    "second_sameQP_list1 = []\n",
    "second_sameQP_list2 = []\n",
    "second_sameQP_list3 = []\n",
    "second_sameQP_list4 = []\n",
    "second_sameQP_list5 = []\n",
    "second_sameQP_list6 = []\n",
    "second_sameQP_list7 = []\n",
    "second_sameQP_list8 = []\n",
    "second_sameQP_list9 = []\n",
    "second_sameQP_list10 = []\n",
    "\n",
    "second_recompress_sameQP_list1 = []\n",
    "second_recompress_sameQP_list2 = []\n",
    "second_recompress_sameQP_list3 = []\n",
    "second_recompress_sameQP_list4 = []\n",
    "second_recompress_sameQP_list5 = []\n",
    "second_recompress_sameQP_list6 = []\n",
    "second_recompress_sameQP_list7 = []\n",
    "second_recompress_sameQP_list8 = []\n",
    "second_recompress_sameQP_list9 = []\n",
    "second_recompress_sameQP_list10 = []\n",
    "\n",
    "second_largeQP2_list1 = []\n",
    "second_largeQP2_list2 = []\n",
    "second_largeQP2_list3 = []\n",
    "second_largeQP2_list4 = []\n",
    "second_largeQP2_list5 = []\n",
    "second_largeQP2_list6 = []\n",
    "second_largeQP2_list7 = []\n",
    "second_largeQP2_list8 = []\n",
    "second_largeQP2_list9 = []\n",
    "second_largeQP2_list10 = []\n",
    "\n",
    "second_recompress_largeQP2_list1 = []\n",
    "second_recompress_largeQP2_list2 = []\n",
    "second_recompress_largeQP2_list3 = []\n",
    "second_recompress_largeQP2_list4 = []\n",
    "second_recompress_largeQP2_list5 = []\n",
    "second_recompress_largeQP2_list6 = []\n",
    "second_recompress_largeQP2_list7 = []\n",
    "second_recompress_largeQP2_list8 = []\n",
    "second_recompress_largeQP2_list9 = []\n",
    "second_recompress_largeQP2_list10 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_lists(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath_csv+f'HEIF_images_single_csv/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath_csv+f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath_csv+f'HEIF_images_second_csv/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath_csv+f'HEIF_images_triple_csv/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath_csv+f'HEIF_images_second_sameQP_csv/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath_csv+f'HEIF_images_triple_sameQP_csv/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath_csv+f'HEIF_images_second_largeQP_csv/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath_csv+f'HEIF_images_triple_largeQP_csv/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "            \n",
    "\n",
    "# train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5, train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "single_lists = [single_list1, single_list2,\n",
    "                single_list3, single_list4,\n",
    "                single_list5, single_list6,\n",
    "                single_list7, single_list8,\n",
    "                single_list9, single_list10]\n",
    "\n",
    "single_recompress_lists = [single_recompress_list1, single_recompress_list2,\n",
    "                           single_recompress_list3, single_recompress_list4,\n",
    "                           single_recompress_list5, single_recompress_list6,\n",
    "                           single_recompress_list7, single_recompress_list8,\n",
    "                           single_recompress_list9, single_recompress_list10]\n",
    "\n",
    "\n",
    "second_largeQP1_lists = [second_largeQP1_list1, second_largeQP1_list2,\n",
    "                        second_largeQP1_list3, second_largeQP1_list4,\n",
    "                        second_largeQP1_list5, second_largeQP1_list6,\n",
    "                        second_largeQP1_list7, second_largeQP1_list8,\n",
    "                        second_largeQP1_list9, second_largeQP1_list10]\n",
    "\n",
    "second_recompress_largeQP1_lists = [second_recompress_largeQP1_list1, second_recompress_largeQP1_list2,\n",
    "                           second_recompress_largeQP1_list3, second_recompress_largeQP1_list4,\n",
    "                           second_recompress_largeQP1_list5, second_recompress_largeQP1_list6,\n",
    "                           second_recompress_largeQP1_list7, second_recompress_largeQP1_list8,\n",
    "                           second_recompress_largeQP1_list9, second_recompress_largeQP1_list10]\n",
    "\n",
    "\n",
    "second_sameQP_lists = [second_sameQP_list1, second_sameQP_list2,\n",
    "                        second_sameQP_list3, second_sameQP_list4,\n",
    "                        second_sameQP_list5, second_sameQP_list6,\n",
    "                        second_sameQP_list7, second_sameQP_list8,\n",
    "                        second_sameQP_list9, second_sameQP_list10]\n",
    "\n",
    "second_recompress_sameQP_lists = [second_recompress_sameQP_list1, second_recompress_sameQP_list2,\n",
    "                           second_recompress_sameQP_list3, second_recompress_sameQP_list4,\n",
    "                           second_recompress_sameQP_list5, second_recompress_sameQP_list6,\n",
    "                           second_recompress_sameQP_list7, second_recompress_sameQP_list8,\n",
    "                           second_recompress_sameQP_list9, second_recompress_sameQP_list10]\n",
    "\n",
    "\n",
    "second_largeQP2_lists = [second_largeQP2_list1, second_largeQP2_list2,\n",
    "                        second_largeQP2_list3, second_largeQP2_list4,\n",
    "                        second_largeQP2_list5, second_largeQP2_list6,\n",
    "                        second_largeQP2_list7, second_largeQP2_list8,\n",
    "                        second_largeQP2_list9, second_largeQP2_list10]\n",
    "\n",
    "second_recompress_largeQP2_lists = [second_recompress_largeQP2_list1, second_recompress_largeQP2_list2,\n",
    "                           second_recompress_largeQP2_list3, second_recompress_largeQP2_list4,\n",
    "                           second_recompress_largeQP2_list5, second_recompress_largeQP2_list6,\n",
    "                           second_recompress_largeQP2_list7, second_recompress_largeQP2_list8,\n",
    "                           second_recompress_largeQP2_list9, second_recompress_largeQP2_list10]\n",
    "\n",
    "\n",
    "\n",
    "for train_list, single_list, single_recompress_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                           single_lists,\n",
    "                                                                                                                                                                                                                           single_recompress_lists,\n",
    "                                                                                                                                                                                                                           second_largeQP1_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP1_lists,\n",
    "                                                                                                                                                                                                                           second_sameQP_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_sameQP_lists,\n",
    "                                                                                                                                                                                                                           second_largeQP2_lists,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP2_lists\n",
    "                                                                                                                                                                                                                          ):\n",
    "    process_train_lists(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "1710\n",
      "1710\n",
      "300\n",
      "300\n",
      "1170\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(single_lists[6]))\n",
    "print(len(single_recompress_lists[6]))\n",
    "print(len(second_largeQP1_lists[6]))\n",
    "print(len(second_recompress_largeQP1_lists[6]))\n",
    "print(len(second_sameQP_lists[6]))\n",
    "print(len(second_recompress_sameQP_lists[6]))\n",
    "print(len(second_largeQP2_lists[6]))\n",
    "print(len(second_recompress_largeQP2_lists[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list1 = []\n",
    "single_list2 = []\n",
    "single_list3 = []\n",
    "single_list4 = []\n",
    "single_list5 = []\n",
    "single_list6 = []\n",
    "single_list7 = []\n",
    "single_list8 = []\n",
    "single_list9 = []\n",
    "single_list10 = []\n",
    "\n",
    "single_recompress_list1 = []\n",
    "single_recompress_list2 = []\n",
    "single_recompress_list3 = []\n",
    "single_recompress_list4 = []\n",
    "single_recompress_list5 = []\n",
    "single_recompress_list6 = []\n",
    "single_recompress_list7 = []\n",
    "single_recompress_list8 = []\n",
    "single_recompress_list9 = []\n",
    "single_recompress_list10 = []\n",
    "\n",
    "second_largeQP1_list1 = []\n",
    "second_largeQP1_list2 = []\n",
    "second_largeQP1_list3 = []\n",
    "second_largeQP1_list4 = []\n",
    "second_largeQP1_list5 = []\n",
    "second_largeQP1_list6 = []\n",
    "second_largeQP1_list7 = []\n",
    "second_largeQP1_list8 = []\n",
    "second_largeQP1_list9 = []\n",
    "second_largeQP1_list10 = []\n",
    "\n",
    "second_recompress_largeQP1_list1 = []\n",
    "second_recompress_largeQP1_list2 = []\n",
    "second_recompress_largeQP1_list3 = []\n",
    "second_recompress_largeQP1_list4 = []\n",
    "second_recompress_largeQP1_list5 = []\n",
    "second_recompress_largeQP1_list6 = []\n",
    "second_recompress_largeQP1_list7 = []\n",
    "second_recompress_largeQP1_list8 = []\n",
    "second_recompress_largeQP1_list9 = []\n",
    "second_recompress_largeQP1_list10 = []\n",
    "\n",
    "second_sameQP_list1 = []\n",
    "second_sameQP_list2 = []\n",
    "second_sameQP_list3 = []\n",
    "second_sameQP_list4 = []\n",
    "second_sameQP_list5 = []\n",
    "second_sameQP_list6 = []\n",
    "second_sameQP_list7 = []\n",
    "second_sameQP_list8 = []\n",
    "second_sameQP_list9 = []\n",
    "second_sameQP_list10 = []\n",
    "\n",
    "second_recompress_sameQP_list1 = []\n",
    "second_recompress_sameQP_list2 = []\n",
    "second_recompress_sameQP_list3 = []\n",
    "second_recompress_sameQP_list4 = []\n",
    "second_recompress_sameQP_list5 = []\n",
    "second_recompress_sameQP_list6 = []\n",
    "second_recompress_sameQP_list7 = []\n",
    "second_recompress_sameQP_list8 = []\n",
    "second_recompress_sameQP_list9 = []\n",
    "second_recompress_sameQP_list10 = []\n",
    "\n",
    "second_largeQP2_list1 = []\n",
    "second_largeQP2_list2 = []\n",
    "second_largeQP2_list3 = []\n",
    "second_largeQP2_list4 = []\n",
    "second_largeQP2_list5 = []\n",
    "second_largeQP2_list6 = []\n",
    "second_largeQP2_list7 = []\n",
    "second_largeQP2_list8 = []\n",
    "second_largeQP2_list9 = []\n",
    "second_largeQP2_list10 = []\n",
    "\n",
    "second_recompress_largeQP2_list1 = []\n",
    "second_recompress_largeQP2_list2 = []\n",
    "second_recompress_largeQP2_list3 = []\n",
    "second_recompress_largeQP2_list4 = []\n",
    "second_recompress_largeQP2_list5 = []\n",
    "second_recompress_largeQP2_list6 = []\n",
    "second_recompress_largeQP2_list7 = []\n",
    "second_recompress_largeQP2_list8 = []\n",
    "second_recompress_largeQP2_list9 = []\n",
    "second_recompress_largeQP2_list10 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath_pkl = \"/Prove/Yoshihisa/HEIF_ghost/PKL/\"\n",
    "\n",
    "\n",
    "def process_train_lists2(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list):\n",
    "    \n",
    "    for image in train_list:\n",
    "        single_path = osp.join(rootpath_pkl+f'pkl_single/{image}_*')\n",
    "        single_recompress_path = osp.join(rootpath_pkl+f'pkl_second_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP1_path = osp.join(rootpath_pkl+f'pkl_second/{image}_*')\n",
    "        second_recompress_largeQP1_path = osp.join(rootpath_pkl+f'pkl_triple/{image}_*')\n",
    "        \n",
    "        second_sameQP_path = osp.join(rootpath_pkl+f'pkl_second_sameQP/{image}_*')\n",
    "        second_recompress_sameQP_path = osp.join(rootpath_pkl+f'pkl_triple_sameQP/{image}_*')\n",
    "        \n",
    "        second_largeQP2_path = osp.join(rootpath_pkl+f'pkl_second_largeQP/{image}_*')\n",
    "        second_recompress_largeQP2_path = osp.join(rootpath_pkl+f'pkl_triple_largeQP/{image}_*')\n",
    "        \n",
    "\n",
    "        for path in sorted(glob.glob(single_path)):\n",
    "            single_list.append(path)\n",
    "        for path in sorted(glob.glob(single_recompress_path)):\n",
    "            single_recompress_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP1_path)):\n",
    "            second_largeQP1_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP1_path)):\n",
    "            second_recompress_largeQP1_list.append(path)\n",
    "                \n",
    "        for path in sorted(glob.glob(second_sameQP_path)):\n",
    "            second_sameQP_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_sameQP_path)):\n",
    "            second_recompress_sameQP_list.append(path)\n",
    "            \n",
    "        for path in sorted(glob.glob(second_largeQP2_path)):\n",
    "            second_largeQP2_list.append(path)\n",
    "        for path in sorted(glob.glob(second_recompress_largeQP2_path)):\n",
    "            second_recompress_largeQP2_list.append(path)\n",
    "            \n",
    "\n",
    "# train_lists = [train_list1, train_list2, train_list3, train_list4, train_list5, train_list6, train_list7, train_list8, train_list9, train_list10]\n",
    "\n",
    "single_listsA = [single_list1, single_list2,\n",
    "                single_list3, single_list4,\n",
    "                single_list5, single_list6,\n",
    "                single_list7, single_list8,\n",
    "                single_list9, single_list10]\n",
    "\n",
    "single_recompress_listsA = [single_recompress_list1, single_recompress_list2,\n",
    "                           single_recompress_list3, single_recompress_list4,\n",
    "                           single_recompress_list5, single_recompress_list6,\n",
    "                           single_recompress_list7, single_recompress_list8,\n",
    "                           single_recompress_list9, single_recompress_list10]\n",
    "\n",
    "\n",
    "second_largeQP1_listsA = [second_largeQP1_list1, second_largeQP1_list2,\n",
    "                        second_largeQP1_list3, second_largeQP1_list4,\n",
    "                        second_largeQP1_list5, second_largeQP1_list6,\n",
    "                        second_largeQP1_list7, second_largeQP1_list8,\n",
    "                        second_largeQP1_list9, second_largeQP1_list10]\n",
    "\n",
    "second_recompress_largeQP1_listsA = [second_recompress_largeQP1_list1, second_recompress_largeQP1_list2,\n",
    "                           second_recompress_largeQP1_list3, second_recompress_largeQP1_list4,\n",
    "                           second_recompress_largeQP1_list5, second_recompress_largeQP1_list6,\n",
    "                           second_recompress_largeQP1_list7, second_recompress_largeQP1_list8,\n",
    "                           second_recompress_largeQP1_list9, second_recompress_largeQP1_list10]\n",
    "\n",
    "\n",
    "second_sameQP_listsA = [second_sameQP_list1, second_sameQP_list2,\n",
    "                        second_sameQP_list3, second_sameQP_list4,\n",
    "                        second_sameQP_list5, second_sameQP_list6,\n",
    "                        second_sameQP_list7, second_sameQP_list8,\n",
    "                        second_sameQP_list9, second_sameQP_list10]\n",
    "\n",
    "second_recompress_sameQP_listsA = [second_recompress_sameQP_list1, second_recompress_sameQP_list2,\n",
    "                           second_recompress_sameQP_list3, second_recompress_sameQP_list4,\n",
    "                           second_recompress_sameQP_list5, second_recompress_sameQP_list6,\n",
    "                           second_recompress_sameQP_list7, second_recompress_sameQP_list8,\n",
    "                           second_recompress_sameQP_list9, second_recompress_sameQP_list10]\n",
    "\n",
    "\n",
    "second_largeQP2_listsA = [second_largeQP2_list1, second_largeQP2_list2,\n",
    "                        second_largeQP2_list3, second_largeQP2_list4,\n",
    "                        second_largeQP2_list5, second_largeQP2_list6,\n",
    "                        second_largeQP2_list7, second_largeQP2_list8,\n",
    "                        second_largeQP2_list9, second_largeQP2_list10]\n",
    "\n",
    "second_recompress_largeQP2_listsA = [second_recompress_largeQP2_list1, second_recompress_largeQP2_list2,\n",
    "                           second_recompress_largeQP2_list3, second_recompress_largeQP2_list4,\n",
    "                           second_recompress_largeQP2_list5, second_recompress_largeQP2_list6,\n",
    "                           second_recompress_largeQP2_list7, second_recompress_largeQP2_list8,\n",
    "                           second_recompress_largeQP2_list9, second_recompress_largeQP2_list10]\n",
    "\n",
    "\n",
    "\n",
    "for train_list, single_list, single_recompress_list, second_largeQP1_list, second_recompress_largeQP1_list, second_sameQP_list, second_recompress_sameQP_list, second_largeQP2_list, second_recompress_largeQP2_list in zip(train_lists, \n",
    "                                                                                                                                                                                                                           single_listsA,\n",
    "                                                                                                                                                                                                                           single_recompress_listsA,\n",
    "                                                                                                                                                                                                                           second_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP1_listsA,\n",
    "                                                                                                                                                                                                                           second_sameQP_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_sameQP_listsA,\n",
    "                                                                                                                                                                                                                           second_largeQP2_listsA,\n",
    "                                                                                                                                                                                                                           second_recompress_largeQP2_listsA\n",
    "                                                                                                                                                                                                                          ):\n",
    "    process_train_lists2(train_list, single_list, single_recompress_list, \n",
    "                        second_largeQP1_list, second_recompress_largeQP1_list, \n",
    "                        second_sameQP_list, second_recompress_sameQP_list,\n",
    "                        second_largeQP2_list, second_recompress_largeQP2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "1710\n",
      "1710\n",
      "300\n",
      "300\n",
      "1170\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(single_listsA[6]))\n",
    "print(len(single_recompress_listsA[6]))\n",
    "print(len(second_largeQP1_listsA[6]))\n",
    "print(len(second_recompress_largeQP1_listsA[6]))\n",
    "print(len(second_sameQP_listsA[6]))\n",
    "print(len(second_recompress_sameQP_listsA[6]))\n",
    "print(len(second_largeQP2_listsA[6]))\n",
    "print(len(second_recompress_largeQP2_listsA[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "100\n",
      "100\n",
      "100\n",
      "train_csv_list:  600\n"
     ]
    }
   ],
   "source": [
    "single_csv1 = list(zip(single_lists[0], single_listsA[0], single_recompress_lists[0], single_recompress_listsA[0]))\n",
    "single_csv2 = list(zip(single_lists[1], single_listsA[1], single_recompress_lists[1], single_recompress_listsA[1]))\n",
    "single_csv3 = list(zip(single_lists[2], single_listsA[2], single_recompress_lists[2], single_recompress_listsA[2]))\n",
    "single_csv4 = list(zip(single_lists[3], single_listsA[3], single_recompress_lists[3], single_recompress_listsA[3]))\n",
    "single_csv5 = list(zip(single_lists[4], single_listsA[4], single_recompress_lists[4], single_recompress_listsA[4]))\n",
    "single_csv6 = list(zip(single_lists[5], single_listsA[5], single_recompress_lists[5], single_recompress_listsA[5]))\n",
    "single_csv7 = list(zip(single_lists[6], single_listsA[6], single_recompress_lists[6], single_recompress_listsA[6]))\n",
    "single_csv8 = list(zip(single_lists[7], single_listsA[7], single_recompress_lists[7], single_recompress_listsA[7]))\n",
    "single_csv9 = list(zip(single_lists[8], single_listsA[8], single_recompress_lists[8], single_recompress_listsA[8]))\n",
    "single_csv10 = list(zip(single_lists[9], single_listsA[9], single_recompress_lists[9], single_recompress_listsA[9]))\n",
    "print(len(single_csv7))\n",
    "\n",
    "second_largeQP1_csv1 = list(zip(second_largeQP1_lists[0], second_largeQP1_listsA[0], second_recompress_largeQP1_lists[0], second_recompress_largeQP1_listsA[0]))\n",
    "second_largeQP1_csv2 = list(zip(second_largeQP1_lists[1], second_largeQP1_listsA[1], second_recompress_largeQP1_lists[1], second_recompress_largeQP1_listsA[1]))\n",
    "second_largeQP1_csv3 = list(zip(second_largeQP1_lists[2], second_largeQP1_listsA[2], second_recompress_largeQP1_lists[2], second_recompress_largeQP1_listsA[2]))\n",
    "second_largeQP1_csv4 = list(zip(second_largeQP1_lists[3], second_largeQP1_listsA[3], second_recompress_largeQP1_lists[3], second_recompress_largeQP1_listsA[3]))\n",
    "second_largeQP1_csv5 = list(zip(second_largeQP1_lists[4], second_largeQP1_listsA[4], second_recompress_largeQP1_lists[4], second_recompress_largeQP1_listsA[4]))\n",
    "second_largeQP1_csv6 = list(zip(second_largeQP1_lists[5], second_largeQP1_listsA[5], second_recompress_largeQP1_lists[5], second_recompress_largeQP1_listsA[5]))\n",
    "second_largeQP1_csv7 = list(zip(second_largeQP1_lists[6], second_largeQP1_listsA[6], second_recompress_largeQP1_lists[6], second_recompress_largeQP1_listsA[6]))\n",
    "second_largeQP1_csv8 = list(zip(second_largeQP1_lists[7], second_largeQP1_listsA[7], second_recompress_largeQP1_lists[7], second_recompress_largeQP1_listsA[7]))\n",
    "second_largeQP1_csv9 = list(zip(second_largeQP1_lists[8], second_largeQP1_listsA[8], second_recompress_largeQP1_lists[8], second_recompress_largeQP1_listsA[8]))\n",
    "second_largeQP1_csv10 = list(zip(second_largeQP1_lists[9], second_largeQP1_listsA[9], second_recompress_largeQP1_lists[9], second_recompress_largeQP1_listsA[9]))\n",
    "\n",
    "second_largeQP1_csv1 = random.sample(second_largeQP1_csv1, 100)\n",
    "second_largeQP1_csv2 = random.sample(second_largeQP1_csv2, 100)\n",
    "second_largeQP1_csv3 = random.sample(second_largeQP1_csv3, 100)\n",
    "second_largeQP1_csv4 = random.sample(second_largeQP1_csv4, 100)\n",
    "second_largeQP1_csv5 = random.sample(second_largeQP1_csv5, 100)\n",
    "second_largeQP1_csv6 = random.sample(second_largeQP1_csv6, 100)\n",
    "second_largeQP1_csv7 = random.sample(second_largeQP1_csv7, 100)\n",
    "second_largeQP1_csv8 = random.sample(second_largeQP1_csv8, 100)\n",
    "second_largeQP1_csv9 = random.sample(second_largeQP1_csv9, 100)\n",
    "second_largeQP1_csv10 = random.sample(second_largeQP1_csv10, 100)\n",
    "print(len(second_largeQP1_csv7))\n",
    "\n",
    "second_sameQP_csv1 = list(zip(second_sameQP_lists[0], second_sameQP_listsA[0], second_recompress_sameQP_lists[0], second_recompress_sameQP_listsA[0]))\n",
    "second_sameQP_csv2 = list(zip(second_sameQP_lists[1], second_sameQP_listsA[1], second_recompress_sameQP_lists[1], second_recompress_sameQP_listsA[1]))\n",
    "second_sameQP_csv3 = list(zip(second_sameQP_lists[2], second_sameQP_listsA[2], second_recompress_sameQP_lists[2], second_recompress_sameQP_listsA[2]))\n",
    "second_sameQP_csv4 = list(zip(second_sameQP_lists[3], second_sameQP_listsA[3], second_recompress_sameQP_lists[3], second_recompress_sameQP_listsA[3]))\n",
    "second_sameQP_csv5 = list(zip(second_sameQP_lists[4], second_sameQP_listsA[4], second_recompress_sameQP_lists[4], second_recompress_sameQP_listsA[4]))\n",
    "second_sameQP_csv6 = list(zip(second_sameQP_lists[5], second_sameQP_listsA[5], second_recompress_sameQP_lists[5], second_recompress_sameQP_listsA[5]))\n",
    "second_sameQP_csv7 = list(zip(second_sameQP_lists[6], second_sameQP_listsA[6], second_recompress_sameQP_lists[6], second_recompress_sameQP_listsA[6]))\n",
    "second_sameQP_csv8 = list(zip(second_sameQP_lists[7], second_sameQP_listsA[7], second_recompress_sameQP_lists[7], second_recompress_sameQP_listsA[7]))\n",
    "second_sameQP_csv9 = list(zip(second_sameQP_lists[8], second_sameQP_listsA[8], second_recompress_sameQP_lists[8], second_recompress_sameQP_listsA[8]))\n",
    "second_sameQP_csv10 = list(zip(second_sameQP_lists[9], second_sameQP_listsA[9], second_recompress_sameQP_lists[9], second_recompress_sameQP_listsA[9]))\n",
    "\n",
    "second_sameQP_csv1 = random.sample(second_sameQP_csv1, 100)\n",
    "second_sameQP_csv2 = random.sample(second_sameQP_csv2, 100)\n",
    "second_sameQP_csv3 = random.sample(second_sameQP_csv3, 100)\n",
    "second_sameQP_csv4 = random.sample(second_sameQP_csv4, 100)\n",
    "second_sameQP_csv5 = random.sample(second_sameQP_csv5, 100)\n",
    "second_sameQP_csv6 = random.sample(second_sameQP_csv6, 100)\n",
    "second_sameQP_csv7 = random.sample(second_sameQP_csv7, 100)\n",
    "second_sameQP_csv8 = random.sample(second_sameQP_csv8, 100)\n",
    "second_sameQP_csv9 = random.sample(second_sameQP_csv9, 100)\n",
    "second_sameQP_csv10 = random.sample(second_sameQP_csv10, 100)\n",
    "print(len(second_sameQP_csv7))\n",
    "\n",
    "second_largeQP2_csv1 = list(zip(second_largeQP2_lists[0], second_largeQP2_listsA[0], second_recompress_largeQP2_lists[0], second_recompress_largeQP2_listsA[0]))\n",
    "second_largeQP2_csv2 = list(zip(second_largeQP2_lists[1], second_largeQP2_listsA[1], second_recompress_largeQP2_lists[1], second_recompress_largeQP2_listsA[1]))\n",
    "second_largeQP2_csv3 = list(zip(second_largeQP2_lists[2], second_largeQP2_listsA[2], second_recompress_largeQP2_lists[2], second_recompress_largeQP2_listsA[2]))\n",
    "second_largeQP2_csv4 = list(zip(second_largeQP2_lists[3], second_largeQP2_listsA[3], second_recompress_largeQP2_lists[3], second_recompress_largeQP2_listsA[3]))\n",
    "second_largeQP2_csv5 = list(zip(second_largeQP2_lists[4], second_largeQP2_listsA[4], second_recompress_largeQP2_lists[4], second_recompress_largeQP2_listsA[4]))\n",
    "second_largeQP2_csv6 = list(zip(second_largeQP2_lists[5], second_largeQP2_listsA[5], second_recompress_largeQP2_lists[5], second_recompress_largeQP2_listsA[5]))\n",
    "second_largeQP2_csv7 = list(zip(second_largeQP2_lists[6], second_largeQP2_listsA[6], second_recompress_largeQP2_lists[6], second_recompress_largeQP2_listsA[6]))\n",
    "second_largeQP2_csv8 = list(zip(second_largeQP2_lists[7], second_largeQP2_listsA[7], second_recompress_largeQP2_lists[7], second_recompress_largeQP2_listsA[7]))\n",
    "second_largeQP2_csv9 = list(zip(second_largeQP2_lists[8], second_largeQP2_listsA[8], second_recompress_largeQP2_lists[8], second_recompress_largeQP2_listsA[8]))\n",
    "second_largeQP2_csv10 = list(zip(second_largeQP2_lists[9], second_largeQP2_listsA[9], second_recompress_largeQP2_lists[9], second_recompress_largeQP2_listsA[9]))\n",
    "\n",
    "second_largeQP2_csv1 = random.sample(second_largeQP2_csv1, 100)\n",
    "second_largeQP2_csv2 = random.sample(second_largeQP2_csv2, 100)\n",
    "second_largeQP2_csv3 = random.sample(second_largeQP2_csv3, 100)\n",
    "second_largeQP2_csv4 = random.sample(second_largeQP2_csv4, 100)\n",
    "second_largeQP2_csv5 = random.sample(second_largeQP2_csv5, 100)\n",
    "second_largeQP2_csv6 = random.sample(second_largeQP2_csv6, 100)\n",
    "second_largeQP2_csv7 = random.sample(second_largeQP2_csv7, 100)\n",
    "second_largeQP2_csv8 = random.sample(second_largeQP2_csv8, 100)\n",
    "second_largeQP2_csv9 = random.sample(second_largeQP2_csv9, 100)\n",
    "second_largeQP2_csv10 = random.sample(second_largeQP2_csv10, 100)\n",
    "print(len(second_largeQP2_csv7))\n",
    "\n",
    "\n",
    "train_csv_list1 = single_csv1 + second_largeQP1_csv1 + second_sameQP_csv1 + second_largeQP2_csv1\n",
    "train_csv_list2 = single_csv2 + second_largeQP1_csv2 + second_sameQP_csv2 + second_largeQP2_csv2\n",
    "train_csv_list3 = single_csv3 + second_largeQP1_csv3 + second_sameQP_csv3 + second_largeQP2_csv3\n",
    "train_csv_list4 = single_csv4 + second_largeQP1_csv4 + second_sameQP_csv4 + second_largeQP2_csv4\n",
    "train_csv_list5 = single_csv5 + second_largeQP1_csv5 + second_sameQP_csv5 + second_largeQP2_csv5\n",
    "train_csv_list6 = single_csv6 + second_largeQP1_csv6 + second_sameQP_csv6 + second_largeQP2_csv6\n",
    "train_csv_list7 = single_csv7 + second_largeQP1_csv7 + second_sameQP_csv7 + second_largeQP2_csv7\n",
    "train_csv_list8 = single_csv8 + second_largeQP1_csv8 + second_sameQP_csv8 + second_largeQP2_csv8\n",
    "train_csv_list9 = single_csv9 + second_largeQP1_csv9 + second_sameQP_csv9 + second_largeQP2_csv9\n",
    "train_csv_list10 = single_csv10 + second_largeQP1_csv10 + second_sameQP_csv10 + second_largeQP2_csv10\n",
    "\n",
    "print(\"train_csv_list: \", len(train_csv_list7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(probabilities, alpha=1):\n",
    "    \"\"\"\n",
    "    ラプラス平滑化を行う関数\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): 平滑化する確率分布のリスト\n",
    "    alpha (float): 平滑化パラメータ\n",
    "    \n",
    "    Returns:\n",
    "    smoothed_probabilities (list): 平滑化された確率分布のリスト\n",
    "    \"\"\"\n",
    "    total_count = sum(probabilities)\n",
    "    num_elements = len(probabilities)\n",
    "    \n",
    "    smoothed_probabilities = [(count + alpha) / (total_count + alpha * num_elements) for count in probabilities]\n",
    "    \n",
    "    return smoothed_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_csv_lists(train_csv_list):\n",
    "    pu_columns = [\"PU1_64\", \"PU1_32\", \"PU1_16\", \"PU1_8\", \"PU1_4\",  \n",
    "                  \"PU2_64\",\"PU2_32\", \"PU2_16\", \"PU2_8\", \"PU2_4\"]\n",
    "\n",
    "#     luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_2\",\"LU1_3\",\n",
    "#                          \"LU1_4\",\"LU1_5\",\"LU1_6\",\"LU1_7\",\n",
    "#                          \"LU1_8\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\n",
    "#                          \"LU1_12\",\"LU1_13\",\"LU1_14\",\"LU1_15\",\n",
    "#                          \"LU1_16\",\"LU1_17\",\"LU1_18\",\"LU1_19\",\n",
    "#                          \"LU1_20\",\"LU1_21\",\"LU1_22\",\"LU1_23\",\n",
    "#                          \"LU1_24\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "#                          \"LU1_28\",\"LU1_29\",\"LU1_30\",\"LU1_31\",\n",
    "#                          \"LU1_32\",\"LU1_33\",\"LU1_34\",\n",
    "                         \n",
    "#                          \"LU2_0\",\"LU2_1\",\"LU2_2\",\"LU2_3\",\n",
    "#                          \"LU2_4\",\"LU2_5\",\"LU2_6\",\"LU2_7\",\n",
    "#                          \"LU2_8\",\"LU2_9\",\"LU2_10\",\"LU2_11\",\n",
    "#                          \"LU2_12\",\"LU2_13\",\"LU2_14\",\"LU2_15\",\n",
    "#                          \"LU2_16\",\"LU2_17\",\"LU2_18\",\"LU2_19\",\n",
    "#                          \"LU2_20\",\"LU2_21\",\"LU2_22\",\"LU2_23\",\n",
    "#                          \"LU2_24\",\"LU2_25\",\"LU2_26\",\"LU2_27\",\n",
    "#                          \"LU2_28\",\"LU2_29\",\"LU2_30\",\"LU2_31\",\n",
    "#                          \"LU2_32\",\"LU2_33\",\"LU2_34\"]\n",
    "    \n",
    "    luminance_columns = [\"LU1_0\",\"LU1_1\",\"LU1_9\",\"LU1_10\",\"LU1_11\",\"LU1_25\",\"LU1_26\",\"LU1_27\",\n",
    "                         \"LU2_0\",\"LU2_1\",\"LU2_9\",\"LU2_10\",\"LU2_11\", \"LU2_25\",\"LU2_26\",\"LU2_27\"]\n",
    "\n",
    "    chrominance_columns = [\"CH1_0\", \"CH1_1\", \"CH1_10\", \"CH1_26\", \"CH1_34\", \"CH1_36\", \n",
    "                           \"CH2_0\", \"CH2_1\", \"CH2_10\", \"CH2_26\", \"CH2_34\", \"CH2_36\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_columns = [\"LABEL\"]\n",
    "    mae1_columns = [f\"MAE1_{i}\" for i in range(52)]\n",
    "    mae2_columns = [f\"MAE2_{i}\" for i in range(52)]\n",
    "    mae_columns = [\"MAE\"]\n",
    "    final_qp_columns = [\"FINAL_QP\"]\n",
    "    kl_divergence1 = [\"KLD_PU\"]\n",
    "    kl_divergence2 = [\"KLD_LUMA\"]\n",
    "    kl_divergence3 = [\"KLD_CHROMA\"]\n",
    "    ratio_columns1 = [\"RATIO1\"]\n",
    "    ratio_columns2 = [\"RATIO2\"]\n",
    "    \n",
    "    train_df1_1 = pd.DataFrame(columns=pu_columns)\n",
    "    train_df1_2 = pd.DataFrame(columns=luminance_columns)\n",
    "    train_df1_3 = pd.DataFrame(columns=chrominance_columns)\n",
    "    LABEL = pd.DataFrame(columns=label_columns)\n",
    "    RATIO1 = pd.DataFrame(columns=ratio_columns1)\n",
    "    RATIO2 = pd.DataFrame(columns=ratio_columns2)\n",
    "    train_df3 = pd.DataFrame(columns=mae1_columns)\n",
    "    train_df4 = pd.DataFrame(columns=mae2_columns)\n",
    "    MAE = pd.DataFrame(columns=mae_columns)\n",
    "    FINAL_QP = pd.DataFrame(columns=final_qp_columns)\n",
    "    kl_divergence_df1 = pd.DataFrame(columns=kl_divergence1)\n",
    "    kl_divergence_df2 = pd.DataFrame(columns=kl_divergence2)\n",
    "    kl_divergence_df3 = pd.DataFrame(columns=kl_divergence3)\n",
    "\n",
    "    for path1, path2, path3, path4 in train_csv_list:\n",
    "        label = 1 if (\"2ndQP\" in path1) and (\"3rdQP\" in path3) else 0\n",
    "        train_pkl_list = [path2, path4]\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path3)\n",
    "        \n",
    "        # 平滑化を行う\n",
    "        probabilities_df1 = laplace_smoothing([df1.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        probabilities_df2 = laplace_smoothing([df2.loc[i, \"pu_counts\"] for i in [0,1,2,3,4]])\n",
    "        kl_divergence1 = entropy(probabilities_df1, probabilities_df2)\n",
    "        \n",
    "        probabilities_df3 = laplace_smoothing([df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        probabilities_df4 = laplace_smoothing([df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]])\n",
    "        kl_divergence2 = entropy(probabilities_df3, probabilities_df4)\n",
    "        \n",
    "        probabilities_df5 = laplace_smoothing([df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        probabilities_df6 = laplace_smoothing([df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]])\n",
    "        kl_divergence3 = entropy(probabilities_df5, probabilities_df6)\n",
    "        \n",
    "        \n",
    "        pu_values = [df1.loc[i, \"pu_counts\"] for i in range(5)] + [df2.loc[i, \"pu_counts\"] for i in range(5)]\n",
    "        # lu_values = [df1.loc[i, \"luminance_counts\"] for i in range(35)] + [df2.loc[i, \"luminance_counts\"] for i in range(35)]\n",
    "        lu_values = [df1.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]] + [df2.loc[i, \"luminance_counts\"] for i in [0,1,9,10,11,25,26,27]]\n",
    "        ch_values = [df1.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]] + [df2.loc[i, \"chroma_counts\"] for i in [0,1,10,26,34,36]]\n",
    "        \n",
    "        train_df1_1 = pd.concat([train_df1_1, pd.DataFrame([pu_values], columns=pu_columns)], ignore_index=True)\n",
    "        train_df1_2= pd.concat([train_df1_2, pd.DataFrame([lu_values], columns=luminance_columns)], ignore_index=True)\n",
    "        train_df1_3 = pd.concat([train_df1_3, pd.DataFrame([ch_values], columns=chrominance_columns)], ignore_index=True)\n",
    "        \n",
    "        kl_divergence_df1 = pd.concat([kl_divergence_df1, pd.DataFrame({\"KLD_PU\": [kl_divergence1]})], ignore_index=True)\n",
    "        kl_divergence_df2 = pd.concat([kl_divergence_df2, pd.DataFrame({\"KLD_LUMA\": [kl_divergence2]})], ignore_index=True)\n",
    "        kl_divergence_df3 = pd.concat([kl_divergence_df3, pd.DataFrame({\"KLD_CHROMA\": [kl_divergence3]})], ignore_index=True)\n",
    "\n",
    "\n",
    "        LABEL = pd.concat([LABEL, pd.DataFrame({\"LABEL\": [label]})], ignore_index=True)\n",
    "\n",
    "        final_QP = extract_finalQP(train_pkl_list[0])\n",
    "\n",
    "        mae_d1 = calculate_mae(train_pkl_list[0])\n",
    "        mae_d2 = calculate_mae(train_pkl_list[1])\n",
    "        ratio1 = ratio_double_compressed(mae_d1, final_QP)\n",
    "        ratio2 = ratio_double_compressed(mae_d2, final_QP)\n",
    "\n",
    "        RATIO1 = pd.concat([RATIO1, pd.DataFrame({\"RATIO1\": [ratio1]})], ignore_index=True)\n",
    "        RATIO2 = pd.concat([RATIO2, pd.DataFrame({\"RATIO2\": [ratio2]})], ignore_index=True)\n",
    "\n",
    "        train_df3 = pd.concat([train_df3, pd.DataFrame({f\"MAE1_{i}\": [mae_d1[i]] for i in range(52)})], ignore_index=True)\n",
    "        train_df4 = pd.concat([train_df4, pd.DataFrame({f\"MAE2_{i}\": [mae_d2[i]] for i in range(52)})], ignore_index=True)\n",
    "        MAE = pd.concat([MAE, pd.DataFrame({\"MAE\": [mae_d1]})], ignore_index=True)\n",
    "        FINAL_QP = pd.concat([FINAL_QP, pd.DataFrame({\"FINAL_QP\": [final_QP]})], ignore_index=True)\n",
    "\n",
    "    train_df1_1.reset_index(drop=True, inplace=True)\n",
    "    train_df1_2.reset_index(drop=True, inplace=True)\n",
    "    train_df1_3.reset_index(drop=True, inplace=True)\n",
    "    LABEL.reset_index(drop=True, inplace=True)\n",
    "    RATIO1.reset_index(drop=True, inplace=True)\n",
    "    RATIO2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df1.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df2.reset_index(drop=True, inplace=True)\n",
    "    kl_divergence_df3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # train_df = pd.concat([train_df1_1, train_df1_2, train_df1_3, train_df3, train_df4], axis=1)\n",
    "    train_df = pd.concat([FINAL_QP, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "    train_df_onlyGhost = pd.concat([FINAL_QP, train_df1_1, train_df1_2, train_df1_3, kl_divergence_df1, kl_divergence_df2, kl_divergence_df3, RATIO1, RATIO2], axis=1)\n",
    "\n",
    "    return train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, train_df_onlyGhost1, LABEL1, MAE1, FINAL_QP1 = process_train_csv_lists(train_csv_list1)\n",
    "train_df2, train_df_onlyGhost2, LABEL2, MAE2, FINAL_QP2 = process_train_csv_lists(train_csv_list2)\n",
    "train_df3, train_df_onlyGhost3, LABEL3, MAE3, FINAL_QP3 = process_train_csv_lists(train_csv_list3)\n",
    "train_df4, train_df_onlyGhost4, LABEL4, MAE4, FINAL_QP4 = process_train_csv_lists(train_csv_list4)\n",
    "train_df5, train_df_onlyGhost5, LABEL5, MAE5, FINAL_QP5 = process_train_csv_lists(train_csv_list5)\n",
    "train_df6, train_df_onlyGhost6, LABEL6, MAE6, FINAL_QP6 = process_train_csv_lists(train_csv_list6)\n",
    "train_df7, train_df_onlyGhost7, LABEL7, MAE7, FINAL_QP7 = process_train_csv_lists(train_csv_list7)\n",
    "train_df8, train_df_onlyGhost8, LABEL8, MAE8, FINAL_QP8 = process_train_csv_lists(train_csv_list8)\n",
    "train_df9, train_df_onlyGhost9, LABEL9, MAE9, FINAL_QP9 = process_train_csv_lists(train_csv_list9)\n",
    "train_df10, train_df_onlyGhost10, LABEL10, MAE10, FINAL_QP10 = process_train_csv_lists(train_csv_list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FINAL_QP    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0         10  0.000925  0.000371   0.000748  0.044224  0.032297\n",
      "1         16  0.000763  0.000432   0.000256   0.01898  0.010814\n",
      "2         20  0.000257   0.00093   0.000485  0.008844  0.005208\n",
      "3         24  0.000115  0.000349   0.001575  0.026822  0.021389\n",
      "4         27  0.000056  0.001513   0.002698  0.238011  0.211598\n",
      "..       ...       ...       ...        ...       ...       ...\n",
      "595       39  0.000459  0.009606   0.005401  0.159634  0.165134\n",
      "596       45  0.001821  0.011087    0.00767  0.109629  0.136579\n",
      "597       16  0.000555  0.006544   0.000547  0.285606  0.131969\n",
      "598       42  0.003567  0.002549    0.00759  0.121984  0.125414\n",
      "599       32  0.000096  0.002376   0.006934  0.223639  0.202169\n",
      "\n",
      "[600 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データフレームを結合\n",
    "combined_train_df = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], ignore_index=True)\n",
    "combined_train_df_onlyGhost = pd.concat([train_df_onlyGhost1, train_df_onlyGhost2, train_df_onlyGhost3, train_df_onlyGhost4, train_df_onlyGhost5, train_df_onlyGhost6, train_df_onlyGhost7, train_df_onlyGhost8, train_df_onlyGhost9, train_df_onlyGhost10], ignore_index=True)\n",
    "combined_LABEL = pd.concat([LABEL1, LABEL2, LABEL3, LABEL4, LABEL5, LABEL6, LABEL7, LABEL8, LABEL9, LABEL10], ignore_index=True)\n",
    "combined_MAE = pd.concat([MAE1, MAE2, MAE3, MAE4, MAE5, MAE6, MAE7, MAE8, MAE9, MAE10], ignore_index=True)\n",
    "combined_FINAL_QP = pd.concat([FINAL_QP1, FINAL_QP2, FINAL_QP3, FINAL_QP4, FINAL_QP5, FINAL_QP6, FINAL_QP7, FINAL_QP8, FINAL_QP9, FINAL_QP10], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 6)\n",
      "(6000, 44)\n",
      "(6000, 1)\n",
      "(6000, 1)\n",
      "(6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df.shape)\n",
    "print(combined_train_df_onlyGhost.shape)\n",
    "print(combined_LABEL.shape)\n",
    "print(combined_MAE.shape)\n",
    "print(combined_FINAL_QP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "Combined Train DF:\n",
      "  FINAL_QP    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10  0.000925  0.000371   0.000748  0.044224  0.032297\n",
      "1       16  0.000763  0.000432   0.000256   0.01898  0.010814\n",
      "2       20  0.000257   0.00093   0.000485  0.008844  0.005208\n",
      "3       24  0.000115  0.000349   0.001575  0.026822  0.021389\n",
      "4       27  0.000056  0.001513   0.002698  0.238011  0.211598\n",
      "Combined Train DF Only Ghost:\n",
      "  FINAL_QP PU1_64 PU1_32 PU1_16  PU1_8  PU1_4 PU2_64 PU2_32 PU2_16  PU2_8  PU2_4  LU1_0 LU1_1 LU1_9 LU1_10 LU1_11 LU1_25 LU1_26 LU1_27  LU2_0 LU2_1 LU2_9 LU2_10 LU2_11 LU2_25 LU2_26 LU2_27  CH1_0 CH1_1 CH1_10 CH1_26 CH1_34 CH1_36  CH2_0  CH2_1 CH2_10 CH2_26 CH2_34 CH2_36    KLD_PU  KLD_LUMA KLD_CHROMA    RATIO1    RATIO2\n",
      "0       10      0    192   2176  15020  42612      0    192   2160  13932  43716   7281  5097  2538   2675   1653   4030   2787   1907   7288  5454  2592   2684   1621   4067   2801   1872  13848  9588   7340   8852   1380  18992  13836  10116   7416   9216   1308  18108  0.000925  0.000371   0.000748  0.044224  0.032297\n",
      "1       16      0   1152   3168  21296  34384      0   1152   3088  20248  35512   7635  5984  2694   2585   1715   3910   2932   1790   7726  6404  2732   2557   1703   3986   2908   1731  12524  5836   5248   6144   1604  28644  12060   6016   5368   6196   1532  28828  0.000763  0.000432   0.000256   0.01898  0.010814\n",
      "2       20      0   2304   8352  21436  27908      0   2240   8320  20888  28552   9213  6704  2577   2327   1549   4351   3107   1601   9338  7219  2476   2359   1499   4581   2956   1536   9508  4604   3984   4624   1360  35920   8976   4464   4160   4624   1272  36504  0.000257   0.00093   0.000485  0.008844  0.005208\n",
      "3       24      0   4928  10496  21808  22768      0   4864  10544  21440  23152  10690  7817  2386   2154   1448   4694   2854   1564  10957  8261  2364   2160   1421   4823   2783   1531   6028  2284   2832   2932   1324  44600   5500   2228   2648   2800   1012  45812  0.000115  0.000349   0.001575  0.026822  0.021389\n",
      "4       27      0   6848  12896  21236  19020      0   6848  13152  21072  18928  10365  8012  2632   2104   1442   5615   3002   1475  10992  8654  2647   2070   1238   5495   2928   1433   5036  1796   2528   2056   1116  47468   4584   1512   2296   1648    876  49084  0.000056  0.001513   0.002698  0.238011  0.211598\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\")\n",
    "print(\"Combined Train DF:\")\n",
    "print(combined_train_df.head())\n",
    "print(\"Combined Train DF Only Ghost:\")\n",
    "print(combined_train_df_onlyGhost.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling:\n",
      "X_train:\n",
      "       0         1         2         3         4         5\n",
      "0  0.125  0.041442  0.002086  0.001859  0.042165  0.030874\n",
      "1  0.275  0.034177  0.002439  0.000628  0.016867  0.009358\n",
      "2  0.375  0.011500  0.005312  0.001200  0.006709  0.003743\n",
      "3  0.475  0.005127  0.001960  0.003929  0.024726  0.019950\n",
      "4  0.550  0.002513  0.008670  0.006740  0.236374  0.210446\n",
      "X_train_onlyGhost:\n",
      "      0    1         2         3         4         5    6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43\n",
      "0  0.125  0.0  0.003257  0.070607  0.520660  0.749051  0.0  0.003257  0.069089  0.475560  0.761762  0.158283  0.089518  0.193888  0.102865  0.137028  0.286262  0.088979  0.213001  0.169742  0.092169  0.199537  0.097779  0.134862  0.275710  0.089167  0.205331  0.313814  0.387801  0.416100  0.499887  0.116870  0.292868  0.306894  0.399842  0.386733  0.507266  0.121336  0.283015  0.041442  0.002086  0.001859  0.042165  0.030874\n",
      "1  0.275  0.0  0.019544  0.103033  0.738214  0.604416  0.0  0.019544  0.098772  0.691152  0.618805  0.169676  0.114984  0.205867  0.099399  0.142180  0.277738  0.094152  0.199933  0.184823  0.119085  0.210340  0.093037  0.141701  0.270219  0.092978  0.189865  0.283811  0.236046  0.297506  0.346962  0.135840  0.459305  0.267501  0.237787  0.279933  0.341039  0.142115  0.466489  0.034177  0.002439  0.000628  0.016867  0.009358\n",
      "2  0.375  0.0  0.039088  0.272490  0.743067  0.490578  0.0  0.038002  0.266121  0.712998  0.497526  0.220463  0.135655  0.196882  0.089463  0.128386  0.309064  0.100396  0.178823  0.240325  0.142177  0.190586  0.085645  0.124687  0.310555  0.094687  0.168476  0.215464  0.186216  0.225850  0.261125  0.115176  0.584770  0.199095  0.176443  0.216938  0.254513  0.117996  0.597864  0.011500  0.005312  0.001200  0.006709  0.003743\n",
      "3  0.475  0.0  0.083605  0.342573  0.755962  0.400225  0.0  0.082519  0.337257  0.731841  0.403429  0.267999  0.167609  0.182216  0.082801  0.119993  0.333428  0.091370  0.174690  0.296068  0.171701  0.181944  0.078215  0.118182  0.326961  0.088526  0.167928  0.136603  0.092380  0.160544  0.165575  0.112127  0.734446  0.121994  0.088063  0.138089  0.154117  0.093878  0.757171  0.005127  0.001960  0.003929  0.024726  0.019950\n",
      "4  0.550  0.0  0.116178  0.421025  0.736134  0.334341  0.0  0.116178  0.420676  0.719279  0.329825  0.257539  0.173208  0.201106  0.080875  0.119495  0.398849  0.096650  0.164749  0.297273  0.182836  0.203781  0.074855  0.102919  0.372517  0.093690  0.157179  0.114123  0.072642  0.143311  0.116106  0.094512  0.783901  0.101677  0.059763  0.119733  0.090709  0.081262  0.813172  0.002513  0.008670  0.006740  0.236374  0.210446\n",
      "Length of X_train_restored[0]: 600\n",
      "Length of X_train_onlyGhost_restored[0]: 600\n",
      "Length of MAE_restored[0]: 600\n",
      "Length of FINAL_QP_restored[0]: 600\n",
      "Length of Y_train_restored[0]: 600\n",
      "Length of X_train_restored[1]: 600\n",
      "Length of X_train_onlyGhost_restored[1]: 600\n",
      "Length of MAE_restored[1]: 600\n",
      "Length of FINAL_QP_restored[1]: 600\n",
      "Length of Y_train_restored[1]: 600\n",
      "Length of X_train_restored[2]: 600\n",
      "Length of X_train_onlyGhost_restored[2]: 600\n",
      "Length of MAE_restored[2]: 600\n",
      "Length of FINAL_QP_restored[2]: 600\n",
      "Length of Y_train_restored[2]: 600\n",
      "Length of X_train_restored[3]: 600\n",
      "Length of X_train_onlyGhost_restored[3]: 600\n",
      "Length of MAE_restored[3]: 600\n",
      "Length of FINAL_QP_restored[3]: 600\n",
      "Length of Y_train_restored[3]: 600\n",
      "Length of X_train_restored[4]: 600\n",
      "Length of X_train_onlyGhost_restored[4]: 600\n",
      "Length of MAE_restored[4]: 600\n",
      "Length of FINAL_QP_restored[4]: 600\n",
      "Length of Y_train_restored[4]: 600\n",
      "Length of X_train_restored[5]: 600\n",
      "Length of X_train_onlyGhost_restored[5]: 600\n",
      "Length of MAE_restored[5]: 600\n",
      "Length of FINAL_QP_restored[5]: 600\n",
      "Length of Y_train_restored[5]: 600\n",
      "Length of X_train_restored[6]: 600\n",
      "Length of X_train_onlyGhost_restored[6]: 600\n",
      "Length of MAE_restored[6]: 600\n",
      "Length of FINAL_QP_restored[6]: 600\n",
      "Length of Y_train_restored[6]: 600\n",
      "Length of X_train_restored[7]: 600\n",
      "Length of X_train_onlyGhost_restored[7]: 600\n",
      "Length of MAE_restored[7]: 600\n",
      "Length of FINAL_QP_restored[7]: 600\n",
      "Length of Y_train_restored[7]: 600\n",
      "Length of X_train_restored[8]: 600\n",
      "Length of X_train_onlyGhost_restored[8]: 600\n",
      "Length of MAE_restored[8]: 600\n",
      "Length of FINAL_QP_restored[8]: 600\n",
      "Length of Y_train_restored[8]: 600\n",
      "Length of X_train_restored[9]: 600\n",
      "Length of X_train_onlyGhost_restored[9]: 600\n",
      "Length of MAE_restored[9]: 600\n",
      "Length of FINAL_QP_restored[9]: 600\n",
      "Length of Y_train_restored[9]: 600\n"
     ]
    }
   ],
   "source": [
    "def process_results_to_lists(train_df, train_df_onlyGhost, LABEL, MAE, FINAL_QP, scaler_main=None, scaler_ghost=None, fit_scaler=True):\n",
    "\n",
    "    scaler_main = MinMaxScaler()\n",
    "    scaler_ghost = MinMaxScaler()\n",
    "    X_train = scaler_main.fit_transform(train_df)\n",
    "    X_train_onlyGhost = scaler_ghost.fit_transform(train_df_onlyGhost)\n",
    "\n",
    "    MAE_array = MAE.values\n",
    "    FINAL_QP_array = FINAL_QP.values\n",
    "    Y_train = LABEL['LABEL'].astype(int)\n",
    "\n",
    "    return X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train, scaler_main, scaler_ghost\n",
    "\n",
    "# スケーリングの処理\n",
    "X_train, X_train_onlyGhost, MAE_array, FINAL_QP_array, Y_train, scaler_main, scaler_ghost = process_results_to_lists(\n",
    "    combined_train_df, combined_train_df_onlyGhost, combined_LABEL, combined_MAE, combined_FINAL_QP, fit_scaler=True\n",
    ")\n",
    "\n",
    "# スケーリング後のデータを表示\n",
    "print(\"After scaling:\")\n",
    "print(\"X_train:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "print(\"X_train_onlyGhost:\")\n",
    "print(pd.DataFrame(X_train_onlyGhost).head())\n",
    "\n",
    "# データを元に戻すための関数\n",
    "def restore_data_to_original_order(data, num_splits, original_lengths):\n",
    "    restored_data = []\n",
    "    start_index = 0\n",
    "    for length in original_lengths:\n",
    "        restored_data.append(data[start_index:start_index + length])\n",
    "        start_index += length\n",
    "    return restored_data\n",
    "\n",
    "# 元のデータフレームの長さ\n",
    "original_lengths = [len(train_df1), len(train_df2), len(train_df3), len(train_df4), len(train_df5), \n",
    "                    len(train_df6), len(train_df7), len(train_df8), len(train_df9), len(train_df10)]\n",
    "\n",
    "\n",
    "# リストを初期化\n",
    "X_train_list = []\n",
    "X_train_onlyGhost_list = []\n",
    "MAE_list = []\n",
    "FINAL_QP_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "# データを元の順序に戻す\n",
    "X_train_list = restore_data_to_original_order(X_train, 10, original_lengths)\n",
    "X_train_onlyGhost_list = restore_data_to_original_order(X_train_onlyGhost, 10, original_lengths)\n",
    "MAE_list = restore_data_to_original_order(MAE_array, 10, original_lengths)\n",
    "FINAL_QP_list = restore_data_to_original_order(FINAL_QP_array, 10, original_lengths)\n",
    "Y_train_list = restore_data_to_original_order(Y_train, 10, original_lengths)\n",
    "\n",
    "# 確認用の出力\n",
    "for i in range(10):\n",
    "    print(f\"Length of X_train_restored[{i}]: {len(X_train_list[i])}\")\n",
    "    print(f\"Length of X_train_onlyGhost_restored[{i}]: {len(X_train_onlyGhost_list[i])}\")\n",
    "    print(f\"Length of MAE_restored[{i}]: {len(MAE_list[i])}\")\n",
    "    print(f\"Length of FINAL_QP_restored[{i}]: {len(FINAL_QP_list[i])}\")\n",
    "    print(f\"Length of Y_train_restored[{i}]: {len(Y_train_list[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fold-1>\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7196    0.7700    0.7440       300\n",
      "           1     0.7527    0.7000    0.7254       300\n",
      "\n",
      "    accuracy                         0.7350       600\n",
      "   macro avg     0.7362    0.7350    0.7347       600\n",
      "weighted avg     0.7362    0.7350    0.7347       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6049    0.6633    0.6328       300\n",
      "           1     0.6273    0.5667    0.5954       300\n",
      "\n",
      "    accuracy                         0.6150       600\n",
      "   macro avg     0.6161    0.6150    0.6141       600\n",
      "weighted avg     0.6161    0.6150    0.6141       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7850    0.8400    0.8116       300\n",
      "           1     0.8280    0.7700    0.7979       300\n",
      "\n",
      "    accuracy                         0.8050       600\n",
      "   macro avg     0.8065    0.8050    0.8048       600\n",
      "weighted avg     0.8065    0.8050    0.8048       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7614    0.7767    0.7690       300\n",
      "           1     0.7721    0.7567    0.7643       300\n",
      "\n",
      "    accuracy                         0.7667       600\n",
      "   macro avg     0.7668    0.7667    0.7666       600\n",
      "weighted avg     0.7668    0.7667    0.7666       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5717    0.9300    0.7081       300\n",
      "           1     0.8125    0.3033    0.4417       300\n",
      "\n",
      "    accuracy                         0.6167       600\n",
      "   macro avg     0.6921    0.6167    0.5749       600\n",
      "weighted avg     0.6921    0.6167    0.5749       600\n",
      "\n",
      "<Fold-2>\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7531    0.8133    0.7821       300\n",
      "           1     0.7971    0.7333    0.7639       300\n",
      "\n",
      "    accuracy                         0.7733       600\n",
      "   macro avg     0.7751    0.7733    0.7730       600\n",
      "weighted avg     0.7751    0.7733    0.7730       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6409    0.7200    0.6782       300\n",
      "           1     0.6806    0.5967    0.6359       300\n",
      "\n",
      "    accuracy                         0.6583       600\n",
      "   macro avg     0.6608    0.6583    0.6570       600\n",
      "weighted avg     0.6608    0.6583    0.6570       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7980    0.8033    0.8007       300\n",
      "           1     0.8020    0.7967    0.7993       300\n",
      "\n",
      "    accuracy                         0.8000       600\n",
      "   macro avg     0.8000    0.8000    0.8000       600\n",
      "weighted avg     0.8000    0.8000    0.8000       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7803    0.8167    0.7980       300\n",
      "           1     0.8077    0.7700    0.7884       300\n",
      "\n",
      "    accuracy                         0.7933       600\n",
      "   macro avg     0.7940    0.7933    0.7932       600\n",
      "weighted avg     0.7940    0.7933    0.7932       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5541    0.9733    0.7062       300\n",
      "           1     0.8904    0.2167    0.3485       300\n",
      "\n",
      "    accuracy                         0.5950       600\n",
      "   macro avg     0.7222    0.5950    0.5273       600\n",
      "weighted avg     0.7222    0.5950    0.5273       600\n",
      "\n",
      "<Fold-3>\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7462    0.8133    0.7783       300\n",
      "           1     0.7949    0.7233    0.7574       300\n",
      "\n",
      "    accuracy                         0.7683       600\n",
      "   macro avg     0.7705    0.7683    0.7679       600\n",
      "weighted avg     0.7705    0.7683    0.7679       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6424    0.7367    0.6863       300\n",
      "           1     0.6914    0.5900    0.6367       300\n",
      "\n",
      "    accuracy                         0.6633       600\n",
      "   macro avg     0.6669    0.6633    0.6615       600\n",
      "weighted avg     0.6669    0.6633    0.6615       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8088    0.8600    0.8336       300\n",
      "           1     0.8505    0.7967    0.8227       300\n",
      "\n",
      "    accuracy                         0.8283       600\n",
      "   macro avg     0.8297    0.8283    0.8282       600\n",
      "weighted avg     0.8297    0.8283    0.8282       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7969    0.8500    0.8226       300\n",
      "           1     0.8393    0.7833    0.8103       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8181    0.8167    0.8165       600\n",
      "weighted avg     0.8181    0.8167    0.8165       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5649    0.9433    0.7066       300\n",
      "           1     0.8283    0.2733    0.4110       300\n",
      "\n",
      "    accuracy                         0.6083       600\n",
      "   macro avg     0.6966    0.6083    0.5588       600\n",
      "weighted avg     0.6966    0.6083    0.5588       600\n",
      "\n",
      "<Fold-4>\n",
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7385    0.8567    0.7932       300\n",
      "           1     0.8294    0.6967    0.7572       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7839    0.7767    0.7752       600\n",
      "weighted avg     0.7839    0.7767    0.7752       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6378    0.8100    0.7137       300\n",
      "           1     0.7397    0.5400    0.6243       300\n",
      "\n",
      "    accuracy                         0.6750       600\n",
      "   macro avg     0.6888    0.6750    0.6690       600\n",
      "weighted avg     0.6888    0.6750    0.6690       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8080    0.8700    0.8379       300\n",
      "           1     0.8592    0.7933    0.8250       300\n",
      "\n",
      "    accuracy                         0.8317       600\n",
      "   macro avg     0.8336    0.8317    0.8314       600\n",
      "weighted avg     0.8336    0.8317    0.8314       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8091    0.8333    0.8210       300\n",
      "           1     0.8282    0.8033    0.8156       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8186    0.8183    0.8183       600\n",
      "weighted avg     0.8186    0.8183    0.8183       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5642    0.7767    0.6536       300\n",
      "           1     0.6417    0.4000    0.4928       300\n",
      "\n",
      "    accuracy                         0.5883       600\n",
      "   macro avg     0.6029    0.5883    0.5732       600\n",
      "weighted avg     0.6029    0.5883    0.5732       600\n",
      "\n",
      "<Fold-5>\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7283    0.8400    0.7802       300\n",
      "           1     0.8110    0.6867    0.7437       300\n",
      "\n",
      "    accuracy                         0.7633       600\n",
      "   macro avg     0.7697    0.7633    0.7619       600\n",
      "weighted avg     0.7697    0.7633    0.7619       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6472    0.7400    0.6905       300\n",
      "           1     0.6965    0.5967    0.6427       300\n",
      "\n",
      "    accuracy                         0.6683       600\n",
      "   macro avg     0.6719    0.6683    0.6666       600\n",
      "weighted avg     0.6719    0.6683    0.6666       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7354    0.9267    0.8201       300\n",
      "           1     0.9009    0.6667    0.7663       300\n",
      "\n",
      "    accuracy                         0.7967       600\n",
      "   macro avg     0.8182    0.7967    0.7932       600\n",
      "weighted avg     0.8182    0.7967    0.7932       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7932    0.8567    0.8237       300\n",
      "           1     0.8442    0.7767    0.8090       300\n",
      "\n",
      "    accuracy                         0.8167       600\n",
      "   macro avg     0.8187    0.8167    0.8164       600\n",
      "weighted avg     0.8187    0.8167    0.8164       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5630    0.9533    0.7079       300\n",
      "           1     0.8478    0.2600    0.3980       300\n",
      "\n",
      "    accuracy                         0.6067       600\n",
      "   macro avg     0.7054    0.6067    0.5529       600\n",
      "weighted avg     0.7054    0.6067    0.5529       600\n",
      "\n",
      "<Fold-6>\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7492    0.7767    0.7627       300\n",
      "           1     0.7682    0.7400    0.7538       300\n",
      "\n",
      "    accuracy                         0.7583       600\n",
      "   macro avg     0.7587    0.7583    0.7583       600\n",
      "weighted avg     0.7587    0.7583    0.7583       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6437    0.7167    0.6782       300\n",
      "           1     0.6805    0.6033    0.6396       300\n",
      "\n",
      "    accuracy                         0.6600       600\n",
      "   macro avg     0.6621    0.6600    0.6589       600\n",
      "weighted avg     0.6621    0.6600    0.6589       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7920    0.8633    0.8262       300\n",
      "           1     0.8498    0.7733    0.8098       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8209    0.8183    0.8180       600\n",
      "weighted avg     0.8209    0.8183    0.8180       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7948    0.8133    0.8040       300\n",
      "           1     0.8089    0.7900    0.7993       300\n",
      "\n",
      "    accuracy                         0.8017       600\n",
      "   macro avg     0.8018    0.8017    0.8016       600\n",
      "weighted avg     0.8018    0.8017    0.8016       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5700    0.9367    0.7087       300\n",
      "           1     0.8224    0.2933    0.4324       300\n",
      "\n",
      "    accuracy                         0.6150       600\n",
      "   macro avg     0.6962    0.6150    0.5706       600\n",
      "weighted avg     0.6962    0.6150    0.5706       600\n",
      "\n",
      "<Fold-7>\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7361    0.8367    0.7832       300\n",
      "           1     0.8108    0.7000    0.7513       300\n",
      "\n",
      "    accuracy                         0.7683       600\n",
      "   macro avg     0.7734    0.7683    0.7672       600\n",
      "weighted avg     0.7734    0.7683    0.7672       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6813    0.7267    0.7032       300\n",
      "           1     0.7071    0.6600    0.6828       300\n",
      "\n",
      "    accuracy                         0.6933       600\n",
      "   macro avg     0.6942    0.6933    0.6930       600\n",
      "weighted avg     0.6942    0.6933    0.6930       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8190    0.8900    0.8530       300\n",
      "           1     0.8796    0.8033    0.8397       300\n",
      "\n",
      "    accuracy                         0.8467       600\n",
      "   macro avg     0.8493    0.8467    0.8464       600\n",
      "weighted avg     0.8493    0.8467    0.8464       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8161    0.8133    0.8147       300\n",
      "           1     0.8140    0.8167    0.8153       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8150    0.8150    0.8150       600\n",
      "weighted avg     0.8150    0.8150    0.8150       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5691    0.9467    0.7109       300\n",
      "           1     0.8416    0.2833    0.4239       300\n",
      "\n",
      "    accuracy                         0.6150       600\n",
      "   macro avg     0.7054    0.6150    0.5674       600\n",
      "weighted avg     0.7054    0.6150    0.5674       600\n",
      "\n",
      "<Fold-8>\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7441    0.8433    0.7906       300\n",
      "           1     0.8192    0.7100    0.7607       300\n",
      "\n",
      "    accuracy                         0.7767       600\n",
      "   macro avg     0.7817    0.7767    0.7757       600\n",
      "weighted avg     0.7817    0.7767    0.7757       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5818    0.9600    0.7245       300\n",
      "           1     0.8857    0.3100    0.4593       300\n",
      "\n",
      "    accuracy                         0.6350       600\n",
      "   macro avg     0.7338    0.6350    0.5919       600\n",
      "weighted avg     0.7338    0.6350    0.5919       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8165    0.8600    0.8377       300\n",
      "           1     0.8521    0.8067    0.8288       300\n",
      "\n",
      "    accuracy                         0.8333       600\n",
      "   macro avg     0.8343    0.8333    0.8332       600\n",
      "weighted avg     0.8343    0.8333    0.8332       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7957    0.8567    0.8250       300\n",
      "           1     0.8448    0.7800    0.8111       300\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.8202    0.8183    0.8181       600\n",
      "weighted avg     0.8202    0.8183    0.8181       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5567    0.9333    0.6974       300\n",
      "           1     0.7938    0.2567    0.3879       300\n",
      "\n",
      "    accuracy                         0.5950       600\n",
      "   macro avg     0.6752    0.5950    0.5426       600\n",
      "weighted avg     0.6752    0.5950    0.5426       600\n",
      "\n",
      "<Fold-9>\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7352    0.8700    0.7969       300\n",
      "           1     0.8408    0.6867    0.7560       300\n",
      "\n",
      "    accuracy                         0.7783       600\n",
      "   macro avg     0.7880    0.7783    0.7765       600\n",
      "weighted avg     0.7880    0.7783    0.7765       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6472    0.7033    0.6741       300\n",
      "           1     0.6752    0.6167    0.6446       300\n",
      "\n",
      "    accuracy                         0.6600       600\n",
      "   macro avg     0.6612    0.6600    0.6594       600\n",
      "weighted avg     0.6612    0.6600    0.6594       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8140    0.8167    0.8153       300\n",
      "           1     0.8161    0.8133    0.8147       300\n",
      "\n",
      "    accuracy                         0.8150       600\n",
      "   macro avg     0.8150    0.8150    0.8150       600\n",
      "weighted avg     0.8150    0.8150    0.8150       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8197    0.8333    0.8264       300\n",
      "           1     0.8305    0.8167    0.8235       300\n",
      "\n",
      "    accuracy                         0.8250       600\n",
      "   macro avg     0.8251    0.8250    0.8250       600\n",
      "weighted avg     0.8251    0.8250    0.8250       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5651    0.9400    0.7059       300\n",
      "           1     0.8218    0.2767    0.4140       300\n",
      "\n",
      "    accuracy                         0.6083       600\n",
      "   macro avg     0.6935    0.6083    0.5599       600\n",
      "weighted avg     0.6935    0.6083    0.5599       600\n",
      "\n",
      "<Fold-10>\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "Summary_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7522    0.8400    0.7937       300\n",
      "           1     0.8189    0.7233    0.7681       300\n",
      "\n",
      "    accuracy                         0.7817       600\n",
      "   macro avg     0.7856    0.7817    0.7809       600\n",
      "weighted avg     0.7856    0.7817    0.7809       600\n",
      "\n",
      "Summary_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5880    0.9467    0.7254       300\n",
      "           1     0.8632    0.3367    0.4844       300\n",
      "\n",
      "    accuracy                         0.6417       600\n",
      "   macro avg     0.7256    0.6417    0.6049       600\n",
      "weighted avg     0.7256    0.6417    0.6049       600\n",
      "\n",
      "Summary2_RBF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7933    0.8700    0.8299       300\n",
      "           1     0.8561    0.7733    0.8126       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8247    0.8217    0.8212       600\n",
      "weighted avg     0.8247    0.8217    0.8212       600\n",
      "\n",
      "Summary2_LINEAR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8227    0.8200    0.8214       300\n",
      "           1     0.8206    0.8233    0.8220       300\n",
      "\n",
      "    accuracy                         0.8217       600\n",
      "   macro avg     0.8217    0.8217    0.8217       600\n",
      "weighted avg     0.8217    0.8217    0.8217       600\n",
      "\n",
      "Summary old_model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5700    0.9633    0.7162       300\n",
      "           1     0.8817    0.2733    0.4173       300\n",
      "\n",
      "    accuracy                         0.6183       600\n",
      "   macro avg     0.7259    0.6183    0.5668       600\n",
      "weighted avg     0.7259    0.6183    0.5668       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cの範囲を指定\n",
    "C_values = {'C': [0.01, 0.1, 1, 10, 100, 1000, 2000, 3000, 4000, 5000]}\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 結果のデータフレームを初期化\n",
    "results = pd.DataFrame(columns=['C_RBF', 'Score_RBF', 'tnr_rbf', 'tpr_rbf',\n",
    "                                'C_LINEAR', 'Score_LINEAR', 'tnr_linear', 'tpr_linear',\n",
    "                                'C_OG_RBF', 'Score_OG_RBF', 'tnr_og_rbf', 'tpr_og_rbf',\n",
    "                                'C_OG_LINEAR', 'Score_OG_LINEAR', 'tnr_og_linear', 'tpr_og_linear',\n",
    "                                'Threshold', 'Score_old', 'tnr_old', 'tpr_old'])\n",
    "\n",
    "X_index = np.arange(10)  # インデックスとして0から9までの数字を用意\n",
    "\n",
    "# ループで各分割のtrain_idsとtest_idsを取得\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X_index)):\n",
    "    print(f\"<Fold-{fold+1}>\")\n",
    "    print(\"Train indices:\", train_ids)\n",
    "    print(\"Test indices:\", test_ids)\n",
    "        \n",
    "    test_data = [X_train_list[i] for i in test_ids]\n",
    "    test_data_OG = [X_train_onlyGhost_list[i] for i in test_ids]\n",
    "    test_label = [Y_train_list[i] for i in test_ids]\n",
    "    \n",
    "    train_data = [X_train_list[i] for i in train_ids]\n",
    "    train_data_OG = [X_train_onlyGhost_list[i] for i in train_ids]\n",
    "    train_label = [Y_train_list[i] for i in train_ids]\n",
    "    \n",
    "    train_data = [item for data in train_data for item in data]\n",
    "    train_data_OG = [item for data in train_data_OG for item in data]\n",
    "    train_label = [item for data in train_label for item in data]\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(train_data, train_label, test_size=600, random_state=42)\n",
    "    X_train_OG, X_val_OG, _, _ = train_test_split(train_data_OG, train_label, test_size=600, random_state=42)\n",
    "    \n",
    "    test_data = [item for data in test_data for item in data]\n",
    "    test_data_OG = [item for data in test_data_OG for item in data]\n",
    "    test_label = [item for data in test_label for item in data]\n",
    "\n",
    "    MAE_data = [MAE_list[i] for i in test_ids]\n",
    "    MAE_data = [item for data in MAE_data for item in data]\n",
    "    \n",
    "    FINAL_QP_data = [FINAL_QP_list[i] for i in test_ids]\n",
    "    FINAL_QP_data = [item for data in FINAL_QP_data for item in data]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    best_predicted_labels = []\n",
    "    best_ground_truth_labels = []\n",
    "                \n",
    "    for threshold in np.arange(0.00, 1.01, 0.01):\n",
    "        results_old = np.array([is_double_compressed(MAE_data[i], FINAL_QP_data[i], threshold) for i in range(600)])\n",
    "        predicted_labels = results_old.astype(int)\n",
    "        ground_truth_labels = np.array(test_label)\n",
    "        accuracy = np.sum(ground_truth_labels == predicted_labels) / len(ground_truth_labels)\n",
    "    \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "            best_predicted_labels = predicted_labels\n",
    "            best_ground_truth_labels = ground_truth_labels\n",
    "    \n",
    "    best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = 0, None, None\n",
    "    best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = 0, None, None\n",
    "    \n",
    "    best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = 0, None, None\n",
    "    best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = 0, None, None\n",
    "        \n",
    "    for C_value in C_values['C']:    \n",
    "        svm_model_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        svm_model_onlyGhost_RBF = SVC(kernel='rbf', C=C_value)\n",
    "        \n",
    "        svm_model_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "        svm_model_onlyGhost_LINEAR = SVC(kernel='linear', C=C_value)\n",
    "\n",
    "        svm_model_RBF.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_RBF.fit(X_train_OG, Y_train)\n",
    "        \n",
    "        svm_model_LINEAR.fit(X_train, Y_train)\n",
    "        svm_model_onlyGhost_LINEAR.fit(X_train_OG, Y_train)\n",
    "\n",
    "        val_accuracy_RBF = accuracy_score(Y_val, svm_model_RBF.predict(X_val))\n",
    "        val_accuracy_onlyGhost_RBF = accuracy_score(Y_val, svm_model_onlyGhost_RBF.predict(X_val_OG))\n",
    "        \n",
    "        val_accuracy_LINEAR = accuracy_score(Y_val, svm_model_LINEAR.predict(X_val))\n",
    "        val_accuracy_onlyGhost_LINEAR = accuracy_score(Y_val, svm_model_onlyGhost_LINEAR.predict(X_val_OG))\n",
    "\n",
    "        if val_accuracy_RBF > best_val_score_RBF:\n",
    "            best_val_score_RBF, best_svm_model_RBF, best_c_value_RBF = val_accuracy_RBF, svm_model_RBF, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_RBF > best_val_score_onlyGhost_RBF:\n",
    "            best_val_score_onlyGhost_RBF, best_svm_model_onlyGhost_RBF, best_c_value_onlyGhost_RBF = val_accuracy_onlyGhost_RBF, svm_model_onlyGhost_RBF, C_value\n",
    "            \n",
    "        if val_accuracy_LINEAR > best_val_score_LINEAR:\n",
    "            best_val_score_LINEAR, best_svm_model_LINEAR, best_c_value_LINEAR = val_accuracy_LINEAR, svm_model_LINEAR, C_value\n",
    "\n",
    "        if val_accuracy_onlyGhost_LINEAR > best_val_score_onlyGhost_LINEAR:\n",
    "            best_val_score_onlyGhost_LINEAR, best_svm_model_onlyGhost_LINEAR, best_c_value_onlyGhost_LINEAR = val_accuracy_onlyGhost_LINEAR, svm_model_onlyGhost_LINEAR, C_value\n",
    "\n",
    "    test_predictions_RBF = best_svm_model_RBF.predict(test_data)\n",
    "    test_accuracy_RBF = accuracy_score(test_label, test_predictions_RBF)\n",
    "    report_RBF = classification_report(test_label, test_predictions_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_RBF)\n",
    "    tnr_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_RBF:\\n{report_RBF}')\n",
    "    \n",
    "    test_predictions_LINEAR = best_svm_model_LINEAR.predict(test_data)\n",
    "    test_accuracy_LINEAR = accuracy_score(test_label, test_predictions_LINEAR)\n",
    "    report_LINEAR = classification_report(test_label, test_predictions_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_LINEAR)\n",
    "    tnr_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary_LINEAR:\\n{report_LINEAR}')\n",
    "        \n",
    "    test_predictions_onlyGhost_RBF = best_svm_model_onlyGhost_RBF.predict(test_data_OG)\n",
    "    test_accuracy_onlyGhost_RBF = accuracy_score(test_label, test_predictions_onlyGhost_RBF)\n",
    "    report_onlyGhost_RBF = classification_report(test_label, test_predictions_onlyGhost_RBF, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_RBF)\n",
    "    tnr_og_rbf = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_rbf = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary2_RBF:\\n{report_onlyGhost_RBF}')\n",
    "    \n",
    "    test_predictions_onlyGhost_LINEAR = best_svm_model_onlyGhost_LINEAR.predict(test_data_OG)\n",
    "    test_accuracy_onlyGhost_LINEAR = accuracy_score(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    report_onlyGhost_LINEAR = classification_report(test_label, test_predictions_onlyGhost_LINEAR, digits=4)\n",
    "    conf_matrix = confusion_matrix(test_label, test_predictions_onlyGhost_LINEAR)\n",
    "    tnr_og_linear = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_og_linear = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary2_LINEAR:\\n{report_onlyGhost_LINEAR}')\n",
    "    \n",
    "    report_old = classification_report(best_ground_truth_labels, best_predicted_labels, labels=[0,1], target_names=['0', '1'], zero_division=0, digits=4)\n",
    "    conf_matrix = confusion_matrix(best_ground_truth_labels, best_predicted_labels)\n",
    "    tnr_old = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    tpr_old = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(f'Summary old_model:\\n{report_old}')\n",
    "    \n",
    "    result_row = {'C_RBF': best_c_value_RBF, 'Score_RBF': test_accuracy_RBF, 'tnr_rbf': tnr_rbf, 'tpr_rbf': tpr_rbf,\n",
    "              'C_LINEAR': best_c_value_LINEAR, 'Score_LINEAR': test_accuracy_LINEAR, 'tnr_linear': tnr_linear, 'tpr_linear': tpr_linear,\n",
    "              'C_OG_RBF': best_c_value_onlyGhost_RBF, 'Score_OG_RBF': test_accuracy_onlyGhost_RBF, 'tnr_og_rbf': tnr_og_rbf, 'tpr_og_rbf': tpr_og_rbf,\n",
    "              'C_OG_LINEAR': best_c_value_onlyGhost_LINEAR, 'Score_OG_LINEAR': test_accuracy_onlyGhost_LINEAR, 'tnr_og_linear': tnr_og_linear, 'tpr_og_linear': tpr_og_linear,\n",
    "              'Threshold': best_threshold, 'Score_old': best_accuracy, 'tnr_old': tnr_old, 'tpr_old': tpr_old}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model  Average TNR  Average TPR  Average Test Score  Standard Deviation  Max Test Score  Min Test Score\n",
      "0      RBF        82.60        71.00               76.80                1.36           78.17           73.50\n",
      "1   LINEAR        77.23        54.17               65.70                2.19           69.33           61.50\n",
      "2     RBF2        86.00        77.93               81.97                1.60           84.67           79.67\n",
      "3  LINEAR2        82.70        79.17               80.93                1.77           82.50           76.67\n",
      "4      OLD        92.97        28.37               60.67                1.05           61.83           58.83\n"
     ]
    }
   ],
   "source": [
    "# 各統計情報を100倍して小数点第4位までの表記に変更\n",
    "statistics_data = {\n",
    "    'Model': ['RBF', 'LINEAR', 'RBF2', 'LINEAR2', 'OLD'],\n",
    "    'Average TNR': [round(results['tnr_rbf'].mean() * 100, 2), round(results['tnr_linear'].mean() * 100, 2), round(results['tnr_og_rbf'].mean() * 100, 2), round(results['tnr_og_linear'].mean() * 100, 2), round(results['tnr_old'].mean() * 100, 2)],\n",
    "    'Average TPR': [round(results['tpr_rbf'].mean() * 100, 2), round(results['tpr_linear'].mean() * 100, 2), round(results['tpr_og_rbf'].mean() * 100, 2), round(results['tpr_og_linear'].mean() * 100, 2), round(results['tpr_old'].mean() * 100, 2)],\n",
    "    'Average Test Score': [round(results['Score_RBF'].mean() * 100, 2), round(results['Score_LINEAR'].mean() * 100, 2), round(results['Score_OG_RBF'].mean() * 100, 2), round(results['Score_OG_LINEAR'].mean() * 100, 2), round(results['Score_old'].mean() * 100, 2)],\n",
    "    'Standard Deviation': [round(results['Score_RBF'].std() * 100, 2), round(results['Score_LINEAR'].std() * 100, 2), round(results['Score_OG_RBF'].std() * 100, 2), round(results['Score_OG_LINEAR'].std() * 100, 2), round(results['Score_old'].std() * 100, 2)],\n",
    "    'Max Test Score': [round(results['Score_RBF'].max() * 100, 2), round(results['Score_LINEAR'].max() * 100, 2), round(results['Score_OG_RBF'].max() * 100, 2), round(results['Score_OG_LINEAR'].max() * 100, 2), round(results['Score_old'].max() * 100, 2)],\n",
    "    'Min Test Score': [round(results['Score_RBF'].min() * 100, 2), round(results['Score_LINEAR'].min() * 100, 2), round(results['Score_OG_RBF'].min() * 100, 2), round(results['Score_OG_LINEAR'].min() * 100, 2), round(results['Score_old'].min() * 100, 2)],\n",
    "}\n",
    "\n",
    "# DataFrameを作成\n",
    "statistics_df = pd.DataFrame(statistics_data)\n",
    "\n",
    "# 表示\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5000\n",
      "1    2000\n",
      "2    4000\n",
      "3    1000\n",
      "4    5000\n",
      "5    1000\n",
      "6    4000\n",
      "7    4000\n",
      "8    3000\n",
      "9    5000\n",
      "Name: C_RBF, dtype: object\n",
      "0    1000\n",
      "1      10\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "5       1\n",
      "6      10\n",
      "7     0.1\n",
      "8      10\n",
      "9     0.1\n",
      "Name: C_LINEAR, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['C_RBF'])\n",
    "print(results['C_LINEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_csv('statistics_data3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
